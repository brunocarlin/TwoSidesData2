[{"authors":["admin"],"categories":null,"content":"Bruno Testaguzza Carlin is an student of Data Science at Insper.\nExperience using R and BI tools, improving Python skills.\nCurrently working at Bayer.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Bruno Testaguzza Carlin is an student of Data Science at Insper.\nExperience using R and BI tools, improving Python skills.\nCurrently working at Bayer.","tags":null,"title":"Bruno Carlin","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536462000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536462000,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\nSlides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906560000,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00-02:00","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Bruno Carlin"],"categories":["R and Python","r-project"],"content":"  Libraries Second Post Objectives Define the variables used in the conclusion Using masks or other methods to filter the data Visualizing the hypothesis Conclusion Before we start Reservations Data Dictionary   Python Importing the dataset from part 1 Difference in means Linear Regression Linearity Random Non-Collinearity Exogeneity Homoscedasticity / Homogeneity of Variance/ Assumption of Equal Variance  Fitting the linear regression Linear Regression plots QQ plot Scale-Location Plot Leverage plot   Final Remarks Next post    Libraries Let’s see what version of python this env is running.\nreticulate::py_config() ## python: /home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/bin/python ## libpython: /opt/python/3.7/lib/libpython3.7m.so ## pythonhome: /opt/python/3.7:/opt/python/3.7 ## virtualenv: /home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/bin/activate_this.py ## version: 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0] ## numpy: /home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/lib/python3.7/site-packages/numpy ## numpy_version: 1.18.1 ## ## NOTE: Python version was forced by use_python function import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import scipy.stats as ss import statsmodels.api as sm import statsmodels.formula.api as smf import os from statsmodels.graphics.gofplots import ProbPlot  Second Post Objectives  Define the variables used in the conclusion In our case, we initially choose to use salary ~ sex,region region was added to test whether Simpson’s paradox was at play.\nBut then I augmented our analysis with a simple linear regression.\n Using masks or other methods to filter the data We used it once.\n Visualizing the hypothesis We were advised to use two histograms combined to get a preview of our answer.\n Conclusion Comment on our findings.\n Before we start Reservations This is an exercise where we were supposed to ask a relevant question using the data from the IBGE(Brazil’s main data collector) database of 1970.\nOur group decided to ask whether women received less than man, we expanded the analysis hoping to avoid the Simpson’s paradox.\nThis is just an basic inference, and it’s results are therefore only used for studying purposes I don’t believe any finding would be relevant using just this approach but some basic operations can be used in a more impact full work.\n Data Dictionary We got a Data Dictionary that will be very useful for our Analysis, it contains all the required information about the encoding of the columns and the intended format that the folks at STATA desired.\n Portuguese  Descrição do Registro de Indivíduos nos EUA.\nDataset do software STATA (pago), vamos abri-lo com o pandas e transforma-lo em DataFrame.\nVariável 1 – CHAVE DO INDIVÍDUO ? Formato N - Numérico ? Tamanho 11 dígitos (11 bytes) ? Descrição Sumária Identifica unicamente o indivíduo na amostra.\nVariável 2 - IDADE CALCULADA EM ANOS ? Formato N - Numérico ? Tamanho 3 dígitos (3 bytes) ? Descrição Sumária Identifica a idade do morador em anos completos.\nVariável 3 – SEXO ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 3 ? Descrição Sumária Identifica o sexo do morador. Categorias (1) homem, (2) mulher e (3) gestante.\nVariável 4 – ANOS DE ESTUDO ? Formato N - Numérico ? Tamanho 2 dígitos (2 bytes) ? Quantidade de Categorias 11 ? Descrição Sumária Identifica o número de anos de estudo do morador. Categorias (05) Cinco ou menos, (06) Seis, (07) Sete, (08) Oito, (09) Nove, (10) Dez, (11) Onze, (12) Doze, (13) Treze, (14) Quatorze, (15) Quinze ou mais.\nVariável 5 – COR OU RAÇA ? Formato N - Numérico ? Tamanho 2 dígitos (2 bytes) ? Quantidade de Categorias 6 ? Descrição Sumária Identifica a Cor ou Raça declarada pelo morador. Categorias (01) Branca, (02) Preta, (03) Amarela, (04) Parda, (05) Indígena e (09) Não Sabe.\nVariável 6 – VALOR DO SALÁRIO (ANUALIZADO) ? Formato N - Numérico ? Tamanho 8 dígitos (8 bytes) ? Quantidade de Decimais 2 ? Descrição Sumária Identifica o valor resultante do salário anual do indivíduo. Categorias especiais (-1) indivíduo ausente na data da pesquisa e (999999) indivíduo não quis responder.\nVariável 7 – ESTADO CIVIL ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 2 ? Descrição Sumária Dummy que identifica o estado civil declarado pelo morador. Categorias (1) Casado, (0) não casado.\nVariável 8 – REGIÃO GEOGRÁFICA ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 5 ? Descrição Sumária Identifica a região geográfica do morador. Categorias (1) Norte, (2) Nordeste, (3) Sudeste, (4) Sul e (5) Centro-oeste.\n  English  Description of the US Individual Registry.\nDataset of the STATA software (paid), we will open it with pandas and turn it into DataFrame.\nVariable 1 - KEY OF THE INDIVIDUAL? Format N - Numeric? Size 11 digits (11 bytes)? Summary Description Uniquely identifies the individual in the sample.\nVariable 2 - AGE CALCULATED IN YEARS? Format N - Numeric? Size 3 digits (3 bytes)? Summary Description Identifies the age of the resident in full years.\nVariable 3 - SEX? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 3? Summary Description Identifies the gender of the resident. Categories (1) men, (2) women and (3) pregnant women.\nVariable 4 - YEARS OF STUDY? Format N - Numeric? Size 2 digits (2 bytes)? Number of Categories 11? Summary Description Identifies the number of years of study of the resident. Categories (05) Five or less, (06) Six, (07) Seven, (08) Eight, (09) Nine, (10) Dec, (11) Eleven, (12) Twelve, (13) Thirteen, (14 ) Fourteen, (15) Fifteen or more.\nVariable 5 - COLOR OR RACE? Format N - Numeric? Size 2 digits (2 bytes)? Number of Categories 6? Summary Description Identifies the Color or Race declared by the resident. Categories (01) White, (02) Black, (03) Yellow, (04) Brown, (05) Indigenous and (09) Don’t know.\nVariable 6 - WAGE VALUE (ANNUALIZED)? Format N - Numeric? Size 8 digits (8 bytes)? Number of decimals 2? Summary Description Identifies the amount resulting from the individual’s annual salary. Special categories (-1) individual absent on the survey date and (999999) individual did not want to answer.\nVariable 7 - CIVIL STATE? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 2? Summary Description Dummy that identifies the marital status declared by the resident. Categories (1) Married, (0) Not married.\nVariable 8 - GEOGRAPHICAL REGION? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 5? Summary Description Identifies the resident’s geographic region. Categories (1) North, (2) Northeast, (3) Southeast, (4) South and (5) Midwest.\n    Python Importing the dataset from part 1 You can also dowload it from the github page from this blog\ndf_sex_thesis =pd.read_feather(r.file_path_linux + \u0026#39;/sex_thesis_assignment.feather\u0026#39;) df_sex_thesis.info() ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## RangeIndex: 65795 entries, 0 to 65794 ## Data columns (total 9 columns): ## index 65795 non-null int64 ## age 65795 non-null int64 ## sex 65795 non-null object ## years_study 65795 non-null category ## color_race 65795 non-null object ## salary 65795 non-null float64 ## civil_status 65795 non-null object ## region 65795 non-null object ## log_salary 65795 non-null float64 ## dtypes: category(1), float64(2), int64(2), object(4) ## memory usage: 4.1+ MB Let’s get going first define which variables to add to the hypothesis, to isolate the factor of salary ~ sex, if we consider that our sample of individuals is random in nature comparing the means of the individuals given their sex and seeing if there is a significant difference in their means.\nA good graphic to get an idea if these effects would be significant was the bar plots used in part 1.\nWhen working with Categorical variable it is possible to use a groupby approach to glimpse at the difference in means.\n Difference in means Using the log salary feature from post 1.\ndf_agg1 = df_sex_thesis.groupby(\u0026#39;sex\u0026#39;).mean().log_salary df_agg1 ## sex ## man 9.026607 ## woman 8.607023 ## Name: log_salary, dtype: float64 Remember that in order to transform back our log variables you can do e^variable like this e ^ 9.03 is 8321.57 but the log of the mean is not the same as the mean of the log.\ndf_agg2 = df_sex_thesis.groupby(\u0026#39;sex\u0026#39;).mean().salary df_agg2 ## sex ## man 14302.491879 ## woman 10642.502734 ## Name: salary, dtype: float64 9.03 is not the same as 9.57\nTherefore which one should be done first log or mean?\nThe most common order is log then mean, because it is the order that reduces variance the most, you can read more about this here\n Group by explanation  Group by in pandas is a method that accepts a list of elements in this case just ‘sex’ and applies consequent operation in each group, in this case the mean method from a pandas DataFrame, .salary returns just the mean for the salary variable.\n df_sex_thesis.groupby([\u0026#39;sex\u0026#39;,\u0026#39;region\u0026#39;]).mean().log_salary ## sex region ## man midwest 9.155421 ## north 8.678263 ## south 9.123554 ## southeast 9.113084 ## woman midwest 8.847172 ## north 8.291580 ## northeast 9.462870 ## south 8.345867 ## southeast 8.766985 ## Name: log_salary, dtype: float64 Adding standard deviations\ndf_sex_thesis.groupby([\u0026#39;sex\u0026#39;]).std().log_salary ## sex ## man 1.397496 ## woman 2.009225 ## Name: log_salary, dtype: float64 These are really big Standard deviations! Remembering from stats that +2 SD’s gives about a 95% confidence interval we are not even close.\nCombining mean and std using pandas agg method.\ndf_agg =df_sex_thesis.groupby([\u0026#39;sex\u0026#39;]).agg([\u0026#39;mean\u0026#39;,\u0026#39;std\u0026#39;]).log_salary Calculating boundaries\ndf_agg[\u0026#39;lower_bound\u0026#39;] = df_agg[\u0026#39;mean\u0026#39;] - df_agg[\u0026#39;std\u0026#39;] * 2 df_agg[\u0026#39;upper_bound\u0026#39;] = df_agg[\u0026#39;mean\u0026#39;] + df_agg[\u0026#39;std\u0026#39;] * 2 df_agg ## mean std lower_bound upper_bound ## sex ## man 9.026607 1.397496 6.231615 11.821599 ## woman 8.607023 2.009225 4.588573 12.625473 Cool, but to verbose to be repeated multiple times, it is better to convert this series of operations into a function.\ndef groupby_bound(df,groupby_variables,value_variables): df_agg = df.groupby(groupby_variables).agg([\u0026#39;mean\u0026#39;,\u0026#39;std\u0026#39;])[value_variables] df_agg[\u0026#39;lower_bound\u0026#39;] = df_agg[\u0026#39;mean\u0026#39;] - df_agg[\u0026#39;std\u0026#39;] * 2 df_agg[\u0026#39;upper_bound\u0026#39;] = df_agg[\u0026#39;mean\u0026#39;] + df_agg[\u0026#39;std\u0026#39;] * 2 return df_agg groupby_bound(df=df_sex_thesis,groupby_variables=\u0026#39;sex\u0026#39;,value_variables=\u0026#39;log_salary\u0026#39;) ## mean std lower_bound upper_bound ## sex ## man 9.026607 1.397496 6.231615 11.821599 ## woman 8.607023 2.009225 4.588573 12.625473 Let’s try to find the difference in salary on some strata of the population.\ngroupby_bound(df=df_sex_thesis,groupby_variables=[\u0026#39;sex\u0026#39;,\u0026#39;region\u0026#39;],value_variables=\u0026#39;log_salary\u0026#39;) ## mean std lower_bound upper_bound ## sex region ## man midwest 9.155421 1.192631 6.770160 11.540683 ## north 8.678263 1.733378 5.211507 12.145019 ## south 9.123554 1.426591 6.270371 11.976736 ## southeast 9.113084 1.226845 6.659395 11.566773 ## woman midwest 8.847172 1.575228 5.696717 11.997627 ## north 8.291580 2.470543 3.350495 13.232665 ## northeast 9.462870 1.172049 7.118773 11.806968 ## south 8.345867 2.461307 3.423253 13.268482 ## southeast 8.766985 1.639338 5.488309 12.045660 No.\ngroupby_bound(df=df_sex_thesis,groupby_variables=[\u0026#39;sex\u0026#39;,\u0026#39;civil_status\u0026#39;],value_variables=\u0026#39;log_salary\u0026#39;) ## mean std lower_bound upper_bound ## sex civil_status ## man married 9.173335 1.247871 6.677593 11.669077 ## not_married 8.810261 1.567870 5.674521 11.946001 ## woman married 8.487709 2.263061 3.961586 13.013831 ## not_married 8.771966 1.578347 5.615272 11.928659 No.\ngroupby_bound(df=df_sex_thesis,groupby_variables=[\u0026#39;sex\u0026#39;,\u0026#39;civil_status\u0026#39;],value_variables=\u0026#39;log_salary\u0026#39;) ## mean std lower_bound upper_bound ## sex civil_status ## man married 9.173335 1.247871 6.677593 11.669077 ## not_married 8.810261 1.567870 5.674521 11.946001 ## woman married 8.487709 2.263061 3.961586 13.013831 ## not_married 8.771966 1.578347 5.615272 11.928659 No.\ngroupby_bound(df=df_sex_thesis,groupby_variables=[\u0026#39;sex\u0026#39;,\u0026#39;color_race\u0026#39;],value_variables=\u0026#39;log_salary\u0026#39;) ## mean std lower_bound upper_bound ## sex color_race ## man black 8.885541 1.215289 6.454962 11.316119 ## brown 8.878127 1.348974 6.180179 11.576076 ## indigenous 7.480596 3.328801 0.822994 14.138197 ## white 9.215328 1.369700 6.475927 11.954728 ## yellow 9.503222 1.398690 6.705843 12.300601 ## woman black 8.617966 1.683712 5.250543 11.985389 ## brown 8.518060 2.025707 4.466647 12.569473 ## indigenous 7.623917 3.342682 0.938553 14.309282 ## white 8.696991 2.001864 4.693263 12.700718 ## yellow 8.904886 1.877233 5.150420 12.659351 No.\ngroupby_bound(df=df_sex_thesis,groupby_variables=[\u0026#39;sex\u0026#39;,\u0026#39;years_study\u0026#39;],value_variables=\u0026#39;log_salary\u0026#39;) ## mean std lower_bound upper_bound ## sex years_study ## man 5.0 8.702990 1.495661 5.711668 11.694312 ## 6.0 8.836372 1.285517 6.265338 11.407407 ## 7.0 8.883292 1.302857 6.277578 11.489006 ## 8.0 8.939945 1.342110 6.255724 11.624166 ## 9.0 8.873640 1.292672 6.288296 11.458984 ## 10.0 9.064804 1.122187 6.820430 11.309177 ## 11.0 9.182335 1.163925 6.854486 11.510184 ## 12.0 9.274507 1.187977 6.898553 11.650462 ## 13.0 9.308213 1.651300 6.005613 12.610812 ## 14.0 9.563542 1.295638 6.972267 12.154817 ## 15.0 10.083954 1.334792 7.414369 12.753538 ## woman 5.0 8.150536 2.574328 3.001881 13.299191 ## 6.0 8.577851 1.854171 4.869508 12.286194 ## 7.0 8.572043 1.743439 5.085165 12.058921 ## 8.0 8.480459 1.952266 4.575927 12.384991 ## 9.0 8.609875 1.583736 5.442403 11.777346 ## 10.0 8.735525 1.403048 5.929428 11.541622 ## 11.0 8.755282 1.518318 5.718647 11.791918 ## 12.0 8.906622 1.505541 5.895540 11.917705 ## 13.0 8.963868 1.555327 5.853213 12.074523 ## 14.0 9.146458 1.263748 6.618962 11.673954 ## 15.0 9.572653 1.228241 7.116171 12.029134 Also no.\nDoes that mean that there were no Gender pay differences in Brazil in 1970?\nNo, it just means that there were no signs of this difference when looking at the whole population combined with one extra factor, but what if we combine all factors and isolate each influence in the salary? This would be a way to analyse the Ceteris Paribus(all else equal) effect of each feature in the salary, here is where Linear Regression comes in.\n Linear Regression But what is Linear Regression? you might ask, wasn’t it just one method for prediction? Not really, Linear Regression coefficients are really useful for hypothesis testing, meaning that tossing everything at it and then interpreting the results that come out without having to individually compare each feature pair, while also capturing the effect that all features have simultaneously.\nIs Linear Regression always perfect? No. In fact most of the time the results are a little biased or a underestimate the variance or are just flat out wrong.\nTo understand the kinds of errors we might face when doing a linear regression we can use the Gauss Markov Theorem.\nTerminology:\nPredictor/Independent Variable: Theses are the features e.g sex,years_study, region we can have p predictors where p = n -1 and n is the numbers of rows our dataset possesses in this case 65795 rows are present.\nPredicted/Dependent variable: This is the single “column” also called ‘target’ that we are modeling in this case we can use either log_salary or salary.\nfor a more in depth read this great blog post and for a more in depth usage in R and Python\nLinearity To get good results using Linear Regression the relationship of the Predictors and the Predicted variable has to be a linear relationship, to check for Linearity it is possible to use a simple line plot, and look for patterns like a parabola that would indicate that the Predictor has a quadratic relationship with the Dependent variable, there are ways of fixing non-linear relationships like we did with log_salary or by taking the power of the Predictor.\nLinearity can easily be tested for numerical Predictors, categorical predictors are harder to test, so in our case we only checked the age feature.\nNow it is time to flex these matplotlib graphs…\nx = df_sex_thesis[\u0026#39;age\u0026#39;] y = df_sex_thesis[\u0026#39;log_salary\u0026#39;] plt.scatter(x, y) z = np.polyfit(x, y, 1) p = np.poly1d(z) plt.plot(x,p(x),\u0026quot;r--\u0026quot;) plt.show() It seems age is not a great fit let’s try to also log age as well.\nx = np.log(df_sex_thesis[\u0026#39;age\u0026#39;]) y = df_sex_thesis[\u0026#39;log_salary\u0026#39;] plt.scatter(x, y) z = np.polyfit(x, y, 1) p = np.poly1d(z) plt.plot(x,p(x),\u0026quot;r--\u0026quot;) plt.show() Better but still close to no impact, maybe if we filter our sample to just the earning population, we can improve on it, we can call theses filters ‘masks’ in pandas.\ndf_filter = df_sex_thesis[df_sex_thesis[\u0026#39;log_salary\u0026#39;]\u0026gt; 2] df_filter ## index age sex ... civil_status region log_salary ## 0 0 53 man ... married north 11.060384 ## 1 1 49 woman ... married north 9.427336 ## 2 2 22 woman ... not_married northeast 8.378713 ## 3 3 55 man ... married north 11.478344 ## 4 4 56 woman ... married north 11.969090 ## ... ... ... ... ... ... ... ... ## 65790 66465 34 woman ... married midwest 9.427336 ## 65791 66466 40 man ... married midwest 7.793999 ## 65792 66467 36 woman ... married midwest 7.793999 ## 65793 66468 27 woman ... married midwest 8.617075 ## 65794 66469 37 man ... married midwest 6.134157 ## ## [63973 rows x 9 columns] x = df_filter[\u0026#39;age\u0026#39;] y = df_filter[\u0026#39;log_salary\u0026#39;] plt.scatter(x, y) z = np.polyfit(x, y, 1) p = np.poly1d(z) plt.plot(x,p(x),\u0026quot;r--\u0026quot;) plt.show() There is some slight improvement, it is a good question whether to filter otherwise sane values, the point is that the entire analysis would change, changing to salary ~ sex in the earning population in 1970 in Brazil instead of the salary ~ sex for the whole population in 1970 in Brazil.\nI think in both cases analyzing the Gender Pay Gap would be interesting it is even possible to split the hypothesis in two, analysing if Men and Women earn the same, and if Men and Women are have the same employment rate.\nSo for here on out We are analyzing just the Gender Pay Difference of employed people.\n Random This is a vital hypotheses it means that the observations(rows) were chosen at random for the entire Brazilian population, in this case We choose to trust that IBGE did a good job, if IBGE failed to correctly sample the population or if we mess to much with our filters we risk invalidating the whole process, yes that is right, if you don’t respect this hypothesis everything you have analysed is worthless.\n Non-Collinearity The effect of each Predictors is reduced when you introduce Colinear predictors, you are spliting the effect between the Predictors whenever a new predictor is added, meaning that you are in the worst case only calculating half of the coefficient, Collinearity always happens, Women live more so age is related to Sex, therefore age ‘steals’ part of the calculated effect from Sex, the more variables you introduce to your Linear Regression model the more that Collinearity plagues your estimations, everything is correlated.\nSo be careful when doing Linear Regression for estimating Ceteris Paribus effects so that you don’t introduce too many features or features that are too correlated with you hypothesis, remember that our hypothesis is Salary ~ Sex.\nA good way too know if you are introducing too much Collinearity is looking at the heatmap.\ncorr = pd.get_dummies(df_sex_thesis[[\u0026#39;sex\u0026#39;,\u0026#39;age\u0026#39;]]).corr() sns.heatmap(corr) This is quite cloudy let’s get rid of the Sex interaction with itself.\nWe are using a really cool pandas operation inspired this stack overflow answer, and combining it with the negate operator ‘~’ effectively selecting just the columns that don’t start with sex.\nnew_corr = corr.loc[:,~corr.columns.str.startswith(\u0026#39;sex\u0026#39;)] sns.heatmap(new_corr) Very little correlation, we are fine.\nOnce again we don’t usually calculate the correlation between Categorical Variables.\n Exogeneity If violated this hypothesis blasts your study into oblivion, Exogeneity is a one way road, your Independent Variables influence your Dependent Variable, and that is it.\nDiscussion on whether we are violating this assumption creates really cool intellectual pursuits, Nobel’s were won discovering if there was some violation to this assumption see Trygve_Haavelmo.\nIn our case let’s hypothesize for all variables\nSex ~ Salary - Maybe people that get richer/poorer change Sex, probably not.\nAge ~ Salary - You can’t buy year with money . Years Study ~ Salary - Possible but this probably only happen to the latter years of education, still worth considering. Color/Race ~ Salary - No. Civil Status ~ Salary - Yes I can see that, taking this feature out.\nRegion ~ Salary - Do richer people migrate to richer regions? I think so, taking this feature out.\nIt is also nice to notice that this may be reason why we call the Predicted Variable the Independent Variable.\n Homoscedasticity / Homogeneity of Variance/ Assumption of Equal Variance Assumption of Equal Variance of predicted values means that for any value for the whole distribution of the Dependent Variable the estimated values remain equally distributted, meaning that we are as sure on our predictions for 1000 moneys as for 100000 moneys this assumption is really hard to adhere.\nIf broken the variance of the coefficients may be under or over estimated, meaning that we may fail to consider relevant features or consider wrongly irrelevant features, there are many formal statistical tests for this assumption let’s use scipy’s Bartlett’s test for homogeneity of variances where Ho is Homoscedasticity confirmation meaning we hope for p-values \u0026lt; 0.05.\nWe used a significance level of 5% for this assignment.\nss.bartlett(df_filter[\u0026#39;log_salary\u0026#39;],df_filter[\u0026#39;age\u0026#39;]) ## BartlettResult(statistic=232468.57113475894, pvalue=0.0) Don’t reject H0 -\u0026gt; ok\nAnother way to check this assumption is using tests called Breusch-Pagan and Goldfeld-Quandt post fitting the linear model.\n  Fitting the linear regression Fitting the linear regression using yet another library called statsmodels.\nmod = smf.ols(formula=\u0026#39;log_salary ~ sex + age + years_study + color_race\u0026#39;, data=df_filter) model_fit = mod.fit() print(model_fit.summary()) ## OLS Regression Results ## ============================================================================== ## Dep. Variable: log_salary R-squared: 0.133 ## Model: OLS Adj. R-squared: 0.133 ## Method: Least Squares F-statistic: 613.6 ## Date: sáb, 25 jan 2020 Prob (F-statistic): 0.00 ## Time: 21:25:45 Log-Likelihood: -81546. ## No. Observations: 63973 AIC: 1.631e+05 ## Df Residuals: 63956 BIC: 1.633e+05 ## Df Model: 16 ## Covariance Type: nonrobust ## ============================================================================================ ## coef std err t P\u0026gt;|t| [0.025 0.975] ## -------------------------------------------------------------------------------------------- ## Intercept 8.3200 0.019 440.777 0.000 8.283 8.357 ## sex[T.woman] -0.2125 0.007 -30.970 0.000 -0.226 -0.199 ## years_study[T.6.0] 0.1520 0.020 7.756 0.000 0.114 0.190 ## years_study[T.7.0] 0.1728 0.018 9.441 0.000 0.137 0.209 ## years_study[T.8.0] 0.1721 0.014 12.391 0.000 0.145 0.199 ## years_study[T.9.0] 0.1395 0.019 7.449 0.000 0.103 0.176 ## years_study[T.10.0] 0.2414 0.018 13.444 0.000 0.206 0.277 ## years_study[T.11.0] 0.3314 0.009 35.414 0.000 0.313 0.350 ## years_study[T.12.0] 0.3826 0.018 21.100 0.000 0.347 0.418 ## years_study[T.13.0] 0.5989 0.025 24.032 0.000 0.550 0.648 ## years_study[T.14.0] 0.6619 0.025 25.988 0.000 0.612 0.712 ## years_study[T.15.0] 1.0396 0.013 78.438 0.000 1.014 1.066 ## color_race[T.brown] 0.0487 0.013 3.696 0.000 0.023 0.075 ## color_race[T.indigenous] 0.0587 0.040 1.451 0.147 -0.021 0.138 ## color_race[T.white] 0.1805 0.013 13.736 0.000 0.155 0.206 ## color_race[T.yellow] 0.2401 0.050 4.776 0.000 0.142 0.339 ## age 0.0129 0.000 40.197 0.000 0.012 0.014 ## ============================================================================== ## Omnibus: 14771.140 Durbin-Watson: 1.806 ## Prob(Omnibus): 0.000 Jarque-Bera (JB): 45603.025 ## Skew: -1.188 Prob(JB): 0.00 ## Kurtosis: 6.386 Cond. No. 584. ## ============================================================================== ## ## Warnings: ## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Looking at the results, it is possible that there isGender Pay Gap, calculating the difference in estimated salaries done by\n plus intercept + e ^ beta_variable = 5077,12 minus intercept 4105,16\n equals 971,96.  There is a 971,96 difference between men and women salaries in the earning population of Brazil in 1970 quite significant at 7,59% of the mean salary at the time.\nLinear Regression plots Using the code from this excellent post and combining it with the understanding from this post.\n# fitted values (need a constant term for intercept) model_fitted_y = model_fit.fittedvalues # model residuals model_residuals = model_fit.resid # normalized residuals model_norm_residuals = model_fit.get_influence().resid_studentized_internal # absolute squared normalized residuals model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals)) # absolute residuals model_abs_resid = np.abs(model_residuals) # leverage, from statsmodels internals model_leverage = model_fit.get_influence().hat_matrix_diag # cook\u0026#39;s distance, from statsmodels internals model_cooks = model_fit.get_influence().cooks_distance[0] Initializing some variables. ### Residual plot\nplot_lm_1 = plt.figure(1) plot_lm_1.axes[0] = sns.residplot(model_fitted_y, \u0026#39;log_salary\u0026#39;, data=df_filter, lowess=True, scatter_kws={\u0026#39;alpha\u0026#39;: 0.5}, line_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;, \u0026#39;lw\u0026#39;: 1, \u0026#39;alpha\u0026#39;: 0.8}) plot_lm_1.axes[0].set_title(\u0026#39;Residuals vs Fitted\u0026#39;) plot_lm_1.axes[0].set_xlabel(\u0026#39;Fitted values\u0026#39;) plot_lm_1.axes[0].set_ylabel(\u0026#39;Residuals\u0026#39;) # annotations abs_resid = model_abs_resid.sort_values(ascending=False) abs_resid_top_3 = abs_resid[:3] for i in abs_resid_top_3.index: plot_lm_1.axes[0].annotate(i, xy=(model_fitted_y[i], model_residuals[i])); Here we are looking for the red line to get as close to the doted black line meaning that our Predictors would have a perfectly linear relationship with our Dependent variable following the assumption of linearity.\nI think we are close enough.\n QQ plot QQ = ProbPlot(model_norm_residuals) plot_lm_2 = QQ.qqplot(line=\u0026#39;45\u0026#39;, alpha=0.5, color=\u0026#39;#4C72B0\u0026#39;, lw=1) plot_lm_2.axes[0].set_title(\u0026#39;Normal Q-Q\u0026#39;) plot_lm_2.axes[0].set_xlabel(\u0026#39;Theoretical Quantiles\u0026#39;) plot_lm_2.axes[0].set_ylabel(\u0026#39;Standardized Residuals\u0026#39;); # annotations abs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0) abs_norm_resid_top_3 = abs_norm_resid[:3] for r, i in enumerate(abs_norm_resid_top_3): plot_lm_2.axes[0].annotate(i, xy=(np.flip(QQ.theoretical_quantiles, 0)[r], model_norm_residuals[i])); Here we are looking for the circles to get as close to the red line as possible meaning that our variables follow a normal distribution and therefore our p-values are not biased.\nI think we have two problems the extremes may be a bit too distant and there are three concerning outliers.\n Scale-Location Plot plot_lm_3 = plt.figure(3) plt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5) sns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, scatter=False, ci=False, lowess=True, line_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;, \u0026#39;lw\u0026#39;: 1, \u0026#39;alpha\u0026#39;: 0.8}) plot_lm_3.axes[0].set_title(\u0026#39;Scale-Location\u0026#39;) plot_lm_3.axes[0].set_xlabel(\u0026#39;Fitted values\u0026#39;) plot_lm_3.axes[0].set_ylabel(\u0026#39;$\\sqrt{|Standardized Residuals|}$\u0026#39;); # annotations abs_sq_norm_resid = np.flip(np.argsort(model_norm_residuals_abs_sqrt), 0) abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3] for i in abs_norm_resid_top_3: plot_lm_3.axes[0].annotate(i, xy=(model_fitted_y[i], model_norm_residuals_abs_sqrt[i])); This is the graph where we check the homoscedasticity assumption, we want the red line to be as straight as possible meaning that our Predictor variance is constant among the Dependent Variable values.\nI think it is fine.\n Leverage plot plot_lm_4 = plt.figure(4) plt.scatter(model_leverage, model_norm_residuals, alpha=0.5) sns.regplot(model_leverage, model_norm_residuals, scatter=False, ci=False, lowess=True, line_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;, \u0026#39;lw\u0026#39;: 1, \u0026#39;alpha\u0026#39;: 0.8}) plot_lm_4.axes[0].set_xlim(0, 0.005) ## (0, 0.005) plot_lm_4.axes[0].set_ylim(-3, 5) ## (-3, 5) plot_lm_4.axes[0].set_title(\u0026#39;Residuals vs Leverage\u0026#39;) plot_lm_4.axes[0].set_xlabel(\u0026#39;Leverage\u0026#39;) plot_lm_4.axes[0].set_ylabel(\u0026#39;Standardized Residuals\u0026#39;) # annotations leverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3] for i in leverage_top_3: plot_lm_4.axes[0].annotate(i, xy=(model_leverage[i], model_norm_residuals[i])) # shenanigans for cook\u0026#39;s distance contours def graph(formula, x_range, label=None): x = x_range y = formula(x) plt.plot(x, y, label=label, lw=1, ls=\u0026#39;--\u0026#39;, color=\u0026#39;red\u0026#39;) p = len(model_fit.params) # number of model parameters graph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), np.linspace(0.000, 0.005, 50), \u0026#39;Cook\\\u0026#39;s distance\u0026#39;) # 0.5 line ## /home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/bin/activate_this.py:1: RuntimeWarning: divide by zero encountered in true_divide ## \u0026quot;\u0026quot;\u0026quot;Activate virtualenv for current interpreter: graph(lambda x: np.sqrt((1 * p * (1 - x)) / x), np.linspace(0.000, 0.005, 50)) # 1 line plt.legend(loc=\u0026#39;upper right\u0026#39;); Finally in this plot we are looking for outliers, it failed on the Python version, but it should show if the outliers plague the betas enough to the point where it may be worth studying removing them.\nWe want the Red line to be as close as possible to the dotted line.\nLooking at the R plot We can say it is fine.\nAt the end I am comfortable not denying our Hypothesis that Salary ~ Sex in 1970 Brazil working population.\nAnd that is it, Statistical analysis with almost no R! .\n   Final Remarks I guess my opinion is important in this post, this was really hard, Python may be an excellent Prediction based language but it lacks so much on my normal Economist features that I have easily available even when using Stata/E-Views/SAS, like look at how much code for a simple linear regression plot!\nI don’t have much hope that this will improve with time, normal statistics just doesn’t get as much hype as Deep Learning and stuff I feel sorry for whoever has to learn stats alongside Python, you guys deserve a Medal! Also I applaud the guys that Developed statsmodels.formula.api it really helps!\nWhoever develops with matplotlib deserves two medals, you guys make me feel dumber than when I read my first Time Series paper and that was a really low point in my self esteem, the graphs turned out great in my honest opinion.\nIf you liked it please share it.\nNext post In the next part we repeat everything from part 1 with a few twists in R using the tidyverse!\n  ","date":1579910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579910400,"objectID":"885c7846165d139fbc00209f4c6a12bd","permalink":"/2020/01/25/exploratory-data-analysis-basics-part2/","publishdate":"2020-01-25T00:00:00Z","relpermalink":"/2020/01/25/exploratory-data-analysis-basics-part2/","section":"post","summary":"Basics exploratory Data Analysis: Part 2 of 4","tags":["R Markdown","reticulate","pandas"],"title":"exploratory data analysis: basics Python part 2","type":"post"},{"authors":["Bruno Carlin"],"categories":["R and Python","r-project"],"content":"  Libraries The Exercise Before we get into it Objectives Reservations Data Dictionary   Python Pre-processing Reading Data Analyzing some basic stuff about our data frame Replacing columns names Cleaning categorical data Seeing the effects of categorical Variables Cleaning numerical data   Saving our work for later Next post   I am currently doing exercises from digital house brasil\nLibraries Let’s see what version of python this env is running.\nimport platform print(platform.python_version()) ## 3.7.4 import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import os  The Exercise Before we get into it Objectives Open and read a DataFrame using pandas Simple stuff right?\n Basic analysis of each column using value counts. I improved a bit on the base python capabilities\n Creating a hypothesis that we care about In our case the hypothesis is simple do women earn on average less than men?\n Data preprocessing We need to clean the data removing outliers, biases or any other factors that could in theory compromise our hypothesis testing.\n Visualize all the variables We were free to apply any technique.\nCategorical Data\n To do in the second post  Define the variables used in the conclusion In our case, we choose to use salary ~ sex,region region was added to test whether Simpson’s paradox was at play.\n Using masks or other methods to filter the data This objective was mostly done using the groupby function.\n Visualizing the hypothesis We were advised to use two histograms combined to get a preview of our answer.\n Conclusion Comment on our findings.\n  Reservations This is an exercise where we were supposed to ask a relevant question using the data from the IBGE(Brazil’s main data collector) database of 1970.\nOur group decided to ask whether women received less than man, we expanded the analysis hoping to avoid the Simpson’s paradox.\nThis is just an basic inference, and it’s results are therefore only used for studying purposes I don’t believe any finding would be relevant using just this approach but some basic operations can be used in a more impact full work.\n Data Dictionary We got a Data Dictionary that will be very useful for our Analysis, it contains all the required information about the encoding of the columns and the intended format that the folks at STATA desired.\n Portuguese  Descrição do Registro de Indivíduos nos EUA.\nDataset do software STATA (pago), vamos abri-lo com o pandas e transforma-lo em DataFrame.\nVariável 1 – CHAVE DO INDIVÍDUO ? Formato N - Numérico ? Tamanho 11 dígitos (11 bytes) ? Descrição Sumária Identifica unicamente o indivíduo na amostra.\nVariável 2 - IDADE CALCULADA EM ANOS ? Formato N - Numérico ? Tamanho 3 dígitos (3 bytes) ? Descrição Sumária Identifica a idade do morador em anos completos.\nVariável 3 – SEXO ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 3 ? Descrição Sumária Identifica o sexo do morador. Categorias (1) homem, (2) mulher e (3) gestante.\nVariável 4 – ANOS DE ESTUDO ? Formato N - Numérico ? Tamanho 2 dígitos (2 bytes) ? Quantidade de Categorias 11 ? Descrição Sumária Identifica o número de anos de estudo do morador. Categorias (05) Cinco ou menos, (06) Seis, (07) Sete, (08) Oito, (09) Nove, (10) Dez, (11) Onze, (12) Doze, (13) Treze, (14) Quatorze, (15) Quinze ou mais.\nVariável 5 – COR OU RAÇA ? Formato N - Numérico ? Tamanho 2 dígitos (2 bytes) ? Quantidade de Categorias 6 ? Descrição Sumária Identifica a Cor ou Raça declarada pelo morador. Categorias (01) Branca, (02) Preta, (03) Amarela, (04) Parda, (05) Indígena e (09) Não Sabe.\nVariável 6 – VALOR DO SALÁRIO (ANUALIZADO) ? Formato N - Numérico ? Tamanho 8 dígitos (8 bytes) ? Quantidade de Decimais 2 ? Descrição Sumária Identifica o valor resultante do salário anual do indivíduo. Categorias especiais (-1) indivíduo ausente na data da pesquisa e (999999) indivíduo não quis responder.\nVariável 7 – ESTADO CIVIL ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 2 ? Descrição Sumária Dummy que identifica o estado civil declarado pelo morador. Categorias (1) Casado, (0) não casado.\nVariável 8 – REGIÃO GEOGRÁFICA ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 5 ? Descrição Sumária Identifica a região geográfica do morador. Categorias (1) Norte, (2) Nordeste, (3) Sudeste, (4) Sul e (5) Centro-oeste.\n  English  Description of the US Individual Registry.\nDataset of the STATA software (paid), we will open it with pandas and turn it into DataFrame.\nVariable 1 - KEY OF THE INDIVIDUAL? Format N - Numeric? Size 11 digits (11 bytes)? Summary Description Uniquely identifies the individual in the sample.\nVariable 2 - AGE CALCULATED IN YEARS? Format N - Numeric? Size 3 digits (3 bytes)? Summary Description Identifies the age of the resident in full years.\nVariable 3 - SEX? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 3? Summary Description Identifies the gender of the resident. Categories (1) men, (2) women and (3) pregnant women.\nVariable 4 - YEARS OF STUDY? Format N - Numeric? Size 2 digits (2 bytes)? Number of Categories 11? Summary Description Identifies the number of years of study of the resident. Categories (05) Five or less, (06) Six, (07) Seven, (08) Eight, (09) Nine, (10) Dec, (11) Eleven, (12) Twelve, (13) Thirteen, (14 ) Fourteen, (15) Fifteen or more.\nVariable 5 - COLOR OR RACE? Format N - Numeric? Size 2 digits (2 bytes)? Number of Categories 6? Summary Description Identifies the Color or Race declared by the resident. Categories (01) White, (02) Black, (03) Yellow, (04) Brown, (05) Indigenous and (09) Don’t know.\nVariable 6 - WAGE VALUE (ANNUALIZED)? Format N - Numeric? Size 8 digits (8 bytes)? Number of decimals 2? Summary Description Identifies the amount resulting from the individual’s annual salary. Special categories (-1) individual absent on the survey date and (999999) individual did not want to answer.\nVariable 7 - CIVIL STATE? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 2? Summary Description Dummy that identifies the marital status declared by the resident. Categories (1) Married, (0) Not married.\nVariable 8 - GEOGRAPHICAL REGION? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 5? Summary Description Identifies the resident’s geographic region. Categories (1) North, (2) Northeast, (3) Southeast, (4) South and (5) Midwest.\n    Python Pre-processing Reading Data The path is specific for my computer but it is easy to adapt\nYou can also dowload it from the github page from this blog\n# Abertura e leitura dos dados em um DeteFrame em Pandas path = r.file_path_linux df = pd.read_csv(path + \u0026#39;/stata_data_1970.csv\u0026#39;)  Analyzing some basic stuff about our data frame #Análise básica dos conteúdos de cada coluna com contagem de valores df.info() ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## RangeIndex: 66470 entries, 0 to 66469 ## Data columns (total 9 columns): ## Unnamed: 0 66470 non-null int64 ## id 66470 non-null float64 ## idade 66470 non-null int64 ## sexo 66470 non-null object ## anos_estudo 66036 non-null float64 ## cor/raca 66228 non-null object ## salario 47878 non-null float64 ## estado_civil 66470 non-null float64 ## regiao 66470 non-null object ## dtypes: float64(4), int64(2), object(3) ## memory usage: 4.6+ MB I do enjoy python’s base value_counts but when used in a loop it can create some ugly outputs, in order to fix I created a function that adds some flavor text to the print output and generates new information about the accumulated percentage of the data being displayed.\nCustom count_values() def pretty_value_counts(data_frame, number_of_rows = 5, cum_perc = True): for col in data_frame: counts = data_frame[col].value_counts(dropna=False) percentages = data_frame[col].value_counts(dropna=False, normalize=True) if cum_perc == True: cum_percentages = percentages.cumsum() tb = pd.concat([counts, percentages, cum_percentages], axis=1, keys=[\u0026#39;counts\u0026#39;, \u0026#39;percentages\u0026#39;, \u0026quot;cum_percentages\u0026quot;] ).head(number_of_rows) else: tb = pd.concat([counts, percentages], axis=1, keys=[\u0026#39;counts\u0026#39;, \u0026#39;percentages\u0026#39;]).head(number_of_rows) print(\u0026quot;Column %s with %s data type\u0026quot; % (col,data_frame[col].dtype), \u0026quot;\\n\u0026quot;, tb, \u0026quot;\\n\u0026quot;) Now we can apply our new function.\n Using a custom function pretty_value_counts(df) ## Column Unnamed: 0 with int64 data type ## counts percentages cum_percentages ## 2047 1 0.000015 0.000015 ## 41601 1 0.000015 0.000030 ## 21151 1 0.000015 0.000045 ## 23198 1 0.000015 0.000060 ## 17053 1 0.000015 0.000075 ## ## Column id with float64 data type ## counts percentages cum_percentages ## 1.100351e+10 2 0.000030 0.000030 ## 3.132701e+10 1 0.000015 0.000045 ## 1.501501e+10 1 0.000015 0.000060 ## 3.230631e+10 1 0.000015 0.000075 ## 5.003991e+10 1 0.000015 0.000090 ## ## Column idade with int64 data type ## counts percentages cum_percentages ## 20 2104 0.031653 0.031653 ## 28 2056 0.030931 0.062585 ## 26 2040 0.030691 0.093275 ## 22 2034 0.030600 0.123875 ## 27 2017 0.030345 0.154220 ## ## Column sexo with object data type ## counts percentages cum_percentages ## mulher 33607 0.505597 0.505597 ## homem 32791 0.493320 0.998917 ## gestante 72 0.001083 1.000000 ## ## Column anos_estudo with float64 data type ## counts percentages cum_percentages ## 5.0 23349 0.351271 0.351271 ## 11.0 16790 0.252595 0.603866 ## 15.0 5636 0.084790 0.688657 ## 8.0 5017 0.075478 0.764134 ## 10.0 2704 0.040680 0.804814 ## ## Column cor/raca with object data type ## counts percentages cum_percentages ## Branca 31689 0.476741 0.476741 ## Parda 28370 0.426809 0.903550 ## Preta 5249 0.078968 0.982518 ## Indigena 597 0.008981 0.991500 ## Amarela 323 0.004859 0.996359 ## ## Column salario with float64 data type ## counts percentages cum_percentages ## NaN 18592 0.279705 0.279705 ## 0.0 1841 0.027697 0.307402 ## -1.0 1101 0.016564 0.323966 ## 999999.0 367 0.005521 0.329487 ## 5229.0 277 0.004167 0.333654 ## ## Column estado_civil with float64 data type ## counts percentages cum_percentages ## 1.0 39066 0.587724 0.587724 ## 0.0 27404 0.412276 1.000000 ## ## Column regiao with object data type ## counts percentages cum_percentages ## sudeste 25220 0.379419 0.379419 ## centro-oeste 14702 0.221182 0.600602 ## norte 14653 0.220445 0.821047 ## sul 11890 0.178878 0.999925 ## nordeste 5 0.000075 1.000000 Just for comparison lets look how we could do the same thing without the function.\n for col in df: df[col].value_counts(dropna=False).head(5) ## 2047 1 ## 41601 1 ## 21151 1 ## 23198 1 ## 17053 1 ## Name: Unnamed: 0, dtype: int64 ## 1.100351e+10 2 ## 3.132701e+10 1 ## 1.501501e+10 1 ## 3.230631e+10 1 ## 5.003991e+10 1 ## Name: id, dtype: int64 ## 20 2104 ## 28 2056 ## 26 2040 ## 22 2034 ## 27 2017 ## Name: idade, dtype: int64 ## mulher 33607 ## homem 32791 ## gestante 72 ## Name: sexo, dtype: int64 ## 5.0 23349 ## 11.0 16790 ## 15.0 5636 ## 8.0 5017 ## 10.0 2704 ## Name: anos_estudo, dtype: int64 ## Branca 31689 ## Parda 28370 ## Preta 5249 ## Indigena 597 ## Amarela 323 ## Name: cor/raca, dtype: int64 ## NaN 18592 ## 0.0 1841 ## -1.0 1101 ## 999999.0 367 ## 5229.0 277 ## Name: salario, dtype: int64 ## 1.0 39066 ## 0.0 27404 ## Name: estado_civil, dtype: int64 ## sudeste 25220 ## centro-oeste 14702 ## norte 14653 ## sul 11890 ## nordeste 5 ## Name: regiao, dtype: int64   Replacing columns names The columns are named in Portuguese we can replace their names for English equivalents in a lot of different ways\ndf.columns ## Index([\u0026#39;Unnamed: 0\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;idade\u0026#39;, \u0026#39;sexo\u0026#39;, \u0026#39;anos_estudo\u0026#39;, \u0026#39;cor/raca\u0026#39;, ## \u0026#39;salario\u0026#39;, \u0026#39;estado_civil\u0026#39;, \u0026#39;regiao\u0026#39;], ## dtype=\u0026#39;object\u0026#39;) My favorite way of doing this sort of trades is using a dictionary defined outside the replace method, the cool thing about replace is that if we liked some of the column names previously defined we can simply omit them, for example, both “Unnamed: 0” and “id” are useless but since their names are already in English I don’t need to mess with them right now\n Translation discussion on race  There is some valid discussion on whether to translate “cor/raca” into ethnic_group or color_race, but I am personally on the opinion that the ones making this data frame in 1970 were probably under other standards of naming conventions and racism accusations so I will keep their naming scheme, I apologize if anyone feels offended by the use of these terms\n dict_cols = {\u0026quot;idade\u0026quot; : \u0026quot;age\u0026quot;, \u0026quot;sexo\u0026quot; : \u0026quot;sex\u0026quot;, \u0026quot;anos_estudo\u0026quot; : \u0026quot;years_study\u0026quot;, \u0026quot;cor/raca\u0026quot; : \u0026quot;color_race\u0026quot;, \u0026quot;salario\u0026quot; : \u0026quot;salary\u0026quot;, \u0026quot;estado_civil\u0026quot; : \u0026quot;civil_status\u0026quot;, \u0026quot;regiao\u0026quot; : \u0026quot;region\u0026quot; } df.rename(columns = dict_cols, inplace = True) Let’s see what changed\ndf.columns ## Index([\u0026#39;Unnamed: 0\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;sex\u0026#39;, \u0026#39;years_study\u0026#39;, \u0026#39;color_race\u0026#39;, \u0026#39;salary\u0026#39;, ## \u0026#39;civil_status\u0026#39;, \u0026#39;region\u0026#39;], ## dtype=\u0026#39;object\u0026#39;) It look fine now we can translate some of our main features\n Cleaning categorical data First we need to know the categories present in each of our columns a simple loop would fails us when we reached a numeric variable, the simplest way to solve that would be using an if statement, another alternative is using conditional execution, I personally don’t know a simple way of doing that in python but I will show it in the R post\nTo discover the numeric and “categorical” variables, know that sometimes you will have to change some elements of these lists but looking at my outputs I think I got all the relevant ones\nFinding which columns are categorical These are the numerical variables\ndf.select_dtypes(include=[np.number]).columns ## Index([\u0026#39;Unnamed: 0\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;years_study\u0026#39;, \u0026#39;salary\u0026#39;, \u0026#39;civil_status\u0026#39;], dtype=\u0026#39;object\u0026#39;) And these are the Categorical variables\nlist_cat = df.select_dtypes(exclude=[np.number]).columns Now we can run a simple loop\nfor col in list_cat: df[col].unique() ## array([\u0026#39;homem\u0026#39;, \u0026#39;mulher\u0026#39;, \u0026#39;gestante\u0026#39;], dtype=object) ## array([\u0026#39;Parda\u0026#39;, \u0026#39;Amarela\u0026#39;, \u0026#39;Indigena\u0026#39;, \u0026#39;Branca\u0026#39;, \u0026#39;Preta\u0026#39;, nan], ## dtype=object) ## array([\u0026#39;norte\u0026#39;, \u0026#39;nordeste\u0026#39;, \u0026#39;sudeste\u0026#39;, \u0026#39;sul\u0026#39;, \u0026#39;centro-oeste\u0026#39;], ## dtype=object) The simpler method is comparing the dtype in each column to the desired output, but this would be harder if we needed the np.numeric\nfor col in df: if df[col].dtype == \u0026quot;O\u0026quot;: df[col].unique() ## array([\u0026#39;homem\u0026#39;, \u0026#39;mulher\u0026#39;, \u0026#39;gestante\u0026#39;], dtype=object) ## array([\u0026#39;Parda\u0026#39;, \u0026#39;Amarela\u0026#39;, \u0026#39;Indigena\u0026#39;, \u0026#39;Branca\u0026#39;, \u0026#39;Preta\u0026#39;, nan], ## dtype=object) ## array([\u0026#39;norte\u0026#39;, \u0026#39;nordeste\u0026#39;, \u0026#39;sudeste\u0026#39;, \u0026#39;sul\u0026#39;, \u0026#39;centro-oeste\u0026#39;], ## dtype=object) The problem with the simpler approach is that sometimes you have columns that are categories and not objects so the simpler approach would fail when the more complex one would not, let’s convert sex to a category to prove my point\ndf.sex =df.sex.astype(\u0026quot;category\u0026quot;) df.dtypes ## Unnamed: 0 int64 ## id float64 ## age int64 ## sex category ## years_study float64 ## color_race object ## salary float64 ## civil_status float64 ## region object ## dtype: object for col in df: if df[col].dtype == \u0026quot;O\u0026quot;: df[col].unique() ## array([\u0026#39;Parda\u0026#39;, \u0026#39;Amarela\u0026#39;, \u0026#39;Indigena\u0026#39;, \u0026#39;Branca\u0026#39;, \u0026#39;Preta\u0026#39;, nan], ## dtype=object) ## array([\u0026#39;norte\u0026#39;, \u0026#39;nordeste\u0026#39;, \u0026#39;sudeste\u0026#39;, \u0026#39;sul\u0026#39;, \u0026#39;centro-oeste\u0026#39;], ## dtype=object) It does not work anymore, of course you can still solve this “problem” with the simpler approach by including a “and” clause on your if statement but at that point you might as well use the more extensible appoach\n Replacing values with an dictionary: 1 column After looking into the categories I can create a dictionary for each column if I want to be safe on repeating terms or I can pass a master dictionary for the whole data frame, I think the column by column approach is tidier but for each their own\ndict_sex = {\u0026quot;mulher\u0026quot; : \u0026quot;woman\u0026quot;, \u0026quot;homem\u0026quot; : \u0026quot;man\u0026quot;, \u0026quot;gestante\u0026quot; : \u0026quot;woman\u0026quot;} # pregnant This is one strange data frame, it probably made sense to split women into pregnant and not pregnant but I think it will only complicate the otherwise simple analyses so I will group both into “woman”\ndf.sex.replace(dict_sex,inplace = True) Showing the new amounts of women/mean\ndf.sex.value_counts() ## woman 33679 ## man 32791 ## Name: sex, dtype: int64 df.sex.unique() ## array([\u0026#39;man\u0026#39;, \u0026#39;woman\u0026#39;], dtype=object) This fails\npretty_value_counts(df.sex) ## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: \u0026#39;man\u0026#39; ## ## Detailed traceback: ## File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt; ## File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 6, in pretty_value_counts ## File \u0026quot;/home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/lib/python3.7/site-packages/pandas/core/series.py\u0026quot;, line 1071, in __getitem__ ## result = self.index.get_value(self, key) ## File \u0026quot;/home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u0026quot;, line 4730, in get_value ## return self._engine.get_value(s, k, tz=getattr(series.dtype, \u0026quot;tz\u0026quot;, None)) ## File \u0026quot;pandas/_libs/index.pyx\u0026quot;, line 80, in pandas._libs.index.IndexEngine.get_value ## File \u0026quot;pandas/_libs/index.pyx\u0026quot;, line 88, in pandas._libs.index.IndexEngine.get_value ## File \u0026quot;pandas/_libs/index.pyx\u0026quot;, line 128, in pandas._libs.index.IndexEngine.get_loc ## File \u0026quot;pandas/_libs/index_class_helper.pxi\u0026quot;, line 91, in pandas._libs.index.Int64Engine._check_type Here is actually a example on why I don’t personally enjoy Pandas conversion of data, the function that we created pretty_value_counts is not gonna work in this example because Pandas converts a single column to an Series object, so we would have to write a pretty_value_counts for Series as well or we would have to mess with the Pandas method or we could convert the series back into a DataFrame like this\npretty_value_counts(pd.DataFrame(data= df.sex)) ## Column sex with object data type ## counts percentages cum_percentages ## woman 33679 0.50668 0.50668 ## man 32791 0.49332 1.00000  Replacing values with an dictionary: multiple columns  Translation discussion on race part 2  Again there is relevant discussion on whether I should translate “Parda” as brown but basically Brazil’s population sometimes answers that their skin color is “Parda” = brown when asked about for many reasons I will propose two, “Preta” black can be used as an racist term so some people prefer to be called “brown”, the second explanation is that most of the population is actually pretty well integrated meaning that there a lot of biracial couples in this case we see something like “Preta” parent + “Branca” parent = “Parda” = in English “brown”.\nThere is also the case for the English equivalent of brown skin we simply use “Indiano” = “Indian”.\nCuriously the term “Negra” =~ \u0026quot;N*gger\u0026quot; is often preferred in Brazil, that may cause some confusion between Portuguese and English speakers.\nI will use brown but do notice that there were multiple sensible approaches here.\n This is a good opportunity to show failures in the master dictionary approach, realize that if I were to replace “nan” as no_answer or something like that python could thrown me an error because there are “nan” in some numerical columns such as salary but instead I get silence conversion of a numerical columns into object columns a dangerous feature.\nfor col in list_cat: df[col].unique() ## array([\u0026#39;man\u0026#39;, \u0026#39;woman\u0026#39;], dtype=object) ## array([\u0026#39;Parda\u0026#39;, \u0026#39;Amarela\u0026#39;, \u0026#39;Indigena\u0026#39;, \u0026#39;Branca\u0026#39;, \u0026#39;Preta\u0026#39;, nan], ## dtype=object) ## array([\u0026#39;norte\u0026#39;, \u0026#39;nordeste\u0026#39;, \u0026#39;sudeste\u0026#39;, \u0026#39;sul\u0026#39;, \u0026#39;centro-oeste\u0026#39;], ## dtype=object) dict_all = {\u0026quot;Parda\u0026quot; : \u0026quot;brown\u0026quot;, \u0026quot;Amarela\u0026quot; : \u0026quot;yellow\u0026quot;, \u0026quot;Indigena\u0026quot; : \u0026quot;indigenous\u0026quot;, \u0026quot;Branca\u0026quot; : \u0026quot;white\u0026quot;, \u0026quot;Preta\u0026quot; : \u0026quot;black\u0026quot;, np.nan : \u0026quot;no_answer\u0026quot;} df.replace(dict_all).salary.dtype ## dtype(\u0026#39;O\u0026#39;) dict_all = {\u0026quot;Parda\u0026quot; : \u0026quot;brown\u0026quot;, #col color_race \u0026quot;Amarela\u0026quot; : \u0026quot;yellow\u0026quot;, \u0026quot;Indigena\u0026quot; : \u0026quot;indigenous\u0026quot;, \u0026quot;Branca\u0026quot; : \u0026quot;white\u0026quot;, \u0026quot;Preta\u0026quot; : \u0026quot;black\u0026quot;, \u0026quot;norte\u0026quot; : \u0026quot;north\u0026quot;, # col region \u0026quot;nordeste\u0026quot; : \u0026quot;northeast\u0026quot;, \u0026quot;sudeste\u0026quot; : \u0026quot;southeast\u0026quot;, \u0026quot;sul\u0026quot; : \u0026quot;south\u0026quot;, \u0026quot;centro-oeste\u0026quot; : \u0026quot;midwest\u0026quot;} Let’s pray that we don’t have this problem and use this shared dictionary\ndf.replace(dict_all, inplace = True)  Did we correctly clean the Categorical Variables? Conversion of types Well not really I would argue that year_study is an categorical variable as well so let’s convert it.\ndf.years_study = df.years_study.astype(\u0026#39;category\u0026#39;) df.years_study.unique() ## [5.0, 8.0, 11.0, 15.0, 13.0, ..., 9.0, 10.0, 14.0, 12.0, NaN] ## Length: 12 ## Categories (11, float64): [5.0, 8.0, 11.0, 15.0, ..., 9.0, 10.0, 14.0, 12.0] Some nan but otherwise this is could be a useful feature, I will convert it back into a numerical column so that if we can easily impute the NaN’s based on a mathematical method such as the mean of the column.\ndf.years_study = df.years_study.astype(\u0026#39;interger\u0026#39;) ## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: data type \u0026#39;interger\u0026#39; not understood ## ## Detailed traceback: ## File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt; ## File \u0026quot;/home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/lib/python3.7/site-packages/pandas/core/generic.py\u0026quot;, line 5882, in astype ## dtype=dtype, copy=copy, errors=errors, **kwargs ## File \u0026quot;/home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/lib/python3.7/site-packages/pandas/core/internals/managers.py\u0026quot;, line 581, in astype ## return self.apply(\u0026quot;astype\u0026quot;, dtype=dtype, **kwargs) ## File \u0026quot;/home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/lib/python3.7/site-packages/pandas/core/internals/managers.py\u0026quot;, line 438, in apply ## applied = getattr(b, f)(**kwargs) ## File \u0026quot;/home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u0026quot;, line 559, in astype ## return self._astype(dtype, copy=copy, errors=errors, values=values, **kwargs) ## File \u0026quot;/home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u0026quot;, line 614, in _astype ## dtype = pandas_dtype(dtype) ## File \u0026quot;/home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u0026quot;, line 2055, in pandas_dtype ## raise TypeError(\u0026quot;data type \u0026#39;{}\u0026#39; not understood\u0026quot;.format(dtype)) Another numpy quirk you can’t use integers because there are NaN values.\ndf.years_study = df.years_study.astype(\u0026#39;float\u0026#39;) Converting civil_status into a category.\ndf.civil_status.unique() ## array([1., 0.]) To know what 1 or 0 mean, so we need to check the dictionary\ndict_civil_status = { 0. : \u0026quot;not_married\u0026quot;, 1. : \u0026quot;married\u0026quot;} df.civil_status = df.civil_status.replace(dict_civil_status) df.civil_status.head() ## 0 married ## 1 married ## 2 not_married ## 3 married ## 4 married ## Name: civil_status, dtype: object Before we deal with numerical variables I will get rid of ‘Unnamed: 0’ and ‘id’ features because they are useless in this case.\ndf.drop(columns=[\u0026#39;Unnamed: 0\u0026#39;, \u0026#39;id\u0026#39;],inplace=True)    Seeing the effects of categorical Variables We can use a colored barplot to see the interaction of these Categorical Variables with our Hypothesis.\nsns_plot = sns.catplot(x=\u0026quot;sex\u0026quot;, y=\u0026quot;salary\u0026quot;, hue=\u0026quot;region\u0026quot;, kind=\u0026quot;bar\u0026quot;, data=df) plt.show(sns_plot) sns_plot = sns.catplot(x=\u0026quot;sex\u0026quot;, y=\u0026quot;salary\u0026quot;, hue=\u0026quot;civil_status\u0026quot;, kind=\u0026quot;bar\u0026quot;, data=df) plt.show(sns_plot) sns_plot = sns.catplot(x=\u0026quot;sex\u0026quot;, y=\u0026quot;salary\u0026quot;, hue=\u0026quot;color_race\u0026quot;, kind=\u0026quot;bar\u0026quot;, data=df) plt.show(sns_plot)  Cleaning numerical data If we pull back the code that we used here are the numerical features of this dataset\ndf.select_dtypes(include=[np.number]).columns ## Index([\u0026#39;age\u0026#39;, \u0026#39;years_study\u0026#39;, \u0026#39;salary\u0026#39;], dtype=\u0026#39;object\u0026#39;) It is very common to reuse these kind of codes in Data Science scripts, so you shouldn’t fell as bad about repeating yourself as you do in other endeavors such in normal software engendering and you call always clean your analysis latter.\nIn order to know what to “clean” in numerical data I like to use plot such as a histogram\ndf.salary.hist(bins = 10) plt.show() Here we can see that the data may have a few outliers at 1000000 and that most of the salary data has a large Positive skew meaning that most data point are left to the mean of the dataset we can see that better using an density plot instead\nplot_density = df.salary.plot.kde() plot_density.set_xlim(0,100000) ## (0, 100000) plot_density #### Replacing variables {#python_custom_function_2}\nIf we go back to our custom function we can find that the values -1 and 999999 are unusually common after consulting the dictionary we decided to replace these values with the mean of the group.\nThis operation would be wrong for machine learning purposes since the mean of our train group would leak information from the test set as well but here in exploratory data analysis it is mostly fine also you need to replace the values with the numpy nan or else this operation doesn’t work as expected.\ndf_copy = df.copy() df_copy.salary.replace({-1: \u0026quot;NaN\u0026quot;,999999:\u0026#39;NaN\u0026#39;},inplace = True) df_copy.salary.fillna(df.salary.mean(),inplace= True) pretty_value_counts(pd.DataFrame(df_copy.salary)) ## Column salary with object data type ## counts percentages cum_percentages ## 19706.790323432902 18592 0.279705 0.279705 ## 0.0 1841 0.027697 0.307402 ## NaN 1468 0.022085 0.329487 ## 5229.0 277 0.004167 0.333654 ## 7200.0 260 0.003912 0.337566 # Create the new na values df.salary.replace({-1:np.nan,999999:np.nan},inplace = True) df.salary.fillna(df.salary.mean(),inplace= True) pretty_value_counts(pd.DataFrame(df.salary)) ## Column salary with float64 data type ## counts percentages cum_percentages ## 12422.39119 20060 0.301790 0.301790 ## 0.00000 1841 0.027697 0.329487 ## 5229.00000 277 0.004167 0.333654 ## 7200.00000 260 0.003912 0.337566 ## 7560.00000 244 0.003671 0.341237 And that is the magic of mutable Data Structures no extra assignments are required, quite useful, but be careful there is no going back if you haven’t saved a copy of your data.\nLog of numerical data There is also a statisticall solution for the Positive skew in our Data we can take the log of the salary column, but we will have to add one to all values since log of 0 goes to -Inf\ndf.log_salary = np.log1p(df.salary) ## /home/bruno-carlin/Documents/GIthub/TwoSidesData2/.venv/bin/activate_this.py:1: UserWarning: Pandas doesn\u0026#39;t allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access ## \u0026quot;\u0026quot;\u0026quot;Activate virtualenv for current interpreter: But be carefull you can’t assign in pandas using the . you need to use the “[” operator\ndf.log_salary ## 0 11.060384 ## 1 9.427336 ## 2 8.378713 ## 3 11.478344 ## 4 11.969090 ## ... ## 66465 9.427336 ## 66466 7.793999 ## 66467 7.793999 ## 66468 8.617075 ## 66469 6.134157 ## Name: salary, Length: 66470, dtype: float64 df.info() ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## RangeIndex: 66470 entries, 0 to 66469 ## Data columns (total 7 columns): ## age 66470 non-null int64 ## sex 66470 non-null object ## years_study 66036 non-null float64 ## color_race 66228 non-null object ## salary 66470 non-null float64 ## civil_status 66470 non-null object ## region 66470 non-null object ## dtypes: float64(2), int64(1), object(4) ## memory usage: 3.6+ MB It simply is gone\nThe right way\ndf[\u0026#39;log_salary\u0026#39;] = np.log1p(df.salary) pretty_value_counts(pd.DataFrame(df.log_salary)) ## Column salary with float64 data type ## counts percentages cum_percentages ## 9.427336 20060 0.301790 0.301790 ## 0.000000 1841 0.027697 0.329487 ## 8.562167 277 0.004167 0.333654 ## 8.881975 260 0.003912 0.337566 ## 8.930759 244 0.003671 0.341237 plot_density = df.log_salary.plot.kde(bw_method= 0.5) plot_density.set_xlim(0,15) ## (0, 15) plot_density It is now a usefull feature for most simple linear models\n Other numerical columns df.age.hist(bins = 20) plt.show() plot_density = df.age.plot.kde() plot_density pretty_value_counts(pd.DataFrame(df.age)) ## Column age with int64 data type ## counts percentages cum_percentages ## 20 2104 0.031653 0.031653 ## 28 2056 0.030931 0.062585 ## 26 2040 0.030691 0.093275 ## 22 2034 0.030600 0.123875 ## 27 2017 0.030345 0.154220 Age seems fine\nRemember from the the categorical variables we passed years_study here so that we could impute its missing values\ndf.info() ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## RangeIndex: 66470 entries, 0 to 66469 ## Data columns (total 8 columns): ## age 66470 non-null int64 ## sex 66470 non-null object ## years_study 66036 non-null float64 ## color_race 66228 non-null object ## salary 66470 non-null float64 ## civil_status 66470 non-null object ## region 66470 non-null object ## log_salary 66470 non-null float64 ## dtypes: float64(3), int64(1), object(4) ## memory usage: 4.1+ MB We are missing 66470 - 66036 = 434 observation, this is a small enough number that we decided to drop these rows\nWhile we are droping missing values lets drop the color_race missing observations as well\ndf.dropna(subset = [\u0026quot;years_study\u0026quot;,\u0026quot;color_race\u0026quot;],inplace= True) df.info() ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## Int64Index: 65795 entries, 0 to 66469 ## Data columns (total 8 columns): ## age 65795 non-null int64 ## sex 65795 non-null object ## years_study 65795 non-null float64 ## color_race 65795 non-null object ## salary 65795 non-null float64 ## civil_status 65795 non-null object ## region 65795 non-null object ## log_salary 65795 non-null float64 ## dtypes: float64(3), int64(1), object(4) ## memory usage: 4.5+ MB Checking on year_study\ndf.years_study.hist(bins = 20) plt.show() Let’s convert it back into a Category\ndf.years_study = df.years_study.astype(\u0026#39;category\u0026#39;)     Saving our work for later Here we have many options we can for example run this script later or save this modified df as a csv, both options are okay but I will promote the usage of an Data format that keeps the mindful choices of encoding that we made into consideration, there are many alternatives in this case as well but I will use feather.\nIt is also always a good idea to separate the Data from the script if you want reproducible work, that is where Excel mostly fails for me.\nSo showing our Data Types\ndf.dtypes ## age int64 ## sex object ## years_study category ## color_race object ## salary float64 ## civil_status object ## region object ## log_salary float64 ## dtype: object Using csv will may lose some Data Types\ndf.to_csv(r.file_path_linux + \u0026#39;/finished_work.csv\u0026#39;) pd.read_csv(r.file_path_linux + \u0026#39;/finished_work.csv\u0026#39;).dtypes ## Unnamed: 0 int64 ## age int64 ## sex object ## years_study float64 ## color_race object ## salary float64 ## civil_status object ## region object ## log_salary float64 ## dtype: object We lost our encoding of years_study and when writing a csv we made this useless to us Unnamed: 0 column\na better way is using the feather file format, you need to pip install pyarrow beforehand\nr.file_path_linux ## \u0026#39;/home/bruno-carlin/Documents/GIthub/TwoSidesData2/content/post/data\u0026#39; df.reset_index().to_feather(r.file_path_linux + \u0026#39;/sex_thesis_assignment.feather\u0026#39;) file_path_linux ## [1] \u0026quot;/home/bruno-carlin/Documents/GIthub/TwoSidesData2/content/post/data\u0026quot; pd.read_feather(r.file_path_linux + \u0026#39;/sex_thesis_assignment.feather\u0026#39;).dtypes ## index int64 ## age int64 ## sex object ## years_study category ## color_race object ## salary float64 ## civil_status object ## region object ## log_salary float64 ## dtype: object Feather does keep the years study dtype, but feather is still in a experimental phase so be carefull with it, parquet unfortunally fails to keep the dtypes I don’t know why.\nIt is also a good idea to keep good file names so that you can easily identify your datasets and scripts.\nIf you then need to delete these files you can do it inside python\nos.remove(r.file_path_linux + \u0026#39;/finished_work.csv\u0026#39;) #os.remove(r.file_path_linux + \u0026#39;/sex_thesis_assignment.feather\u0026#39;)  Next post In the next post I will show the end of the analysis and the “answer” to our hypothesis.\n ","date":1579392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579392000,"objectID":"c2f49cfb9158030459eb0971bd055b44","permalink":"/2020/01/19/exploratory-data-analysis-basics-part1/","publishdate":"2020-01-19T00:00:00Z","relpermalink":"/2020/01/19/exploratory-data-analysis-basics-part1/","section":"post","summary":"Basics exploratory Data Analysis: Part 1 of 4","tags":["R Markdown","reticulate","pandas"],"title":"exploratory data analysis: basics Python part 1","type":"post"},{"authors":["Bruno Carlin"],"categories":["r-project"],"content":"  Turning strings into numbers Reframing the problem Calculating occupied and available minutes Filtering lists Calculating start and end minutes Converting minutes back into readable hours Pairing Start and End Hours Big O problem and Data Science   So to start it all, I learned about this question from the recommended youtube channel Clément Mihailescu in his youtube video \nIn this video both Clément and Tim from Tech with Tim work together to solve this question:\nSuppose you have two People that want to Schedule a meeting, how would you schedule it, if each Person has already set up meetings in this day and that each Person has different working hours\nTo help us understand it the problem provided us with this example data\nPerson 1 has three meetings 9:00 to 10:30, 12:00 to 13:00 and 16:00 to 18:00, this Person works from 9:00 to 20:00\nPerson 2 also has three meetings 10:00 to 11:30, 12:30 to 14:30 and 18:00 to 18:30 this Person works from 10:00 to 18:30\nFor those just starting out with R and the tidyverse I will explain each part in detail in the collapsible parts of the post.\nSo coding this info in R we have\nlibrary(tidyverse) person1 \u0026lt;- list(list(\u0026#39;9:00\u0026#39;, \u0026#39;10:30\u0026#39;), list(\u0026#39;12:00\u0026#39;, \u0026#39;13:00\u0026#39;), list(\u0026#39;16:00\u0026#39;, \u0026#39;18:00\u0026#39;)) allowed_time1 \u0026lt;- list(list(\u0026#39;9:00\u0026#39;,\u0026#39;20:00\u0026#39;)) person2 \u0026lt;- list(list(\u0026#39;10:00\u0026#39;, \u0026#39;11:30\u0026#39;), list(\u0026#39;12:30\u0026#39;, \u0026#39;14:30\u0026#39;), list(\u0026#39;18:00\u0026#39;, \u0026#39;18:30\u0026#39;)) allowed_time2 \u0026lt;- list(list(\u0026#39;10:00\u0026#39;,\u0026#39;18:30\u0026#39;)) If you see the video Tim was able to solve it in 45 minutes while having spent a good amount of the time talking back and forth with his interviewer and explaining his reasoning, that was impressive.\nWell I failed to finish this problem in 45 minutes, in fact it took close to 3 hours to stitch together the solution I am about to show, but I think my solution is something that I am proud of and that it follows most of what I love about functional programming.\nTurning strings into numbers I saw that Tim created a function to compare time in his program\nCreating a function that calculates the amount of minutes in a string such as ‘10:30’\ncalculate_minutes \u0026lt;- function(x) { x \u0026lt;- str_split(x,pattern = \u0026quot;:\u0026quot;) as.integer(x[[1]][[1]]) * 60 + as.integer(x[[1]][[2]]) } calculate_minutes(\u0026#39;10:30\u0026#39;) ## [1] 630  explanation on calculate minutes calculate_minutes splits the string ‘10:30’ into ‘10’ and ‘30’ and then multiples the left hand side by 60, because each hour has 60 minutes in it and then adds the right hand side the minutes to the result 630 = 10 * 60 + 30 minutes.\n  Reframing the problem While at first the calculate_minutes may seem useless in both r and python since ‘14:30’ \u0026lt; “10:30” will return FALSE/False\nWe can reframe the focus on minutes overlapping of each appointment instead of time comparison like Tim used is his solution.\nSo based on this new idea, I created a function to convert the the strings we received into ranges of minutes, basically the interval of each appointment.\nWe will also need the interval of the full Day\nfull_day \u0026lt;- 1:(24*60) # 24 hours * 60 minutes -\u0026gt; range 1: to result Here is the function that applies Step One and Two\ncalculate_interval \u0026lt;- . %\u0026gt;% map(calculate_minutes) %\u0026gt;% reduce(seq) example_appointment \u0026lt;- list(\u0026#39;10:30\u0026#39;,\u0026#39;10:40\u0026#39;) example_appointment %\u0026gt;% calculate_interval ## [1] 630 631 632 633 634 635 636 637 638 639 640  explanation on calculate_interval I use “.” as a shortcut for function(x) {} it is really useful in function pipes like this one\nmap is a function that applies another function to all elements of an list and returns ideally the same number of elements like this,\nexample_list \u0026lt;- list(list(\u0026#39;hi\u0026#39;,\u0026#39;johnny\u0026#39;),list(\u0026#39;how\u0026#39;,\u0026#39;are\u0026#39;,\u0026#39;you\u0026#39;)) map(example_list,.f = str_to_upper) ## [[1]] ## [1] \u0026quot;HI\u0026quot; \u0026quot;JOHNNY\u0026quot; ## ## [[2]] ## [1] \u0026quot;HOW\u0026quot; \u0026quot;ARE\u0026quot; \u0026quot;YOU\u0026quot; I also use pipes (%\u0026gt;%), the pipes allow us to change the nested nature of function calls into a sequential one for example\nexample_list %\u0026gt;% map(str_to_title) ## [[1]] ## [1] \u0026quot;Hi\u0026quot; \u0026quot;Johnny\u0026quot; ## ## [[2]] ## [1] \u0026quot;How\u0026quot; \u0026quot;Are\u0026quot; \u0026quot;You\u0026quot; And finally I use another core function of functional programming reduce, reduce works by applying the same function in a list until the is only one element left for example\nreduce(1:4,sum) ## [1] 10  We then need to use this function on all of our info\nperson1_interval \u0026lt;- person1 %\u0026gt;% map(calculate_interval) person2_interval \u0026lt;- person2 %\u0026gt;% map(calculate_interval) allowed_time1_interval \u0026lt;- allowed_time1 %\u0026gt;% map(calculate_interval) allowed_time2_interval \u0026lt;- allowed_time2 %\u0026gt;% map(calculate_interval)  example Person1\nperson1 %\u0026gt;% map(calculate_interval) ## [[1]] ## [1] 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 ## [20] 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 ## [39] 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 ## [58] 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 ## [77] 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 ## ## [[2]] ## [1] 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 ## [20] 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 ## [39] 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 ## [58] 777 778 779 780 ## ## [[3]] ## [1] 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 ## [16] 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 ## [31] 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 ## [46] 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 ## [61] 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 ## [76] 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 ## [91] 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 ## [106] 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 ## [121] 1080   Calculating occupied and available minutes This is a simple step we collapse the lists into one\nschedule_1_occupied \u0026lt;- person1_interval \u0026lt;- person1_interval %\u0026gt;% reduce(c) schedule_1_avalaible \u0026lt;- allowed_time1_interval %\u0026gt;% reduce(c) schedule_2_occupied \u0026lt;- person2_interval %\u0026gt;% reduce(c) schedule_2_avalaible \u0026lt;- allowed_time2_interval %\u0026gt;% reduce(c)  example reduce c Here is an example of how to collapse a simple list\nlist(c(1,3,4),c(2,5)) %\u0026gt;% reduce(c) ## [1] 1 3 4 2 5   Filtering lists Now finally to our last core functional language function (filter), in the tidyverse filter was split into two functions discard and keep, mostly because the actual filter function is used in dplyr\nWe can start with the full_day and take away occupied minutes and keep valid minutes, all using filter.\npossible_minutes \u0026lt;- full_day %\u0026gt;% discard(~ .x %in% c(schedule_1_occupied,schedule_2_occupied)) %\u0026gt;% keep(~ .x %in% schedule_1_avalaible) %\u0026gt;% keep(~ .x %in% schedule_2_avalaible) possible_minutes ## [1] 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 ## [19] 709 710 711 712 713 714 715 716 717 718 719 871 872 873 874 875 876 877 ## [37] 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 ## [55] 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 ## [73] 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 ## [91] 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 ## [109] 950 951 952 953 954 955 956 957 958 959  explanation filter filter works by receiving an vector or list and return only elements that have passed or failed a test for example\ndiscard(1:10, ~ . \u0026gt; 5) ## [1] 1 2 3 4 5 keep(1:10, ~ . \u0026gt; 5) ## [1] 6 7 8 9 10  Keep in mind that while discarding we can evaluate everything together and the result stays the same, but when keeping it is important in this case to separate the call into two\n Calculating start and end minutes This function solves the problem that we as human prefer to receive just the start and end minutes instead of the whole duration\ncalculate_break_time \u0026lt;- function(intergers){ intergers \u0026lt;- sort(intergers) end \u0026lt;- time \u0026lt;- which(diff(intergers) != 1) begin \u0026lt;- end + 1 intergers[c(1,end,begin,length(intergers)) %\u0026gt;% sort()] }  explanation calculate_break_time This is the function that I am least happy, but basically the two minutes that we know we will need are the smallest minute that will start the first appointment and the last minute which will end the last appointment The in between minutes are found by looking for jumps between minutes using the diff function\ndiff(c(1,3,4,5,9)) ## [1] 2 1 1 4 If there is more than one minute of difference it is the end of an appointment and one minute later there will be the start of a new appointment\n start_end_minutes \u0026lt;- possible_minutes %\u0026gt;% calculate_break_time() start_end_minutes ## [1] 691 719 871 959  Converting minutes back into readable hours A simple function that undoes our transformation if you need to read %/% explanation\nturn_back_into_time \u0026lt;- function(x) { x_hour \u0026lt;- x %/% 60 x_minute \u0026lt;- x %% 60 str_c(x_hour,x_minute,sep = \u0026#39;:\u0026#39;) } (readable_time \u0026lt;- start_end_minutes %\u0026gt;% turn_back_into_time) ## [1] \u0026quot;11:31\u0026quot; \u0026quot;11:59\u0026quot; \u0026quot;14:31\u0026quot; \u0026quot;15:59\u0026quot;  Pairing Start and End Hours I will over complicate the flagging of even and odd numbers to show another really cool functional concept of currying, while currying is not encouraged by the tidyverse the partial functions does make it pretty easy to use (same as Python)\nThe other cool concept is that you can negate a function, in this case turning the results of is_even into is_odd\ndiviseble_by \u0026lt;- function(number_vector,divisor,quocient) { number_vector %% divisor == quocient } is_even \u0026lt;- partial(diviseble_by,divisor = 2,quocient = 0) is_odd \u0026lt;- is_even %\u0026gt;% negate() 1:10 %\u0026gt;% is_even ## [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE 1:10 %\u0026gt;% is_odd ## [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE We then use these functions to unite our start and end minutes\npair_wise_combination \u0026lt;- function(character_vector){ vector_i \u0026lt;- seq_along(character_vector) ends \u0026lt;- vector_i %\u0026gt;% is_even begins \u0026lt;- vector_i %\u0026gt;% is_odd str_c(character_vector[begins],character_vector[ends],sep = \u0026quot; \u0026quot;) } readable_time %\u0026gt;% pair_wise_combination ## [1] \u0026quot;11:31 11:59\u0026quot; \u0026quot;14:31 15:59\u0026quot; That is all folks.\nMy answer also works for n people\nI can answer questions anywhere, please do share it if you have enjoyed it.\n Big O problem and Data Science I also failed because, I would have blankly stared into the interviewer face for a while before admitting that I have no idea the Big O of this answer is\nWhile I do understand big O notation and its importance, I am not a Software Engineer nor a Computer Scientist, I have no idea how efficient my solution is, I know it is fast enough for me, but I understand that big O knowledge is a major difference while learning DS compared to the usual programming paths, rarely if ever people mention the Big O of our algorithms so I never deeply studied about the subject.\n ","date":1578787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578787200,"objectID":"40e2476ab8438866e2c5a95e21cdde17","permalink":"/2020/01/12/google-interview-question-in-r/","publishdate":"2020-01-12T00:00:00Z","relpermalink":"/2020/01/12/google-interview-question-in-r/","section":"post","summary":"Google questions are hard","tags":["tidyverse","programming"],"title":"Google interview question in R - Calendar","type":"post"},{"authors":null,"categories":["r-project"],"content":" FizzBuzz is and old kids games\nNot that popular where I am from Brazil, Fizz Buzz has a simple set of rules\nYou start counting from 1 (obviously) and when a number is a multiple of 3 you say Fizz,\nif the number is a multiple of 5 you say Buzz,\nand if the number is a multiple of both you shout FizzBuzz, And for every other case you can say the number itself, simple right?\nI watched this really cool video on Tom Scott channel and realized that I have never attempted this problem as a programmer\nThis is an blog post full of tricks I will try to point them all out.\nScott’s video\nNaive FizzBuzz Naive FizzBuzz\nfor (i in 1:15){ if(i%%3 == 0 \u0026amp; i%%5 == 0) { print(\u0026#39;FizzBuzz\u0026#39;) } else if(i%%3 == 0) { print(\u0026#39;Fizz\u0026#39;) } else if (i%%5 == 0){ print(\u0026#39;Buzz\u0026#39;) } else { print(i) } } ## [1] 1 ## [1] 2 ## [1] \u0026quot;Fizz\u0026quot; ## [1] 4 ## [1] \u0026quot;Buzz\u0026quot; ## [1] \u0026quot;Fizz\u0026quot; ## [1] 7 ## [1] 8 ## [1] \u0026quot;Fizz\u0026quot; ## [1] \u0026quot;Buzz\u0026quot; ## [1] 11 ## [1] \u0026quot;Fizz\u0026quot; ## [1] 13 ## [1] 14 ## [1] \u0026quot;FizzBuzz\u0026quot; Simple flow control with if, else statements Some basic operators ($, ==)   Extending the loop approach Using Scott’s approach we can improve a bit on the logic\nfor (i in 1:15){ current_out \u0026lt;- \u0026#39;\u0026#39; if(i%%3 == 0) { current_out \u0026lt;- paste0(current_out,\u0026#39;Fizz\u0026#39;) } if (i%%5 == 0){ current_out \u0026lt;- paste0(current_out,\u0026#39;Buzz\u0026#39;) } if (current_out == \u0026#39;\u0026#39;){ print(i) } else print(current_out) } ## [1] 1 ## [1] 2 ## [1] \u0026quot;Fizz\u0026quot; ## [1] 4 ## [1] \u0026quot;Buzz\u0026quot; ## [1] \u0026quot;Fizz\u0026quot; ## [1] 7 ## [1] 8 ## [1] \u0026quot;Fizz\u0026quot; ## [1] \u0026quot;Buzz\u0026quot; ## [1] 11 ## [1] \u0026quot;Fizz\u0026quot; ## [1] 13 ## [1] 14 ## [1] \u0026quot;FizzBuzz\u0026quot; While it is possible to improve open this loop, I think it already is close to the limits of what I would call a very simple example\n Functional approach Thanks Functional FizzBuzz\ndivisor \u0026lt;- function(number, string) { function(d) { if (d %% number == 0) string else \u0026quot;\u0026quot; } } mod3er \u0026lt;- divisor(3, \u0026quot;Fizz\u0026quot;) mod5er \u0026lt;- divisor(5, \u0026quot;Buzz\u0026quot;) fizzbuzz \u0026lt;- function(i) { res \u0026lt;- paste0(mod3er(i), mod5er(i)) ifelse(res == \u0026quot;\u0026quot;, i, res) } sapply(1:15, fizzbuzz) ## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;Fizz\u0026quot; \u0026quot;4\u0026quot; \u0026quot;Buzz\u0026quot; \u0026quot;Fizz\u0026quot; ## [7] \u0026quot;7\u0026quot; \u0026quot;8\u0026quot; \u0026quot;Fizz\u0026quot; \u0026quot;Buzz\u0026quot; \u0026quot;11\u0026quot; \u0026quot;Fizz\u0026quot; ## [13] \u0026quot;13\u0026quot; \u0026quot;14\u0026quot; \u0026quot;FizzBuzz\u0026quot; So enumerating the new concepts here:\nFunctions that create functions (mod3er,mod5er) Functions that create functions that create functions (divisor) Applying functions (sapply) Functional if else (I prefer it)  All of which seen pretty complicated at first but will pay off big time latter.\n My approach (tidyverse) The Basics Loading the tidyverse\nlibrary(tidyverse) ## -- Attaching packages ---------------------------------------------------------------------------------------------------------------------------------------------- tidyverse 1.3.0 -- ## \u0026lt;U+2713\u0026gt; ggplot2 3.2.1 \u0026lt;U+2713\u0026gt; purrr 0.3.3 ## \u0026lt;U+2713\u0026gt; tibble 2.1.3 \u0026lt;U+2713\u0026gt; dplyr 0.8.3 ## \u0026lt;U+2713\u0026gt; tidyr 1.0.0 \u0026lt;U+2713\u0026gt; stringr 1.4.0 ## \u0026lt;U+2713\u0026gt; readr 1.3.1 \u0026lt;U+2713\u0026gt; forcats 0.4.0 ## -- Conflicts ------------------------------------------------------------------------------------------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() divisor \u0026lt;- function(number, string) { function(input) { if_else(condition = input %% number == 0, true = string, false = \u0026quot;\u0026quot;) } } mod3 \u0026lt;- divisor(3, \u0026quot;Fizz\u0026quot;) mod5 \u0026lt;- divisor(5, \u0026quot;Buzz\u0026quot;) list_functions \u0026lt;- list(mod3,mod5) mapper_list \u0026lt;- function(i,list_functions) map(list_functions, exec,i) map(1:15,mapper_list,list_functions) %\u0026gt;% map(reduce,str_c) ## [[1]] ## [1] \u0026quot;\u0026quot; ## ## [[2]] ## [1] \u0026quot;\u0026quot; ## ## [[3]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[4]] ## [1] \u0026quot;\u0026quot; ## ## [[5]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[6]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[7]] ## [1] \u0026quot;\u0026quot; ## ## [[8]] ## [1] \u0026quot;\u0026quot; ## ## [[9]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[10]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[11]] ## [1] \u0026quot;\u0026quot; ## ## [[12]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[13]] ## [1] \u0026quot;\u0026quot; ## ## [[14]] ## [1] \u0026quot;\u0026quot; ## ## [[15]] ## [1] \u0026quot;FizzBuzz\u0026quot; We learned two new tricks:\nExecuting a list of functions using exec reducing an list   Making just one call fancy \u0026lt;- function(i,...) { list_functions \u0026lt;- list(...) mapper_list \u0026lt;- function(i,list_functions) map(list_functions, exec,i) map(i,mapper_list,list_functions) %\u0026gt;% map(reduce,str_c) } fancy(1:15,mod3,mod5) ## [[1]] ## [1] \u0026quot;\u0026quot; ## ## [[2]] ## [1] \u0026quot;\u0026quot; ## ## [[3]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[4]] ## [1] \u0026quot;\u0026quot; ## ## [[5]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[6]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[7]] ## [1] \u0026quot;\u0026quot; ## ## [[8]] ## [1] \u0026quot;\u0026quot; ## ## [[9]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[10]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[11]] ## [1] \u0026quot;\u0026quot; ## ## [[12]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[13]] ## [1] \u0026quot;\u0026quot; ## ## [[14]] ## [1] \u0026quot;\u0026quot; ## ## [[15]] ## [1] \u0026quot;FizzBuzz\u0026quot; One new trick using ellipsis\n Or preparing for an api api_less_fancy \u0026lt;- function(i,list_functions) { mapper_list \u0026lt;- function(i,list_functions) map(list_functions, exec,i) map(i,mapper_list,list_functions) %\u0026gt;% map(reduce,str_c) } api_less_fancy(1:15,list(mod3,mod5)) ## [[1]] ## [1] \u0026quot;\u0026quot; ## ## [[2]] ## [1] \u0026quot;\u0026quot; ## ## [[3]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[4]] ## [1] \u0026quot;\u0026quot; ## ## [[5]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[6]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[7]] ## [1] \u0026quot;\u0026quot; ## ## [[8]] ## [1] \u0026quot;\u0026quot; ## ## [[9]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[10]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[11]] ## [1] \u0026quot;\u0026quot; ## ## [[12]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[13]] ## [1] \u0026quot;\u0026quot; ## ## [[14]] ## [1] \u0026quot;\u0026quot; ## ## [[15]] ## [1] \u0026quot;FizzBuzz\u0026quot;   Extending FizzBuzz Let’s see how easy it is too make the game more difficult:\nChanging names mod3n \u0026lt;- divisor(3, \u0026quot;Buzz\u0026quot;) mod5n \u0026lt;- divisor(5,\u0026#39;Fizz\u0026#39;) fancy(1:15,mod3n,mod5n) ## [[1]] ## [1] \u0026quot;\u0026quot; ## ## [[2]] ## [1] \u0026quot;\u0026quot; ## ## [[3]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[4]] ## [1] \u0026quot;\u0026quot; ## ## [[5]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[6]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[7]] ## [1] \u0026quot;\u0026quot; ## ## [[8]] ## [1] \u0026quot;\u0026quot; ## ## [[9]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[10]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[11]] ## [1] \u0026quot;\u0026quot; ## ## [[12]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[13]] ## [1] \u0026quot;\u0026quot; ## ## [[14]] ## [1] \u0026quot;\u0026quot; ## ## [[15]] ## [1] \u0026quot;BuzzFizz\u0026quot;  Adding divisors mod2 \u0026lt;- divisor(2, \u0026quot;Deuce\u0026quot;) fancy(1:30,mod2,mod3,mod5) ## [[1]] ## [1] \u0026quot;\u0026quot; ## ## [[2]] ## [1] \u0026quot;Deuce\u0026quot; ## ## [[3]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[4]] ## [1] \u0026quot;Deuce\u0026quot; ## ## [[5]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[6]] ## [1] \u0026quot;DeuceFizz\u0026quot; ## ## [[7]] ## [1] \u0026quot;\u0026quot; ## ## [[8]] ## [1] \u0026quot;Deuce\u0026quot; ## ## [[9]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[10]] ## [1] \u0026quot;DeuceBuzz\u0026quot; ## ## [[11]] ## [1] \u0026quot;\u0026quot; ## ## [[12]] ## [1] \u0026quot;DeuceFizz\u0026quot; ## ## [[13]] ## [1] \u0026quot;\u0026quot; ## ## [[14]] ## [1] \u0026quot;Deuce\u0026quot; ## ## [[15]] ## [1] \u0026quot;FizzBuzz\u0026quot; ## ## [[16]] ## [1] \u0026quot;Deuce\u0026quot; ## ## [[17]] ## [1] \u0026quot;\u0026quot; ## ## [[18]] ## [1] \u0026quot;DeuceFizz\u0026quot; ## ## [[19]] ## [1] \u0026quot;\u0026quot; ## ## [[20]] ## [1] \u0026quot;DeuceBuzz\u0026quot; ## ## [[21]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[22]] ## [1] \u0026quot;Deuce\u0026quot; ## ## [[23]] ## [1] \u0026quot;\u0026quot; ## ## [[24]] ## [1] \u0026quot;DeuceFizz\u0026quot; ## ## [[25]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[26]] ## [1] \u0026quot;Deuce\u0026quot; ## ## [[27]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[28]] ## [1] \u0026quot;Deuce\u0026quot; ## ## [[29]] ## [1] \u0026quot;\u0026quot; ## ## [[30]] ## [1] \u0026quot;DeuceFizzBuzz\u0026quot;  Adding new rules less \u0026lt;- function(number, string) { function(input) { if_else(condition = input \u0026lt; number, true = string, false = \u0026quot;\u0026quot;) } } less10 \u0026lt;- less(10,\u0026quot;Small\u0026quot;) fancy(1:15,less10,mod3,mod5) ## [[1]] ## [1] \u0026quot;Small\u0026quot; ## ## [[2]] ## [1] \u0026quot;Small\u0026quot; ## ## [[3]] ## [1] \u0026quot;SmallFizz\u0026quot; ## ## [[4]] ## [1] \u0026quot;Small\u0026quot; ## ## [[5]] ## [1] \u0026quot;SmallBuzz\u0026quot; ## ## [[6]] ## [1] \u0026quot;SmallFizz\u0026quot; ## ## [[7]] ## [1] \u0026quot;Small\u0026quot; ## ## [[8]] ## [1] \u0026quot;Small\u0026quot; ## ## [[9]] ## [1] \u0026quot;SmallFizz\u0026quot; ## ## [[10]] ## [1] \u0026quot;Buzz\u0026quot; ## ## [[11]] ## [1] \u0026quot;\u0026quot; ## ## [[12]] ## [1] \u0026quot;Fizz\u0026quot; ## ## [[13]] ## [1] \u0026quot;\u0026quot; ## ## [[14]] ## [1] \u0026quot;\u0026quot; ## ## [[15]] ## [1] \u0026quot;FizzBuzz\u0026quot; That is it have a great day.\n  ","date":1576972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576972800,"objectID":"44816a2105949ea87250f6f2cd17420e","permalink":"/2019/12/22/fizzbuzz-in-the-tidyverse/","publishdate":"2019-12-22T00:00:00Z","relpermalink":"/2019/12/22/fizzbuzz-in-the-tidyverse/","section":"post","summary":"FizzBuzz is and old kids games\nNot that popular where I am from Brazil, Fizz Buzz has a simple set of rules\nYou start counting from 1 (obviously) and when a number is a multiple of 3 you say Fizz,\nif the number is a multiple of 5 you say Buzz,\nand if the number is a multiple of both you shout FizzBuzz, And for every other case you can say the number itself, simple right?","tags":["tidyverse","programming"],"title":"FizzBuzz in the tidyverse","type":"post"},{"authors":["Bruno Carlin"],"categories":["r-project"],"content":"      Breast Cancer problem.\nThis is a problem that I have trid to solve using just the old tidymodels package and got stuck so here is the new implementation using the amazing tune and workflows packages\nSetting up Rmarkdown root.dir =\n Loading Libraries library(tidyverse) ## -- Attaching packages ---------------------------------------------------------------------------------------------------------------------------------------------- tidyverse 1.3.0 -- ## \u0026lt;U+2713\u0026gt; ggplot2 3.2.1 \u0026lt;U+2713\u0026gt; purrr 0.3.3 ## \u0026lt;U+2713\u0026gt; tibble 2.1.3 \u0026lt;U+2713\u0026gt; dplyr 0.8.3 ## \u0026lt;U+2713\u0026gt; tidyr 1.0.0 \u0026lt;U+2713\u0026gt; stringr 1.4.0 ## \u0026lt;U+2713\u0026gt; readr 1.3.1 \u0026lt;U+2713\u0026gt; forcats 0.4.0 ## -- Conflicts ------------------------------------------------------------------------------------------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(tidymodels) ## Registered S3 method overwritten by \u0026#39;xts\u0026#39;: ## method from ## as.zoo.xts zoo ## -- Attaching packages --------------------------------------------------------------------------------------------------------------------------------------------- tidymodels 0.0.3 -- ## \u0026lt;U+2713\u0026gt; broom 0.5.3 \u0026lt;U+2713\u0026gt; recipes 0.1.8 ## \u0026lt;U+2713\u0026gt; dials 0.0.4 \u0026lt;U+2713\u0026gt; rsample 0.0.5 ## \u0026lt;U+2713\u0026gt; infer 0.5.1 \u0026lt;U+2713\u0026gt; yardstick 0.0.4 ## \u0026lt;U+2713\u0026gt; parsnip 0.0.4 ## -- Conflicts ------------------------------------------------------------------------------------------------------------------------------------------------ tidymodels_conflicts() -- ## x scales::discard() masks purrr::discard() ## x dplyr::filter() masks stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x dials::margin() masks ggplot2::margin() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() ## x recipes::yj_trans() masks scales::yj_trans() library(janitor) ## ## Attaching package: \u0026#39;janitor\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## chisq.test, fisher.test library(skimr) library(DataExplorer) Loading Libraries: 3.65 sec elapsed\n Getting Data Got the dataset with headers on kaggle link, there is also a cool explanation about the problem there.\ndf \u0026lt;- read_csv(\u0026quot;breast_cancer.csv\u0026quot;) ## Warning: Missing column names filled in: \u0026#39;X33\u0026#39; [33] ## Parsed with column specification: ## cols( ## .default = col_double(), ## diagnosis = col_character(), ## X33 = col_character() ## ) ## See spec(...) for full column specifications. ## Warning: 569 parsing failures. ## row col expected actual file ## 1 -- 33 columns 32 columns \u0026#39;breast_cancer.csv\u0026#39; ## 2 -- 33 columns 32 columns \u0026#39;breast_cancer.csv\u0026#39; ## 3 -- 33 columns 32 columns \u0026#39;breast_cancer.csv\u0026#39; ## 4 -- 33 columns 32 columns \u0026#39;breast_cancer.csv\u0026#39; ## 5 -- 33 columns 32 columns \u0026#39;breast_cancer.csv\u0026#39; ## ... ... .......... .......... ................... ## See problems(...) for more details. Set Chunk: 0.16 sec elapsed\nThere is a strange extra column named X33 dealing with that using janitor package\ndf \u0026lt;- df %\u0026gt;% janitor::remove_empty_cols() ## Warning: \u0026#39;janitor::remove_empty_cols\u0026#39; is deprecated. ## Use \u0026#39;remove_empty(\u0026quot;cols\u0026quot;)\u0026#39; instead. ## See help(\u0026quot;Deprecated\u0026quot;) Cleaning Data: 0.01 sec elapsed\n Visualizing the data using DataExplorer and Skimr Skimr is a fast way to get info on your data even though the hist plot fails on my blog :( df %\u0026gt;% skimr::skim()  Table 1: Data summary  Name Piped data  Number of rows 569  Number of columns 32  _______________________   Column type frequency:   character 1  numeric 31  ________________________   Group variables None    Variable type: character\n  skim_variable n_missing complete_rate min max empty n_unique whitespace    diagnosis 0 1 1 1 0 2 0    Variable type: numeric\n  skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist    id 0 1 30371831.43 125020585.61 8670.00 869218.00 906024.00 8813129.00 911320502.00 ▇▁▁▁▁  radius_mean 0 1 14.13 3.52 6.98 11.70 13.37 15.78 28.11 ▂▇▃▁▁  texture_mean 0 1 19.29 4.30 9.71 16.17 18.84 21.80 39.28 ▃▇▃▁▁  perimeter_mean 0 1 91.97 24.30 43.79 75.17 86.24 104.10 188.50 ▃▇▃▁▁  area_mean 0 1 654.89 351.91 143.50 420.30 551.10 782.70 2501.00 ▇▃▂▁▁  smoothness_mean 0 1 0.10 0.01 0.05 0.09 0.10 0.11 0.16 ▁▇▇▁▁  compactness_mean 0 1 0.10 0.05 0.02 0.06 0.09 0.13 0.35 ▇▇▂▁▁  concavity_mean 0 1 0.09 0.08 0.00 0.03 0.06 0.13 0.43 ▇▃▂▁▁  concave points_mean 0 1 0.05 0.04 0.00 0.02 0.03 0.07 0.20 ▇▃▂▁▁  symmetry_mean 0 1 0.18 0.03 0.11 0.16 0.18 0.20 0.30 ▁▇▅▁▁  fractal_dimension_mean 0 1 0.06 0.01 0.05 0.06 0.06 0.07 0.10 ▆▇▂▁▁  radius_se 0 1 0.41 0.28 0.11 0.23 0.32 0.48 2.87 ▇▁▁▁▁  texture_se 0 1 1.22 0.55 0.36 0.83 1.11 1.47 4.88 ▇▅▁▁▁  perimeter_se 0 1 2.87 2.02 0.76 1.61 2.29 3.36 21.98 ▇▁▁▁▁  area_se 0 1 40.34 45.49 6.80 17.85 24.53 45.19 542.20 ▇▁▁▁▁  smoothness_se 0 1 0.01 0.00 0.00 0.01 0.01 0.01 0.03 ▇▃▁▁▁  compactness_se 0 1 0.03 0.02 0.00 0.01 0.02 0.03 0.14 ▇▃▁▁▁  concavity_se 0 1 0.03 0.03 0.00 0.02 0.03 0.04 0.40 ▇▁▁▁▁  concave points_se 0 1 0.01 0.01 0.00 0.01 0.01 0.01 0.05 ▇▇▁▁▁  symmetry_se 0 1 0.02 0.01 0.01 0.02 0.02 0.02 0.08 ▇▃▁▁▁  fractal_dimension_se 0 1 0.00 0.00 0.00 0.00 0.00 0.00 0.03 ▇▁▁▁▁  radius_worst 0 1 16.27 4.83 7.93 13.01 14.97 18.79 36.04 ▆▇▃▁▁  texture_worst 0 1 25.68 6.15 12.02 21.08 25.41 29.72 49.54 ▃▇▆▁▁  perimeter_worst 0 1 107.26 33.60 50.41 84.11 97.66 125.40 251.20 ▇▇▃▁▁  area_worst 0 1 880.58 569.36 185.20 515.30 686.50 1084.00 4254.00 ▇▂▁▁▁  smoothness_worst 0 1 0.13 0.02 0.07 0.12 0.13 0.15 0.22 ▂▇▇▂▁  compactness_worst 0 1 0.25 0.16 0.03 0.15 0.21 0.34 1.06 ▇▅▁▁▁  concavity_worst 0 1 0.27 0.21 0.00 0.11 0.23 0.38 1.25 ▇▅▂▁▁  concave points_worst 0 1 0.11 0.07 0.00 0.06 0.10 0.16 0.29 ▅▇▅▃▁  symmetry_worst 0 1 0.29 0.06 0.16 0.25 0.28 0.32 0.66 ▅▇▁▁▁  fractal_dimension_worst 0 1 0.08 0.02 0.06 0.07 0.08 0.09 0.21 ▇▃▁▁▁    Skimr: 0.15 sec elapsed\n Data Explorer Is a imho a prettier option with individual cool plots and a super powerfull(but slow) report creation tool when working outside of an Rmarkdownm document\ndf %\u0026gt;% DataExplorer::plot_intro() df %\u0026gt;% DataExplorer::plot_bar() df %\u0026gt;% DataExplorer::plot_correlation() Data Explorer individual plots: 1.05 sec elapsed\nThere are much more amaziong tools such as the ggforce package , but I hope you get the gist of the exploration stage.\n  Modeling For now I am going to focus on the tools provided by the tidymodels packages and the KNN, in the future I may come back to add more models and probably to play around the DALEX package a little bit.\nJust to remember M is Malignant and B is Benign, we are trying to correcly classify our patients, I am going to ignore the id Varible since it should not be reliaded upon to generate predictions(Even though it may capture some interesting effects such as better screening for patients on the latter id’s).\nTrain Test Split Usually we split our data into training and test data to ensure a fair evaluation of the models or parameters being tested(hoping to avoid overfitting).\nThe workflow for the tidymodels is that we first split our data.\ndf_split \u0026lt;- df %\u0026gt;% rsample::initial_split(prop = 0.8) Initial Split: 0 sec elapsed\ndf_training \u0026lt;- df_split %\u0026gt;% training() df_testing \u0026lt;- df_split %\u0026gt;% testing() Train test split: 0.02 sec elapsed\nThen we model on our Training Data\n Recipes Recipes are used to preprocess our data, the main mistake here is using the whole data set.\nThe recipe package helps us with this process.\nFor those not familiarized with the formula notation I am fitting the model on all variables except the id variable.\nI am than Normalizing my data since the KNN alghoritm is sensible to the scale of the variables being used, I am also excluding variables with high absolute correlation amongst themselves.\nRecipes are easy to read and can be quite complex\ndf_recipe \u0026lt;- training(df_split) %\u0026gt;% recipe(diagnosis ~ .) %\u0026gt;% step_rm(id) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors(),all_numeric()) %\u0026gt;% step_corr(all_predictors()) recipes: 0 sec elapsed\nWe could then create our train and test data frames by baking our recipe and juicing our recipe\n# df_testing \u0026lt;- df_recipe %\u0026gt;% # bake(testing(df_split)) # df_testing #df_training \u0026lt;- juice(df_recipe) unnamed-chunk-1: 0 sec elapsed\nBut I am going for a Bayes search approch\n Cross Validation and Bayes Search Cross Validation We further divide our data frame into folds in order to improve our certainty that the ideal number of neighbours is right.\ncv_splits \u0026lt;- df_testing %\u0026gt;% vfold_cv(v = 5) cvfold split: 0.02 sec elapsed\n Using the new tune package currently on github library(tune) knn_mod \u0026lt;- nearest_neighbor(neighbors = tune(), weight_func = tune()) %\u0026gt;% set_engine(\u0026quot;kknn\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) unnamed-chunk-2: 0.05 sec elapsed\n Combining everything so far in the new package workflow library(workflows) knn_wflow \u0026lt;- workflow() %\u0026gt;% add_model(knn_mod) %\u0026gt;% add_recipe(df_recipe) unnamed-chunk-3: 0.04 sec elapsed\n Limiting our search knn_param \u0026lt;- knn_wflow %\u0026gt;% parameters() %\u0026gt;% update( neighbors = neighbors(c(3, 50)), weight_func = weight_func(values = c(\u0026quot;rectangular\u0026quot;, \u0026quot;inv\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;triangular\u0026quot;)) ) unnamed-chunk-4: 0.03 sec elapsed\n Searching for the best model I used 5 iterations as the limit for the process because of printing reasons.\nKeep in mind that mtune will maximize just the first metric from the package yardstick\nctrl \u0026lt;- control_bayes(verbose = TRUE,no_improve = 5) set.seed(42) knn_search \u0026lt;- tune_bayes(knn_wflow, resamples = cv_splits, initial = 5, iter = 20, param_info = knn_param, control = ctrl, metrics = metric_set(roc_auc,accuracy)) ##  ## \u0026gt; Generating a set of 5 initial parameter results ## v Initialization complete ##  ## Optimizing roc_auc using the expected improvement ##  ## -- Iteration 1 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ ##  ## i Current best: roc_auc=0.9706 (@iter 0) ## i Gaussian process model ## v Gaussian process model ## i Generating 139 candidates ## i Predicted candidates ## i neighbors=31, weight_func=inv ## i Estimating performance ## v Estimating performance ## \u0026lt;U+24E7\u0026gt; Newest results: roc_auc=0.969 (+/-0.013) ##  ## -- Iteration 2 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ ##  ## i Current best: roc_auc=0.9706 (@iter 0) ## i Gaussian process model ## v Gaussian process model ## i Generating 138 candidates ## i Predicted candidates ## i neighbors=3, weight_func=rectangular ## i Estimating performance ## v Estimating performance ## \u0026lt;U+24E7\u0026gt; Newest results: roc_auc=0.9538 (+/-0.0208) ##  ## -- Iteration 3 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ ##  ## i Current best: roc_auc=0.9706 (@iter 0) ## i Gaussian process model ## v Gaussian process model ## i Generating 137 candidates ## i Predicted candidates ## i neighbors=50, weight_func=rectangular ## i Estimating performance ## v Estimating performance ## \u0026lt;U+24E7\u0026gt; Newest results: roc_auc=0.9432 (+/-0.0214) ##  ## -- Iteration 4 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ ##  ## i Current best: roc_auc=0.9706 (@iter 0) ## i Gaussian process model ## v Gaussian process model ## i Generating 136 candidates ## i Predicted candidates ## i neighbors=15, weight_func=rectangular ## i Estimating performance ## v Estimating performance ## \u0026lt;U+24E7\u0026gt; Newest results: roc_auc=0.9676 (+/-0.0184) ##  ## -- Iteration 5 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ ##  ## i Current best: roc_auc=0.9706 (@iter 0) ## i Gaussian process model ## v Gaussian process model ## i Generating 135 candidates ## i Predicted candidates ## i neighbors=3, weight_func=gaussian ## i Estimating performance ## v Estimating performance ## \u0026lt;U+24E7\u0026gt; Newest results: roc_auc=0.9562 (+/-0.0205) ## ! No improvement for 5 iterations; returning current results. Bayes Search: 26.38 sec elapsed\n Visualizing our search autoplot(knn_search, type = \u0026quot;performance\u0026quot;, metric = \u0026quot;accuracy\u0026quot;) unnamed-chunk-5: 0.16 sec elapsed\nautoplot(knn_search, type = \u0026quot;performance\u0026quot;, metric = \u0026quot;roc_auc\u0026quot;) unnamed-chunk-6: 0.2 sec elapsed\n Seing the best result collect_metrics(knn_search) %\u0026gt;% dplyr::filter(.metric == \u0026quot;accuracy\u0026quot;) %\u0026gt;% arrange(mean %\u0026gt;% desc) ## # A tibble: 10 x 8 ## neighbors weight_func .iter .metric .estimator mean n std_err ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3 gaussian 5 accuracy binary 0.939 5 0.0261 ## 2 3 rectangular 2 accuracy binary 0.939 5 0.0261 ## 3 5 inv 0 accuracy binary 0.939 5 0.0295 ## 4 13 inv 0 accuracy binary 0.939 5 0.0221 ## 5 42 gaussian 0 accuracy binary 0.939 5 0.0294 ## 6 15 rectangular 4 accuracy binary 0.938 5 0.0263 ## 7 33 inv 0 accuracy binary 0.930 5 0.0259 ## 8 31 inv 1 accuracy binary 0.929 5 0.0222 ## 9 27 rectangular 0 accuracy binary 0.921 5 0.0253 ## 10 50 rectangular 3 accuracy binary 0.859 5 0.0207 unnamed-chunk-7: 0.03 sec elapsed\ncollect_metrics(knn_search) %\u0026gt;% dplyr::filter(.metric == \u0026quot;roc_auc\u0026quot;) %\u0026gt;% arrange(mean %\u0026gt;% desc) ## # A tibble: 10 x 8 ## neighbors weight_func .iter .metric .estimator mean n std_err ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 33 inv 0 roc_auc binary 0.971 5 0.0165 ## 2 31 inv 1 roc_auc binary 0.969 5 0.0130 ## 3 42 gaussian 0 roc_auc binary 0.968 5 0.0182 ## 4 15 rectangular 4 roc_auc binary 0.968 5 0.0184 ## 5 13 inv 0 roc_auc binary 0.967 5 0.0203 ## 6 27 rectangular 0 roc_auc binary 0.961 5 0.0168 ## 7 5 inv 0 roc_auc binary 0.959 5 0.0182 ## 8 3 gaussian 5 roc_auc binary 0.956 5 0.0205 ## 9 3 rectangular 2 roc_auc binary 0.954 5 0.0208 ## 10 50 rectangular 3 roc_auc binary 0.943 5 0.0214 unnamed-chunk-8: 0.03 sec elapsed\n Extracting the best model best_metrics \u0026lt;- collect_metrics(knn_search) %\u0026gt;% dplyr::filter(.metric == \u0026quot;roc_auc\u0026quot;) %\u0026gt;% arrange(mean %\u0026gt;% desc) %\u0026gt;% head(1) %\u0026gt;% select(neighbors,weight_func) %\u0026gt;% as.list() unnamed-chunk-9: 0.01 sec elapsed\n Creating production model production_knn \u0026lt;- nearest_neighbor(neighbors = best_metrics$neighbors,weight_func = best_metrics$weight_func) %\u0026gt;% set_engine(\u0026quot;kknn\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) unnamed-chunk-10: 0 sec elapsed\n Creating production wflow production_wflow \u0026lt;- workflow() %\u0026gt;% add_model(production_knn) %\u0026gt;% add_recipe(df_recipe) unnamed-chunk-11: 0 sec elapsed\n Finally applying testing production model fit_prod \u0026lt;- fit(production_wflow,df_training) Fitting production: 0.28 sec elapsed\n Metrics fit_prod %\u0026gt;% predict(df_testing) %\u0026gt;% bind_cols(df_testing %\u0026gt;% transmute(diagnosis = diagnosis %\u0026gt;% as.factor())) %\u0026gt;% yardstick::metrics(truth = diagnosis,estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy binary 0.965 ## 2 kap binary 0.926 Calculating metrics: 0.03 sec elapsed\npredict(fit_prod,df_testing,type = \u0026#39;prob\u0026#39;) %\u0026gt;% bind_cols(df_testing %\u0026gt;% transmute(diagnosis = diagnosis %\u0026gt;% as.factor())) %\u0026gt;% yardstick::roc_auc(truth = diagnosis,.pred_B) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 roc_auc binary 0.989 Calculating metrics auc: 0.04 sec elapsed\n  Another visualization knn_naive \u0026lt;- nearest_neighbor(neighbors = tune()) %\u0026gt;% set_engine(\u0026quot;kknn\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) unnamed-chunk-12: 0 sec elapsed\nCombining everything so far in the new package workflow knn_wflow_naive \u0026lt;- workflow() %\u0026gt;% add_model(knn_naive) %\u0026gt;% add_recipe(df_recipe) unnamed-chunk-13: 0 sec elapsed\nknn_naive_param \u0026lt;- knn_wflow %\u0026gt;% parameters() %\u0026gt;% update( neighbors = neighbors(c(10, 50)) ) unnamed-chunk-14: 0.02 sec elapsed\nOne advantage of the naive search is that it is easy to parallelize\nall_cores \u0026lt;- parallel::detectCores(logical = FALSE) library(doParallel) ## Loading required package: foreach ## ## Attaching package: \u0026#39;foreach\u0026#39; ## The following objects are masked from \u0026#39;package:purrr\u0026#39;: ## ## accumulate, when ## Loading required package: iterators ## Loading required package: parallel cl \u0026lt;- makePSOCKcluster(all_cores) registerDoParallel(cl) set up parallel: 1.29 sec elapsed\nctrl \u0026lt;- control_grid(verbose = FALSE) set.seed(42) naive_search \u0026lt;- tune_grid(knn_wflow_naive, resamples = cv_splits, param_info = knn_naive_param, control = ctrl, grid = 50, metrics = metric_set(roc_auc,accuracy)) naive grid search: 6.31 sec elapsed\nbest_naive_metrics \u0026lt;- collect_metrics(naive_search) %\u0026gt;% dplyr::filter(.metric == \u0026quot;roc_auc\u0026quot;) %\u0026gt;% arrange(mean %\u0026gt;% desc) DT::datatable(best_naive_metrics,options = list(pageLength = 5, scrollX=T))  {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\"],[18,20,24,25,26,28,29,30,31,32,17,21,34,35,22,23,15,16,36,37,38,39,40,41,42,46,10,11,12,13,14,43,44,45,47,48,49],[\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\"],[\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\"],[0.974487179487179,0.974487179487179,0.973717948717949,0.973717948717949,0.973717948717949,0.973717948717949,0.973717948717949,0.973717948717949,0.973717948717949,0.973717948717949,0.972948717948718,0.972820512820513,0.972179487179487,0.972179487179487,0.972051282051282,0.972051282051282,0.971410256410256,0.971410256410256,0.970641025641026,0.970641025641026,0.970641025641026,0.970641025641026,0.970641025641026,0.970641025641026,0.970641025641026,0.970641025641026,0.969871794871795,0.969871794871795,0.969871794871795,0.969871794871795,0.969871794871795,0.969102564102564,0.969102564102564,0.969102564102564,0.969102564102564,0.969102564102564,0.969102564102564],[10,10,5,5,10,10,5,10,5,10,5,5,10,5,10,5,5,5,5,5,10,5,5,5,10,10,5,5,5,5,10,5,5,5,5,5,10],[0.00941568502832165,0.00941568502832165,0.0146765476846401,0.0146765476846401,0.00978436512309337,0.00978436512309337,0.0146765476846401,0.00978436512309337,0.0146765476846401,0.00978436512309337,0.0152483279604168,0.0134532941787636,0.0105578910181491,0.0158368365272237,0.00933992910550258,0.0140098936582539,0.0159840485902653,0.0159840485902653,0.0165285201685211,0.0165285201685211,0.0110190134456807,0.0165285201685211,0.0165285201685211,0.0165285201685211,0.0110190134456807,0.01089902787098,0.0168286018698568,0.0168286018698568,0.0168286018698568,0.0168286018698568,0.0112190679132379,0.0173295208976781,0.0173295208976781,0.0173295208976781,0.0173295208976781,0.0173295208976781,0.0115530139317854]],\"container\":\"\\n \\n \\n  \\n neighbors\\n .metric\\n .estimator\\n mean\\n n\\n std_err\\n \\n \\n\",\"options\":{\"pageLength\":5,\"scrollX\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,4,5,6]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}unnamed-chunk-15: 0.03 sec elapsed\np \u0026lt;- best_naive_metrics %\u0026gt;% ggplot() + aes(neighbors,mean) + geom_point() + ylim(.95,1) p unnamed-chunk-16: 0.16 sec elapsed\n   Checking the timing table (x \u0026lt;- tic.log() %\u0026gt;% as.character() %\u0026gt;% tibble(log = .) %\u0026gt;% separate(log,sep = \u0026#39;: \u0026#39;,into = c(\u0026#39;name\u0026#39;,\u0026#39;time\u0026#39;))) %\u0026gt;% separate(time, sep = \u0026#39; \u0026#39;,c(\u0026#39;measure\u0026#39;,\u0026#39;units\u0026#39;)) %\u0026gt;% mutate(measure = measure %\u0026gt;% as.numeric()) %\u0026gt;% arrange(measure %\u0026gt;% desc()) ## Warning: Expected 2 pieces. Additional pieces discarded in 31 rows [1, 2, 3, 4, ## 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...]. ## # A tibble: 31 x 3 ## name measure units ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Bayes Search 26.4 sec ## 2 naive grid search 6.31 sec ## 3 Loading Libraries 3.65 sec ## 4 set up parallel 1.29 sec ## 5 Data Explorer individual plots 1.05 sec ## 6 Fitting production 0.28 sec ## 7 unnamed-chunk-6 0.2 sec ## 8 Set Chunk 0.16 sec ## 9 unnamed-chunk-5 0.16 sec ## 10 unnamed-chunk-16 0.16 sec ## # … with 21 more rows unnamed-chunk-17: 0.02 sec elapsed\n ","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"fd164806f74a21dae5f074a433a61295","permalink":"/2019/08/15/classification-knn/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/2019/08/15/classification-knn/","section":"post","summary":"Breast Cancer problem.\nThis is a problem that I have trid to solve using just the old tidymodels package and got stuck so here is the new implementation using the amazing tune and workflows packages\nSetting up Rmarkdown root.dir =\n Loading Libraries library(tidyverse) ## -- Attaching packages ---------------------------------------------------------------------------------------------------------------------------------------------- tidyverse 1.3.0 -- ## \u0026lt;U+2713\u0026gt; ggplot2 3.2.1 \u0026lt;U+2713\u0026gt; purrr 0.3.3 ## \u0026lt;U+2713\u0026gt; tibble 2.1.3 \u0026lt;U+2713\u0026gt; dplyr 0.","tags":["tidyverse","tidymodels"],"title":"Classification - KNN","type":"post"},{"authors":null,"categories":["r-project","R and Python"],"content":"\n\n\n\n","date":1554422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554422400,"objectID":"0b1239b1586ff5b25e6b3707ae1fc1ae","permalink":"/2019/04/05/exploratory-data-analysis-basic-statistical-inference/","publishdate":"2019-04-05T00:00:00Z","relpermalink":"/2019/04/05/exploratory-data-analysis-basic-statistical-inference/","section":"post","summary":"","tags":["dplyr","pandas","scipy","tidyverse","reticulate","R Markdown"],"title":"exploratory data analysis: basic statistical inference","type":"post"},{"authors":["Bruno Carlin"],"categories":["R and Python","r-project"],"content":" This is an basic example of how you can use either R or Python to accomplish the same goals, I really enjoy using the tidyverse but as you will see sometimes Python is just the more intuitive option. If you find yourself confused on whether a code chunk is an R or Python code please ask me or check my github page for this project.  1 Getting Started, we will use multiple functions from both languages 1.1 How to set up reticulate? 1.1.1 Setting root folder 1.1.2 Libraries   2 Python 2.1 Knowing data frames 2.1.1 Defining pandas series 2.1.2 Indexing  2.2 Combining two pd series 2.2.1 Create pd series from dictionary 1 2.2.2 Combining the pd series into a data frame 2.2.3 Data frame properties 2.2.4 Creating some new columns 2.2.5 Ordering a data frame 2.2.6 Subsetting  2.3 Real data 2.3.1 Reading data 2.3.2 Variable types 2.3.3 Basic Description 2.3.4 Subsetting data 2.3.5 Creating new columns with real data 2.3.6 Creating a new smaller data frame 2.3.7 Plotting an line plot 2.3.8 Filtering and replace data 2.3.9 Groupby example 2.3.10 Ploting an histogram 2.3.11 Handling Missing values 2.3.12 Replacing names with an dictionary  2.4 Passing Objects 2.4.1 Python to R   3 R 3.1 Knowing data frames 3.1.1 Defining an data frame 3.1.2 Index search  3.2 Creating an data frame from two R series 3.2.1 Create a date frame using an list 3.2.2 Create a date frame using an list 2 3.2.3 Subsetting an data frame using join or cbind 3.2.4 Some info on our data frame 3.2.5 Creating new columns using mutate and basic R 3.2.6 Ordering an data frame using the tidy way arrange or order. 3.2.7 Filtering rows using standard R code or filter.  3.3 Real Case 3.3.1 Two way of importing an csv 3.3.2 Let’s look at our data 3.3.3 Types of columns r 3.3.4 Basic Description real data using Glimpse and str 3.3.5 Subsetting Data with select or base R 3.3.6 Creating a new smaller data frame using transmute and base 3.3.7 Ploting with ggplot 3.3.8 Filtering and replace data 3.3.9 Groupby example in tidyverse 3.3.10 Ploting an histogram using ggplot2 3.3.11 Handling Missing values in R 3.3.12 Replacing names with an case when aproach  3.4 Passing Objects to Python    I am currently doing exercises from digital house brasil\n1 Getting Started, we will use multiple functions from both languages 1.1 How to set up reticulate? 1.1.1 Setting root folder I recommend using the Files tab to find the your system path to the folder containig all the data.\nUse opts_knit to guarantee that your markdown functions will search for files in the folder specified, it is better that setwd() because it works on all languages.\nknitr::opts_knit$set(root.dir = normalizePath( \u0026quot;~/R/Blog/content/post/data\u0026quot;))  1.1.2 Libraries  R part  library(reticulate) library(caTools) library(roperators) library(tidyverse) set.seed(123)   Python part  I am using my second virtual conda if you have just the root switch to conda_list()[[1]][1].\nconda_list()[[1]][2] %\u0026gt;% use_condaenv(required = TRUE) Let’s see what version of python this env is running.\nimport platform print(platform.python_version()) ## 3.7.2 Some basic Data Science Libraries.\nimport numpy as np import matplotlib.pyplot as plt import pandas as pd import os     2 Python 2.1 Knowing data frames 2.1.1 Defining pandas series data = pd.Series([0.25, 0.5, 0.75, 1.0]) data ## 0 0.25 ## 1 0.50 ## 2 0.75 ## 3 1.00 ## dtype: float64 data.values ## array([0.25, 0.5 , 0.75, 1. ]) data.index ## RangeIndex(start=0, stop=4, step=1) data[1] ## 0.5 data[1:3] ## 1 0.50 ## 2 0.75 ## dtype: float64  2.1.2 Indexing data = pd.Series([0.25, 0.5, 0.75, 1.0], index=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]) data ## a 0.25 ## b 0.50 ## c 0.75 ## d 1.00 ## dtype: float64 data[\u0026#39;b\u0026#39;] ## 0.5   2.2 Combining two pd series 2.2.1 Create pd series from dictionary 1 population_dict = {\u0026#39;California\u0026#39;: 38332521, \u0026#39;Florida\u0026#39;: 19552860, \u0026#39;Illinois\u0026#39;: 12882135, \u0026#39;New York\u0026#39;: 19651127, \u0026#39;Texas\u0026#39;: 26448193,} population = pd.Series(population_dict) population ## California 38332521 ## Florida 19552860 ## Illinois 12882135 ## New York 19651127 ## Texas 26448193 ## dtype: int64 population[\u0026#39;California\u0026#39;] ## 38332521 population[\u0026#39;California\u0026#39;:\u0026#39;Illinois\u0026#39;] ## California 38332521 ## Florida 19552860 ## Illinois 12882135 ## dtype: int64 one more example.\narea_dict = {\u0026#39;California\u0026#39;: 423967, \u0026#39;Florida\u0026#39;: 170312, \u0026#39;Illinois\u0026#39;: 149995, \u0026#39;New York\u0026#39;: 141297, \u0026#39;Texas\u0026#39;: 695662} area = pd.Series(area_dict) area ## California 423967 ## Florida 170312 ## Illinois 149995 ## New York 141297 ## Texas 695662 ## dtype: int64  2.2.2 Combining the pd series into a data frame states = pd.DataFrame({\u0026#39;population\u0026#39;: population, \u0026#39;area\u0026#39;: area}) states ## population area ## California 38332521 423967 ## Florida 19552860 170312 ## Illinois 12882135 149995 ## New York 19651127 141297 ## Texas 26448193 695662 type(states) ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; type(states[\u0026quot;population\u0026quot;]) ## \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt; type([states[\u0026quot;population\u0026quot;]]) ## \u0026lt;class \u0026#39;list\u0026#39;\u0026gt;  2.2.3 Data frame properties states.shape ## (5, 2) states.info() ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## Index: 5 entries, California to Texas ## Data columns (total 2 columns): ## population 5 non-null int64 ## area 5 non-null int64 ## dtypes: int64(2) ## memory usage: 280.0+ bytes states.index ## Index([\u0026#39;California\u0026#39;, \u0026#39;Florida\u0026#39;, \u0026#39;Illinois\u0026#39;, \u0026#39;New York\u0026#39;, \u0026#39;Texas\u0026#39;], dtype=\u0026#39;object\u0026#39;) states.columns ## Index([\u0026#39;population\u0026#39;, \u0026#39;area\u0026#39;], dtype=\u0026#39;object\u0026#39;) states[\u0026#39;area\u0026#39;] ## California 423967 ## Florida 170312 ## Illinois 149995 ## New York 141297 ## Texas 695662 ## Name: area, dtype: int64  2.2.4 Creating some new columns states[\u0026#39;density\u0026#39;] = states[\u0026#39;population\u0026#39;] / states[\u0026#39;area\u0026#39;] states ## population area density ## California 38332521 423967 90.413926 ## Florida 19552860 170312 114.806121 ## Illinois 12882135 149995 85.883763 ## New York 19651127 141297 139.076746 ## Texas 26448193 695662 38.018740  2.2.5 Ordering a data frame states.sort_values([\u0026#39;population\u0026#39;], ascending = True) ## population area density ## Illinois 12882135 149995 85.883763 ## Florida 19552860 170312 114.806121 ## New York 19651127 141297 139.076746 ## Texas 26448193 695662 38.018740 ## California 38332521 423967 90.413926 states.sort_values([\u0026#39;area\u0026#39;], ascending = True) ## population area density ## New York 19651127 141297 139.076746 ## Illinois 12882135 149995 85.883763 ## Florida 19552860 170312 114.806121 ## California 38332521 423967 90.413926 ## Texas 26448193 695662 38.018740 states.sort_values([\u0026#39;density\u0026#39;], ascending = True) ## population area density ## Texas 26448193 695662 38.018740 ## Illinois 12882135 149995 85.883763 ## California 38332521 423967 90.413926 ## Florida 19552860 170312 114.806121 ## New York 19651127 141297 139.076746  2.2.6 Subsetting states[\u0026#39;Florida\u0026#39;:\u0026#39;Illinois\u0026#39;] ## population area density ## Florida 19552860 170312 114.806121 ## Illinois 12882135 149995 85.883763 states[1:3] ## population area density ## Florida 19552860 170312 114.806121 ## Illinois 12882135 149995 85.883763 data_pop = (states[\u0026#39;population\u0026#39;] \u0026gt; 19552860) \u0026amp; (states[\u0026#39;area\u0026#39;]\u0026gt;423967) data_pop ## California False ## Florida False ## Illinois False ## New York False ## Texas True ## dtype: bool states[(states[\u0026#39;population\u0026#39;] \u0026gt; 19552860) \u0026amp; (states[\u0026#39;area\u0026#39;]\u0026gt;423967)] ## population area density ## Texas 26448193 695662 38.01874 states[[\u0026#39;area\u0026#39;,\u0026#39;density\u0026#39;]] ## area density ## California 423967 90.413926 ## Florida 170312 114.806121 ## Illinois 149995 85.883763 ## New York 141297 139.076746 ## Texas 695662 38.018740 states[states.density \u0026gt; 100] ## population area density ## Florida 19552860 170312 114.806121 ## New York 19651127 141297 139.076746 states.loc[states.density \u0026gt; 100, [\u0026#39;population\u0026#39;, \u0026#39;density\u0026#39;]] ## population density ## Florida 19552860 114.806121 ## New York 19651127 139.076746 states.loc[states.density \u0026gt; 100][[\u0026#39;population\u0026#39;, \u0026#39;density\u0026#39;]] ## population density ## Florida 19552860 114.806121 ## New York 19651127 139.076746 states.loc[\u0026#39;California\u0026#39;, \u0026#39;density\u0026#39;] ## 90.41392608386974 states.loc[\u0026#39;California\u0026#39;][[\u0026#39;density\u0026#39;]] ## density 90.413926 ## Name: California, dtype: float64 states.iloc[0, 2] ## 90.41392608386974   2.3 Real data 2.3.1 Reading data sales = pd.DataFrame(pd.read_csv(\u0026#39;2019-03-23-exploratory-data-analysis-basic-pandas-and-dplyr/UKretail.csv\u0026#39;,encoding=\u0026#39;latin\u0026#39;)) sales.head() ## InvoiceNo StockCode ... CustomerID Country ## 0 536365 22752 ... 17850.0 United Kingdom ## 1 536365 71053 ... 17850.0 United Kingdom ## 2 536365 84029G ... 17850.0 United Kingdom ## 3 536365 85123A ... 17850.0 United Kingdom ## 4 536366 22633 ... 17850.0 United Kingdom ## ## [5 rows x 8 columns] sales.tail(3) ## InvoiceNo StockCode ... CustomerID Country ## 325142 581587 22899 ... 12680.0 France ## 325143 581587 23254 ... 12680.0 France ## 325144 581587 23256 ... 12680.0 France ## ## [3 rows x 8 columns] sales.index ## RangeIndex(start=0, stop=325145, step=1)  2.3.2 Variable types If you need to return.\ntype(sales)  ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; type(sales[\u0026quot;CustomerID\u0026quot;]) ## \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt; type([sales[\u0026quot;CustomerID\u0026quot;]])  ## \u0026lt;class \u0026#39;list\u0026#39;\u0026gt;  2.3.3 Basic Description sales.shape  ## (325145, 8) sales.columns.values  ## array([\u0026#39;InvoiceNo\u0026#39;, \u0026#39;StockCode\u0026#39;, \u0026#39;Description\u0026#39;, \u0026#39;Quantity\u0026#39;, \u0026#39;InvoiceDate\u0026#39;, ## \u0026#39;UnitPrice\u0026#39;, \u0026#39;CustomerID\u0026#39;, \u0026#39;Country\u0026#39;], dtype=object) sales.info()  ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## RangeIndex: 325145 entries, 0 to 325144 ## Data columns (total 8 columns): ## InvoiceNo 325145 non-null object ## StockCode 325145 non-null object ## Description 324275 non-null object ## Quantity 325145 non-null int64 ## InvoiceDate 325145 non-null object ## UnitPrice 325145 non-null float64 ## CustomerID 244154 non-null float64 ## Country 325145 non-null object ## dtypes: float64(2), int64(1), object(5) ## memory usage: 19.8+ MB sales.describe() ## Quantity UnitPrice CustomerID ## count 325145.000000 325145.000000 244154.000000 ## mean 9.273340 4.845239 15288.823120 ## std 154.394112 116.830451 1713.496816 ## min -80995.000000 -11062.060000 12347.000000 ## 25% 1.000000 1.250000 13959.000000 ## 50% 3.000000 2.080000 15150.000000 ## 75% 10.000000 4.130000 16792.750000 ## max 12540.000000 38970.000000 18287.000000  2.3.4 Subsetting data sales[:4] ## InvoiceNo StockCode ... CustomerID Country ## 0 536365 22752 ... 17850.0 United Kingdom ## 1 536365 71053 ... 17850.0 United Kingdom ## 2 536365 84029G ... 17850.0 United Kingdom ## 3 536365 85123A ... 17850.0 United Kingdom ## ## [4 rows x 8 columns] sales[\u0026quot;CustomerID\u0026quot;].head() ## 0 17850.0 ## 1 17850.0 ## 2 17850.0 ## 3 17850.0 ## 4 17850.0 ## Name: CustomerID, dtype: float64 sales.loc[:,[\u0026#39;Quantity\u0026#39;]].head() ## Quantity ## 0 2 ## 1 6 ## 2 6 ## 3 6 ## 4 6 sales.iloc[:,[3]].head() ## Quantity ## 0 2 ## 1 6 ## 2 6 ## 3 6 ## 4 6 sales.iloc[0:6,2:3] ## Description ## 0 SET 7 BABUSHKA NESTING BOXES ## 1 WHITE METAL LANTERN ## 2 KNITTED UNION FLAG HOT WATER BOTTLE ## 3 WHITE HANGING HEART T-LIGHT HOLDER ## 4 HAND WARMER UNION JACK ## 5 HOME BUILDING BLOCK WORD  2.3.5 Creating new columns with real data sales[\u0026quot;Revenue\u0026quot;] = sales.Quantity * sales.UnitPrice sales.head() ## InvoiceNo StockCode ... Country Revenue ## 0 536365 22752 ... United Kingdom 15.30 ## 1 536365 71053 ... United Kingdom 20.34 ## 2 536365 84029G ... United Kingdom 20.34 ## 3 536365 85123A ... United Kingdom 15.30 ## 4 536366 22633 ... United Kingdom 11.10 ## ## [5 rows x 9 columns]  2.3.6 Creating a new smaller data frame raw_sales = sales[[\u0026quot;Quantity\u0026quot;,\u0026quot;UnitPrice\u0026quot;, \u0026quot;Revenue\u0026quot;]] raw_sales.head() ## Quantity UnitPrice Revenue ## 0 2 7.65 15.30 ## 1 6 3.39 20.34 ## 2 6 3.39 20.34 ## 3 6 2.55 15.30 ## 4 6 1.85 11.10 raw_sales.info() ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## RangeIndex: 325145 entries, 0 to 325144 ## Data columns (total 3 columns): ## Quantity 325145 non-null int64 ## UnitPrice 325145 non-null float64 ## Revenue 325145 non-null float64 ## dtypes: float64(2), int64(1) ## memory usage: 7.4 MB  2.3.7 Plotting an line plot import matplotlib as plt from pylab import * sales.plot(x=\u0026quot;InvoiceDate\u0026quot;, y=\u0026quot;Revenue\u0026quot;, kind=\u0026quot;line\u0026quot;) plt.show()  2.3.8 Filtering and replace data To return\ncancels = sales[sales[\u0026quot;Revenue\u0026quot;]\u0026lt;0] cancels.shape ## (5588, 9) sales.drop(cancels.index, inplace=True) sales.shape ## (319557, 9)  2.3.9 Groupby example CountryGroups = sales.groupby([\u0026quot;Country\u0026quot;])[\u0026quot;Revenue\u0026quot;].sum().reset_index() CountryGroups.sort_values(by= \u0026quot;Revenue\u0026quot;, ascending=False) ## Country Revenue ## 36 United Kingdom 5311080.101 ## 10 EIRE 176304.590 ## 24 Netherlands 165582.790 ## 14 Germany 138778.440 ## 13 France 127193.680 ## 0 Australia 79197.590 ## 31 Spain 36116.710 ## 33 Switzerland 34315.240 ## 3 Belgium 24014.970 ## 25 Norway 23182.220 ## 32 Sweden 21762.450 ## 20 Japan 21072.590 ## 27 Portugal 20109.410 ## 30 Singapore 13383.590 ## 6 Channel Islands 12556.740 ## 12 Finland 12362.880 ## 9 Denmark 11739.370 ## 19 Italy 10837.890 ## 16 Hong Kong 8227.020 ## 7 Cyprus 7781.900 ## 1 Austria 6100.960 ## 18 Israel 4225.780 ## 26 Poland 3974.080 ## 37 Unspecified 2898.650 ## 15 Greece 2677.570 ## 17 Iceland 2461.230 ## 34 USA 2388.740 ## 5 Canada 2093.390 ## 23 Malta 1318.990 ## 35 United Arab Emirates 1277.500 ## 21 Lebanon 1120.530 ## 22 Lithuania 1038.560 ## 11 European Community 876.550 ## 4 Brazil 602.310 ## 28 RSA 573.180 ## 8 Czech Republic 488.580 ## 2 Bahrain 343.400 ## 29 Saudi Arabia 90.720  2.3.10 Ploting an histogram sales[sales[\u0026quot;CustomerID\u0026quot;] == 17850.0][\u0026quot;Revenue\u0026quot;].plot(kind=\u0026quot;hist\u0026quot;) plt.show() another example.\nsales[sales[\u0026quot;StockCode\u0026quot;] == \u0026#39;71053\u0026#39;][\u0026quot;Quantity\u0026quot;].hist() plt.show()  2.3.11 Handling Missing values to return\nsales.info() ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## Int64Index: 319557 entries, 0 to 325144 ## Data columns (total 9 columns): ## InvoiceNo 319557 non-null object ## StockCode 319557 non-null object ## Description 318687 non-null object ## Quantity 319557 non-null int64 ## InvoiceDate 319557 non-null object ## UnitPrice 319557 non-null float64 ## CustomerID 238801 non-null float64 ## Country 319557 non-null object ## Revenue 319557 non-null float64 ## dtypes: float64(3), int64(1), object(5) ## memory usage: 24.4+ MB sales.CustomerID.value_counts(dropna=False).nlargest(3) ## NaN 80756 ## 17841.0 4702 ## 14911.0 3449 ## Name: CustomerID, dtype: int64 sales.CustomerID.fillna(0, inplace=True) sales[sales.CustomerID.isnull()] ## Empty DataFrame ## Columns: [InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country, Revenue] ## Index: [] sales.info() ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; ## Int64Index: 319557 entries, 0 to 325144 ## Data columns (total 9 columns): ## InvoiceNo 319557 non-null object ## StockCode 319557 non-null object ## Description 318687 non-null object ## Quantity 319557 non-null int64 ## InvoiceDate 319557 non-null object ## UnitPrice 319557 non-null float64 ## CustomerID 319557 non-null float64 ## Country 319557 non-null object ## Revenue 319557 non-null float64 ## dtypes: float64(3), int64(1), object(5) ## memory usage: 24.4+ MB  2.3.12 Replacing names with an dictionary mymap = {\u0026#39;United Kingdom\u0026#39;:1, \u0026#39;Netherlands\u0026#39;:2, \u0026#39;Germany\u0026#39;:3, \u0026#39;France\u0026#39;:4, \u0026#39;USA\u0026#39;:5} sales = sales.applymap(lambda s: mymap.get(s) if s in mymap else s) sales.head() ## InvoiceNo StockCode ... Country Revenue ## 0 536365 22752 ... 1 15.30 ## 1 536365 71053 ... 1 20.34 ## 2 536365 84029G ... 1 20.34 ## 3 536365 85123A ... 1 15.30 ## 4 536366 22633 ... 1 11.10 ## ## [5 rows x 9 columns] sales.Country.value_counts().nlargest(7) ## 1 292640 ## 3 5466 ## 4 5026 ## EIRE 4789 ## Spain 1420 ## 2 1393 ## Belgium 1191 ## Name: Country, dtype: int64   2.4 Passing Objects 2.4.1 Python to R data2 = pd.Series([0.25, 0.5, 0.75, 1.0]) data_t = py$data2 data_t ## 0 1 2 3 ## 0.25 0.50 0.75 1.00    3 R 3.1 Knowing data frames 3.1.1 Defining an data frame tidy way data \u0026lt;- tibble(0.25, 0.5, 0.75, 1.0) data ## # A tibble: 1 x 4 ## `0.25` `0.5` `0.75` `1` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.25 0.5 0.75 1 data[2] ## # A tibble: 1 x 1 ## `0.5` ## \u0026lt;dbl\u0026gt; ## 1 0.5 data[2:3] ## # A tibble: 1 x 2 ## `0.5` `0.75` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.5 0.75 Not using tidyverse.\ndata \u0026lt;- data.frame(c(0.25, 0.5, 0.75, 1.0)) rownames(data) \u0026lt;- 1:nrow(data) colnames(data) \u0026lt;- \u0026quot;nope\u0026quot; data ## nope ## 1 0.25 ## 2 0.50 ## 3 0.75 ## 4 1.00  3.1.2 Index search data \u0026lt;- data.frame(c(0.25, 0.5, 0.75, 1.0),row.names = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;d\u0026quot;)) data ## c.0.25..0.5..0.75..1. ## a 0.25 ## b 0.50 ## c 0.75 ## d 1.00 data[\u0026quot;b\u0026quot;,] ## [1] 0.5   3.2 Creating an data frame from two R series 3.2.1 Create a date frame using an list population_dict \u0026lt;- list( \u0026#39;California\u0026#39; = 38332521, \u0026#39;Florida\u0026#39; = 19552860, \u0026#39;Illinois\u0026#39; = 12882135, \u0026#39;New York\u0026#39; = 19651127, \u0026#39;Texas\u0026#39; = 26448193 ) population \u0026lt;- population_dict %\u0026gt;% as_tibble() population[\u0026#39;California\u0026#39;] ## # A tibble: 1 x 1 ## California ## \u0026lt;dbl\u0026gt; ## 1 38332521 population %\u0026gt;% select(California:Illinois) ## # A tibble: 1 x 3 ## California Florida Illinois ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 38332521 19552860 12882135  3.2.2 Create a date frame using an list 2 area_dict = list( \u0026#39;California\u0026#39; = 423967, \u0026#39;Florida\u0026#39; = 170312, \u0026#39;Illinois\u0026#39; = 149995, \u0026#39;New York\u0026#39; = 141297, \u0026#39;Texas\u0026#39; = 695662 ) area_dict %\u0026gt;% as_tibble() -\u0026gt; area area ## # A tibble: 1 x 5 ## California Florida Illinois `New York` Texas ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 423967 170312 149995 141297 695662  3.2.3 Subsetting an data frame using join or cbind The tidy way doesn`t support indexes so we can tidy our data.\ntidy_area \u0026lt;- area %\u0026gt;% gather(key = \u0026quot;state\u0026quot;, value = \u0026quot;area\u0026quot;) tidy_state \u0026lt;- population %\u0026gt;% gather(key = \u0026quot;state\u0026quot;, value = \u0026quot;population\u0026quot;) tidy_area ## # A tibble: 5 x 2 ## state area ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 California 423967 ## 2 Florida 170312 ## 3 Illinois 149995 ## 4 New York 141297 ## 5 Texas 695662 tidy_state ## # A tibble: 5 x 2 ## state population ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 California 38332521 ## 2 Florida 19552860 ## 3 Illinois 12882135 ## 4 New York 19651127 ## 5 Texas 26448193 tidy_area %\u0026gt;% left_join(tidy_state) ## Joining, by = \u0026quot;state\u0026quot; ## # A tibble: 5 x 3 ## state area population ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 California 423967 38332521 ## 2 Florida 170312 19552860 ## 3 Illinois 149995 12882135 ## 4 New York 141297 19651127 ## 5 Texas 695662 26448193 tidy_merge \u0026lt;- cbind(tidy_area,tidy_state[,-1]) states \u0026lt;- tidy_merge  3.2.4 Some info on our data frame class(tidy_merge) ## [1] \u0026quot;data.frame\u0026quot; class(tidy_merge$population) ## [1] \u0026quot;numeric\u0026quot; class(list(tidy_merge[\u0026quot;population\u0026quot;])) ## [1] \u0026quot;list\u0026quot; states %\u0026gt;% dim() ## [1] 5 3 states %\u0026gt;% str() ## \u0026#39;data.frame\u0026#39;: 5 obs. of 3 variables: ## $ state : chr \u0026quot;California\u0026quot; \u0026quot;Florida\u0026quot; \u0026quot;Illinois\u0026quot; \u0026quot;New York\u0026quot; ... ## $ area : num 423967 170312 149995 141297 695662 ## $ population: num 38332521 19552860 12882135 19651127 26448193 states %\u0026gt;% glimpse() ## Observations: 5 ## Variables: 3 ## $ state \u0026lt;chr\u0026gt; \u0026quot;California\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Illinois\u0026quot;, \u0026quot;New York\u0026quot;, \u0026quot;T... ## $ area \u0026lt;dbl\u0026gt; 423967, 170312, 149995, 141297, 695662 ## $ population \u0026lt;dbl\u0026gt; 38332521, 19552860, 12882135, 19651127, 26448193 states[[\u0026quot;Estado\u0026quot;]] ## NULL states %\u0026gt;% colnames() %\u0026gt;% tail(-1) ## [1] \u0026quot;area\u0026quot; \u0026quot;population\u0026quot; states$area ## [1] 423967 170312 149995 141297 695662  3.2.5 Creating new columns using mutate and basic R states$density \u0026lt;- states$population / states$area states ## state area population density ## 1 California 423967 38332521 90.41393 ## 2 Florida 170312 19552860 114.80612 ## 3 Illinois 149995 12882135 85.88376 ## 4 New York 141297 19651127 139.07675 ## 5 Texas 695662 26448193 38.01874 # or states$density \u0026lt;- states[[\u0026quot;population\u0026quot;]] / states[[\u0026quot;area\u0026quot;]] states ## state area population density ## 1 California 423967 38332521 90.41393 ## 2 Florida 170312 19552860 114.80612 ## 3 Illinois 149995 12882135 85.88376 ## 4 New York 141297 19651127 139.07675 ## 5 Texas 695662 26448193 38.01874 #or states %\u0026gt;% mutate(density = population / area) ## state area population density ## 1 California 423967 38332521 90.41393 ## 2 Florida 170312 19552860 114.80612 ## 3 Illinois 149995 12882135 85.88376 ## 4 New York 141297 19651127 139.07675 ## 5 Texas 695662 26448193 38.01874  3.2.6 Ordering an data frame using the tidy way arrange or order. You can also use -c() or desc() sometimes -c() can give strange results.\nstates %\u0026gt;% arrange(desc(population)) ## state area population density ## 1 California 423967 38332521 90.41393 ## 2 Texas 695662 26448193 38.01874 ## 3 New York 141297 19651127 139.07675 ## 4 Florida 170312 19552860 114.80612 ## 5 Illinois 149995 12882135 85.88376 states[order(states$area),] ## state area population density ## 4 New York 141297 19651127 139.07675 ## 3 Illinois 149995 12882135 85.88376 ## 2 Florida 170312 19552860 114.80612 ## 1 California 423967 38332521 90.41393 ## 5 Texas 695662 26448193 38.01874 # Mix and match all three formas states %\u0026gt;% arrange(-c(density),desc(population,area),state) ## state area population density ## 1 New York 141297 19651127 139.07675 ## 2 Florida 170312 19552860 114.80612 ## 3 California 423967 38332521 90.41393 ## 4 Illinois 149995 12882135 85.88376 ## 5 Texas 695662 26448193 38.01874  3.2.7 Filtering rows using standard R code or filter. states[1:3,] ## state area population density ## 1 California 423967 38332521 90.41393 ## 2 Florida 170312 19552860 114.80612 ## 3 Illinois 149995 12882135 85.88376 data_pop \u0026lt;- states[states$population \u0026gt; 19552860 \u0026amp; states$area \u0026gt; 423967,] data_pop ## state area population density ## 5 Texas 695662 26448193 38.01874 states %\u0026gt;% filter(population \u0026gt; 19552860 \u0026amp; area \u0026gt; 423967) ## state area population density ## 1 Texas 695662 26448193 38.01874 you can mix and match filter for rows and select for columns.\nstates %\u0026gt;% filter(density \u0026gt; 100) ## state area population density ## 1 Florida 170312 19552860 114.8061 ## 2 New York 141297 19651127 139.0767 states %\u0026gt;% filter(density \u0026gt; 100) %\u0026gt;% select(population,density) ## population density ## 1 19552860 114.8061 ## 2 19651127 139.0767 states[1,4] ## [1] 90.41393   3.3 Real Case 3.3.1 Two way of importing an csv sales \u0026lt;- read_csv(\u0026#39;2019-03-23-exploratory-data-analysis-basic-pandas-and-dplyr/UKretail.csv\u0026#39;) ## Parsed with column specification: ## cols( ## InvoiceNo = col_character(), ## StockCode = col_character(), ## Description = col_character(), ## Quantity = col_double(), ## InvoiceDate = col_datetime(format = \u0026quot;\u0026quot;), ## UnitPrice = col_double(), ## CustomerID = col_double(), ## Country = col_character() ## ) sales \u0026lt;- read.csv(\u0026#39;2019-03-23-exploratory-data-analysis-basic-pandas-and-dplyr/UKretail.csv\u0026#39;) If you think this looks like an ugly path and a was of space I would agree we can fix this by using one of my favorite thinks from python the \"\"key I avoided.\nI am now using it on the python part to show the power of neat line.\npath_file = \u0026#39;\\ 2019-03-23-exploratory-data-analysis-basic-pandas-and-dplyr/\\ UKretail.csv\u0026#39;  sales \u0026lt;- read_csv(py$path_file) ## Parsed with column specification: ## cols( ## InvoiceNo = col_character(), ## StockCode = col_character(), ## Description = col_character(), ## Quantity = col_double(), ## InvoiceDate = col_datetime(format = \u0026quot;\u0026quot;), ## UnitPrice = col_double(), ## CustomerID = col_double(), ## Country = col_character() ## ) Finally our first usefull python to r functionality!\n 3.3.2 Let’s look at our data sales %\u0026gt;% head() ## # A tibble: 6 x 8 ## InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; ## 1 536365 22752 SET 7 BABU~ 2 2010-12-01 08:26:02 7.65 ## 2 536365 71053 WHITE META~ 6 2010-12-01 08:26:02 3.39 ## 3 536365 84029G KNITTED UN~ 6 2010-12-01 08:26:02 3.39 ## 4 536365 85123A WHITE HANG~ 6 2010-12-01 08:26:02 2.55 ## 5 536366 22633 HAND WARME~ 6 2010-12-01 08:28:02 1.85 ## 6 536367 21754 HOME BUILD~ 3 2010-12-01 08:33:59 5.95 ## # ... with 2 more variables: CustomerID \u0026lt;dbl\u0026gt;, Country \u0026lt;chr\u0026gt; sales %\u0026gt;% tail(3) ## # A tibble: 3 x 8 ## InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; ## 1 581587 22899 CHILDREN\u0026#39;S~ 6 2011-12-09 12:49:59 2.1 ## 2 581587 23254 CHILDRENS ~ 4 2011-12-09 12:49:59 4.15 ## 3 581587 23256 CHILDRENS ~ 4 2011-12-09 12:49:59 4.15 ## # ... with 2 more variables: CustomerID \u0026lt;dbl\u0026gt;, Country \u0026lt;chr\u0026gt;  3.3.3 Types of columns r If you payed attention read_ tries to inform what conversion was used in each column that is specially cool because base R tends to create unesceassary factor whne in fact you are working with strings, but know you can choose between three different implementation of the read command.\nA cool thing about tibbles is that they are in fact still data.frame.\nsales %\u0026gt;% class() ## [1] \u0026quot;spec_tbl_df\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot; Pay attention to the R difference between “[[” and “[” if you recall this is the “opposite” of the python behavior.\nJump to python implementation.\nsales[[\u0026quot;CustomerID\u0026quot;]] %\u0026gt;% class() ## [1] \u0026quot;numeric\u0026quot; sales[\u0026quot;CustomerID\u0026quot;] %\u0026gt;% class() ## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;  3.3.4 Basic Description real data using Glimpse and str sales %\u0026gt;% dim() ## [1] 325145 8 sales %\u0026gt;% colnames() ## [1] \u0026quot;InvoiceNo\u0026quot; \u0026quot;StockCode\u0026quot; \u0026quot;Description\u0026quot; \u0026quot;Quantity\u0026quot; \u0026quot;InvoiceDate\u0026quot; ## [6] \u0026quot;UnitPrice\u0026quot; \u0026quot;CustomerID\u0026quot; \u0026quot;Country\u0026quot; sales %\u0026gt;% glimpse() ## Observations: 325,145 ## Variables: 8 ## $ InvoiceNo \u0026lt;chr\u0026gt; \u0026quot;536365\u0026quot;, \u0026quot;536365\u0026quot;, \u0026quot;536365\u0026quot;, \u0026quot;536365\u0026quot;, \u0026quot;536366\u0026quot;, ... ## $ StockCode \u0026lt;chr\u0026gt; \u0026quot;22752\u0026quot;, \u0026quot;71053\u0026quot;, \u0026quot;84029G\u0026quot;, \u0026quot;85123A\u0026quot;, \u0026quot;22633\u0026quot;, \u0026quot;21... ## $ Description \u0026lt;chr\u0026gt; \u0026quot;SET 7 BABUSHKA NESTING BOXES\u0026quot;, \u0026quot;WHITE METAL LANTE... ## $ Quantity \u0026lt;dbl\u0026gt; 2, 6, 6, 6, 6, 3, 3, 4, 6, 6, 6, 8, 4, 3, 3, 48, 2... ## $ InvoiceDate \u0026lt;dttm\u0026gt; 2010-12-01 08:26:02, 2010-12-01 08:26:02, 2010-12... ## $ UnitPrice \u0026lt;dbl\u0026gt; 7.65, 3.39, 3.39, 2.55, 1.85, 5.95, 5.95, 7.95, 1.... ## $ CustomerID \u0026lt;dbl\u0026gt; 17850, 17850, 17850, 17850, 17850, 13047, 13047, 1... ## $ Country \u0026lt;chr\u0026gt; \u0026quot;United Kingdom\u0026quot;, \u0026quot;United Kingdom\u0026quot;, \u0026quot;United Kingdo... sales %\u0026gt;% str() ## Classes \u0026#39;spec_tbl_df\u0026#39;, \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 325145 obs. of 8 variables: ## $ InvoiceNo : chr \u0026quot;536365\u0026quot; \u0026quot;536365\u0026quot; \u0026quot;536365\u0026quot; \u0026quot;536365\u0026quot; ... ## $ StockCode : chr \u0026quot;22752\u0026quot; \u0026quot;71053\u0026quot; \u0026quot;84029G\u0026quot; \u0026quot;85123A\u0026quot; ... ## $ Description: chr \u0026quot;SET 7 BABUSHKA NESTING BOXES\u0026quot; \u0026quot;WHITE METAL LANTERN\u0026quot; \u0026quot;KNITTED UNION FLAG HOT WATER BOTTLE\u0026quot; \u0026quot;WHITE HANGING HEART T-LIGHT HOLDER\u0026quot; ... ## $ Quantity : num 2 6 6 6 6 3 3 4 6 6 ... ## $ InvoiceDate: POSIXct, format: \u0026quot;2010-12-01 08:26:02\u0026quot; \u0026quot;2010-12-01 08:26:02\u0026quot; ... ## $ UnitPrice : num 7.65 3.39 3.39 2.55 1.85 5.95 5.95 7.95 1.65 2.1 ... ## $ CustomerID : num 17850 17850 17850 17850 17850 ... ## $ Country : chr \u0026quot;United Kingdom\u0026quot; \u0026quot;United Kingdom\u0026quot; \u0026quot;United Kingdom\u0026quot; \u0026quot;United Kingdom\u0026quot; ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. InvoiceNo = col_character(), ## .. StockCode = col_character(), ## .. Description = col_character(), ## .. Quantity = col_double(), ## .. InvoiceDate = col_datetime(format = \u0026quot;\u0026quot;), ## .. UnitPrice = col_double(), ## .. CustomerID = col_double(), ## .. Country = col_character() ## .. ) sales %\u0026gt;% summary() ## InvoiceNo StockCode Description ## Length:325145 Length:325145 Length:325145 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## Quantity InvoiceDate UnitPrice ## Min. :-80995.00 Min. :2010-12-01 08:26:02 Min. :-11062.06 ## 1st Qu.: 1.00 1st Qu.:2011-03-28 12:13:02 1st Qu.: 1.25 ## Median : 3.00 Median :2011-07-20 10:50:59 Median : 2.08 ## Mean : 9.27 Mean :2011-07-04 14:11:43 Mean : 4.85 ## 3rd Qu.: 10.00 3rd Qu.:2011-10-19 10:47:59 3rd Qu.: 4.13 ## Max. : 12540.00 Max. :2011-12-09 12:49:59 Max. : 38970.00 ## ## CustomerID Country ## Min. :12347 Length:325145 ## 1st Qu.:13959 Class :character ## Median :15150 Mode :character ## Mean :15289 ## 3rd Qu.:16793 ## Max. :18287 ## NA\u0026#39;s :80991 If you agree with me that summary sucks on a data.frame object I am glad to show skimr, also if you don’t like summary behaviour on model outputs broom is there to save you, I will talk more about when I make an scikit-learn and caret + tidymodels post.\n 3.3.5 Subsetting Data with select or base R sales[1:4,] ## # A tibble: 4 x 8 ## InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; ## 1 536365 22752 SET 7 BABU~ 2 2010-12-01 08:26:02 7.65 ## 2 536365 71053 WHITE META~ 6 2010-12-01 08:26:02 3.39 ## 3 536365 84029G KNITTED UN~ 6 2010-12-01 08:26:02 3.39 ## 4 536365 85123A WHITE HANG~ 6 2010-12-01 08:26:02 2.55 ## # ... with 2 more variables: CustomerID \u0026lt;dbl\u0026gt;, Country \u0026lt;chr\u0026gt; sales$CustomerID %\u0026gt;% head() ## [1] 17850 17850 17850 17850 17850 13047 sales[[\u0026quot;CustomerID\u0026quot;]] %\u0026gt;% head() ## [1] 17850 17850 17850 17850 17850 13047 sales[,3] %\u0026gt;% head() ## # A tibble: 6 x 1 ## Description ## \u0026lt;chr\u0026gt; ## 1 SET 7 BABUSHKA NESTING BOXES ## 2 WHITE METAL LANTERN ## 3 KNITTED UNION FLAG HOT WATER BOTTLE ## 4 WHITE HANGING HEART T-LIGHT HOLDER ## 5 HAND WARMER UNION JACK ## 6 HOME BUILDING BLOCK WORD sales[1:5,3] ## # A tibble: 5 x 1 ## Description ## \u0026lt;chr\u0026gt; ## 1 SET 7 BABUSHKA NESTING BOXES ## 2 WHITE METAL LANTERN ## 3 KNITTED UNION FLAG HOT WATER BOTTLE ## 4 WHITE HANGING HEART T-LIGHT HOLDER ## 5 HAND WARMER UNION JACK sales$Revenue2 \u0026lt;- sales$Quantity * sales$UnitPrice sales[[\u0026quot;Revenue3\u0026quot;]] \u0026lt;- sales[[\u0026quot;Quantity\u0026quot;]] * sales[[\u0026quot;UnitPrice\u0026quot;]] # () show created objects # Strange behavior right here 6 rowns on head() (sales \u0026lt;- sales %\u0026gt;% mutate(Revenue = Quantity * UnitPrice)) %\u0026gt;% head() ## # A tibble: 6 x 11 ## InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; ## 1 536365 22752 SET 7 BABU~ 2 2010-12-01 08:26:02 7.65 ## 2 536365 71053 WHITE META~ 6 2010-12-01 08:26:02 3.39 ## 3 536365 84029G KNITTED UN~ 6 2010-12-01 08:26:02 3.39 ## 4 536365 85123A WHITE HANG~ 6 2010-12-01 08:26:02 2.55 ## 5 536366 22633 HAND WARME~ 6 2010-12-01 08:28:02 1.85 ## 6 536367 21754 HOME BUILD~ 3 2010-12-01 08:33:59 5.95 ## # ... with 5 more variables: CustomerID \u0026lt;dbl\u0026gt;, Country \u0026lt;chr\u0026gt;, ## # Revenue2 \u0026lt;dbl\u0026gt;, Revenue3 \u0026lt;dbl\u0026gt;, Revenue \u0026lt;dbl\u0026gt; sum(sales$Revenue == sales$Revenue2)/nrow(sales) ## [1] 1 sum(sales$Revenue == sales$Revenue3)/nrow(sales) ## [1] 1 sum(sales$Revenue2 == sales$Revenue3)/nrow(sales) ## [1] 1 # If there were any differences between our columns the sum would return \u0026lt;1   3.3.6 Creating a new smaller data frame using transmute and base raw_sales \u0026lt;- sales %\u0026gt;% select(Quantity, UnitPrice, Revenue) raw_sales %\u0026gt;% head() ## # A tibble: 6 x 3 ## Quantity UnitPrice Revenue ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2 7.65 15.3 ## 2 6 3.39 20.3 ## 3 6 3.39 20.3 ## 4 6 2.55 15.3 ## 5 6 1.85 11.1 ## 6 3 5.95 17.8 raw_sales %\u0026gt;% glimpse() ## Observations: 325,145 ## Variables: 3 ## $ Quantity \u0026lt;dbl\u0026gt; 2, 6, 6, 6, 6, 3, 3, 4, 6, 6, 6, 8, 4, 3, 3, 48, 24,... ## $ UnitPrice \u0026lt;dbl\u0026gt; 7.65, 3.39, 3.39, 2.55, 1.85, 5.95, 5.95, 7.95, 1.65... ## $ Revenue \u0026lt;dbl\u0026gt; 15.30, 20.34, 20.34, 15.30, 11.10, 17.85, 17.85, 31.... raw_sales %\u0026gt;% skimr::skim() ## Skim summary statistics ## n obs: 325145 ## n variables: 3 ## ## -- Variable type:numeric ------------------------------------------------ ## variable missing complete n mean sd p0 p25 p50 p75 ## Quantity 0 325145 325145 9.27 154.39 -80995 1 3 10 ## Revenue 0 325145 325145 17.43 331.85 -168469.6 3.4 9.48 17.4 ## UnitPrice 0 325145 325145 4.85 116.83 -11062.06 1.25 2.08 4.13 ## p100 hist ## 12540 \u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt; ## 38970 \u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt; ## 38970 \u0026lt;U+2581\u0026gt;\u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;  3.3.7 Ploting with ggplot sales %\u0026gt;% ggplot() + aes(x = InvoiceDate, y = Revenue) + geom_line()  3.3.8 Filtering and replace data Here I really couldn`t figure out an easy way to filter using this cancel tricky that works in python.\ncancels = sales$Revenue \u0026lt; 0 cancels %\u0026gt;% nrow() ## NULL invert_func \u0026lt;- function(cancel){ ifelse(cancel == 1, 0, 1) } sales2 = sales[invert_func(cancels),] sales2 %\u0026gt;% dim() ## [1] 319557 11 I really prefer the tidy way also.\nsales \u0026lt;- sales %\u0026gt;% filter(Revenue \u0026gt; 0)  3.3.9 Groupby example in tidyverse I prefer the tidy way here as well.\nCountryGroups \u0026lt;- sales %\u0026gt;% group_by(Country) %\u0026gt;% summarise(sum_revenue = sum(Revenue), number_cases = n()) %\u0026gt;% arrange(-sum_revenue) CountryGroups ## # A tibble: 38 x 3 ## Country sum_revenue number_cases ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 United Kingdom 5311080. 291129 ## 2 EIRE 176305. 4788 ## 3 Netherlands 165583. 1391 ## 4 Germany 138778. 5465 ## 5 France 127194. 5025 ## 6 Australia 79198. 726 ## 7 Spain 36117. 1420 ## 8 Switzerland 34315. 1169 ## 9 Belgium 24015. 1191 ## 10 Norway 23182. 658 ## # ... with 28 more rows skimr::skim(sales) ## Skim summary statistics ## n obs: 318036 ## n variables: 11 ## ## -- Variable type:character ---------------------------------------------- ## variable missing complete n min max empty n_unique ## Country 0 318036 318036 3 20 0 38 ## Description 0 318036 318036 6 35 0 3926 ## InvoiceNo 0 318036 318036 6 7 0 19107 ## StockCode 0 318036 318036 1 12 0 3835 ## ## -- Variable type:numeric ------------------------------------------------ ## variable missing complete n mean sd p0 p25 ## CustomerID 79261 238775 318036 15295.34 1713.1 12347 13969 ## Quantity 0 318036 318036 10.25 38.3 1 1 ## Revenue 0 318036 318036 19.78 104.17 0.001 3.75 ## Revenue2 0 318036 318036 19.78 104.17 0.001 3.75 ## Revenue3 0 318036 318036 19.78 104.17 0.001 3.75 ## UnitPrice 0 318036 318036 3.96 42.53 0.001 1.25 ## p50 p75 p100 hist ## 15157 16800 18287 \u0026lt;U+2587\u0026gt;\u0026lt;U+2586\u0026gt;\u0026lt;U+2587\u0026gt;\u0026lt;U+2587\u0026gt;\u0026lt;U+2586\u0026gt;\u0026lt;U+2586\u0026gt;\u0026lt;U+2586\u0026gt;\u0026lt;U+2587\u0026gt; ## 3 10 4800 \u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt; ## 9.9 17.7 38970 \u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt; ## 9.9 17.7 38970 \u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt; ## 9.9 17.7 38970 \u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt; ## 2.08 4.13 13541.33 \u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt; ## ## -- Variable type:POSIXct ------------------------------------------------ ## variable missing complete n min max median ## InvoiceDate 0 318036 318036 2010-12-01 2011-12-09 2011-07-20 ## n_unique ## 17750  3.3.10 Ploting an histogram using ggplot2 sales %\u0026gt;% filter(CustomerID == 17850) %\u0026gt;% ggplot() + aes(Revenue) + geom_histogram(bins = 20) Another example.\nsales %\u0026gt;% filter(StockCode == 71053) %\u0026gt;% ggplot() + aes(Revenue) + geom_histogram(bins = 20)  3.3.11 Handling Missing values in R Ok I got hand this one to python.\nsales2$CustomerID %\u0026gt;% table(useNA = \u0026#39;always\u0026#39;) %\u0026gt;% sort(decreasing = TRUE) %\u0026gt;% head(3) ## . ## 17850 \u0026lt;NA\u0026gt; ## 319557 0 This is just not simple enough luckly we can create functions for our afflictions, plus this is replacement as an side effect which sucks.\n#sales[sales[[\u0026quot;CustomerID\u0026quot;]] %\u0026gt;% is.na(),\u0026quot;CustomerID\u0026quot;] \u0026lt;- 0 This is an way better tidy way.\n# sales %\u0026gt;% mutate_if(is.numeric, funs(replace(., is.na(.), 0))) sales2 \u0026lt;- sales %\u0026gt;% mutate_at(vars(CustomerID), list( ~replace(., is.na(.), # function that check condition (na) 0) # value to replace could be mean(.,na.rm = T) ) ) Using an stronger method like mice even with an amazing multicore package takes too long for an blogpost, plus I really don’t think there should be an model for CustomerID here is some workflow if you need to split your data.\nnon_character_sales \u0026lt;- sales %\u0026gt;% select_if(function(col) is.numeric(col) | is.factor(col)) # or my favorite select_cases \u0026lt;- function(col) { is.numeric(col) | is.factor(col) } non_character_sales \u0026lt;- sales %\u0026gt;% select_if(select_cases) non_character_sales %\u0026gt;% head() ## # A tibble: 6 x 6 ## Quantity UnitPrice CustomerID Revenue2 Revenue3 Revenue ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2 7.65 17850 15.3 15.3 15.3 ## 2 6 3.39 17850 20.3 20.3 20.3 ## 3 6 3.39 17850 20.3 20.3 20.3 ## 4 6 2.55 17850 15.3 15.3 15.3 ## 5 6 1.85 17850 11.1 11.1 11.1 ## 6 3 5.95 13047 17.8 17.8 17.8 character_sales \u0026lt;- sales %\u0026gt;% select_if(negate(is.numeric)) character_sales %\u0026gt;% head() ## # A tibble: 6 x 5 ## InvoiceNo StockCode Description InvoiceDate Country ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; ## 1 536365 22752 SET 7 BABUSHKA NESTIN~ 2010-12-01 08:26:02 United Ki~ ## 2 536365 71053 WHITE METAL LANTERN 2010-12-01 08:26:02 United Ki~ ## 3 536365 84029G KNITTED UNION FLAG HO~ 2010-12-01 08:26:02 United Ki~ ## 4 536365 85123A WHITE HANGING HEART T~ 2010-12-01 08:26:02 United Ki~ ## 5 536366 22633 HAND WARMER UNION JACK 2010-12-01 08:28:02 United Ki~ ## 6 536367 21754 HOME BUILDING BLOCK W~ 2010-12-01 08:33:59 United Ki~ sales3 \u0026lt;- cbind(character_sales,non_character_sales) # if you need the same order sales3 \u0026lt;- sales3 %\u0026gt;% select(names(sales))   3.3.12 Replacing names with an case when aproach Don’t mix and match numbers and characters else this will cause an error.\nreplace_function \u0026lt;- function(country) { case_when( country == \u0026#39;United Kingdom\u0026#39; ~ \u0026quot;1\u0026quot;, country == \u0026#39;Netherlands\u0026#39; ~ \u0026quot;2\u0026quot;, country == \u0026#39;Germany\u0026#39; ~ \u0026quot;3\u0026quot;, country == \u0026#39;France\u0026#39; ~ \u0026quot;4\u0026quot;, country == \u0026#39;USA\u0026#39; ~ \u0026quot;5\u0026quot;, TRUE ~ country ) } sales3 \u0026lt;- sales3 %\u0026gt;% mutate(new = replace_function(Country)) sales3 %\u0026gt;% head() ## InvoiceNo StockCode Description Quantity ## 1 536365 22752 SET 7 BABUSHKA NESTING BOXES 2 ## 2 536365 71053 WHITE METAL LANTERN 6 ## 3 536365 84029G KNITTED UNION FLAG HOT WATER BOTTLE 6 ## 4 536365 85123A WHITE HANGING HEART T-LIGHT HOLDER 6 ## 5 536366 22633 HAND WARMER UNION JACK 6 ## 6 536367 21754 HOME BUILDING BLOCK WORD 3 ## InvoiceDate UnitPrice CustomerID Country Revenue2 ## 1 2010-12-01 08:26:02 7.65 17850 United Kingdom 15.30 ## 2 2010-12-01 08:26:02 3.39 17850 United Kingdom 20.34 ## 3 2010-12-01 08:26:02 3.39 17850 United Kingdom 20.34 ## 4 2010-12-01 08:26:02 2.55 17850 United Kingdom 15.30 ## 5 2010-12-01 08:28:02 1.85 17850 United Kingdom 11.10 ## 6 2010-12-01 08:33:59 5.95 13047 United Kingdom 17.85 ## Revenue3 Revenue new ## 1 15.30 15.30 1 ## 2 20.34 20.34 1 ## 3 20.34 20.34 1 ## 4 15.30 15.30 1 ## 5 11.10 11.10 1 ## 6 17.85 17.85 1 Two ways of solving our case_count deficiency.\nvalue_counts \u0026lt;- function(column, useNA = \u0026#39;always\u0026#39;, decreasing = TRUE) { column %\u0026gt;% table(useNA = useNA) %\u0026gt;% sort(decreasing = decreasing) } sales3[[\u0026quot;new\u0026quot;]] %\u0026gt;% value_counts() %\u0026gt;% head(7) ## . ## 1 3 4 EIRE Spain 2 Belgium ## 291129 5465 5025 4788 1420 1391 1191   3.4 Passing Objects to Python Simple example.\nsales2 = r.sales2 type(sales2) ## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; We can solve our value_counts problem by simply stealing from python then returning the results to r.\nsales3_solution = \\ r.\\ sales3.\\ new.\\ value_counts().\\ nlargest(7) If we want to continue working in r after the steal.\nsales3_solution = py$sales3_solution sales3_solution ## 1 3 4 EIRE Spain 2 Belgium ## 291129 5465 5025 4788 1420 1391 1191   ","date":1553299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553299200,"objectID":"fdf37e9a2cb5ba6f258876dc40e651f1","permalink":"/2019/03/23/exploratory-data-analysis-basic-pandas-and-dplyr/","publishdate":"2019-03-23T00:00:00Z","relpermalink":"/2019/03/23/exploratory-data-analysis-basic-pandas-and-dplyr/","section":"post","summary":"This is an basic example of how you can use either R or Python to accomplish the same goals, I really enjoy using the tidyverse but as you will see sometimes Python is just the more intuitive option. If you find yourself confused on whether a code chunk is an R or Python code please ask me or check my github page for this project.  1 Getting Started, we will use multiple functions from both languages 1.","tags":["R Markdown","reticulate","pandas","dplyr","tidyverse"],"title":"exploratory data analysis: basic pandas and dplyr","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536462000,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":"#external_link.\n","date":1461726000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461726000,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00-03:00","relpermalink":"/project/external-project/","section":"project","summary":"An r package for combining and testing multiples forecasts.","tags":["libraries","forecast"],"title":"Forecast Bonsai","type":"project"},{"authors":null,"categories":null,"content":"Creating this website and updating it weekly is my personal task for 2019.\nContent includes blogpost, eduction, accomplishments, my TCC and more.\n","date":1461726000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461726000,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00-03:00","relpermalink":"/project/internal-project/","section":"project","summary":"My first personal website.","tags":["Personal Website"],"title":"TwoSidesData","type":"project"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441076400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441076400,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-09-01T00:00:00-03:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372647600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372647600,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00-03:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"A mobile visual clothing search system is presented whereby a smart phone user can either choose a social networking image or capture a new photo of a person wearing clothing of interest and search for similar clothing in a large cloud-based ecommerce database. The phone's GPS location is used to re-rank results by retail store location, to inform the user of local stores where similar clothing items can be tried on.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c03d2cfd6d324043b856ddf2bfef8abe","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"","tags":null,"title":null,"type":"author"},{"authors":null,"categories":null,"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating.","tags":null,"title":"Slides","type":"slides"}]