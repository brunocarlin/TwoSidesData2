[{"authors":["admin"],"categories":null,"content":"Bruno Testaguzza Carlin is an student of Data Science at Insper.\nExperience using R and BI tools, improving Python skills.\nCurrently working at Bayer.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://twosidesdata.netlify.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Bruno Testaguzza Carlin is an student of Data Science at Insper.\nExperience using R and BI tools, improving Python skills.\nCurrently working at Bayer.","tags":null,"title":"Bruno Carlin","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536462000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536462000,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"https://twosidesdata.netlify.com/tutorial/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":"\rClick on the Slides button above to view the built-in slides feature.\n\r\rSlides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906560000,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://twosidesdata.netlify.com/talk/example/","publishdate":"2017-01-01T00:00:00-02:00","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Bruno Carlin"],"categories":["R and Python"],"content":"\r\rGoal\rPython\rImport libraries\rRead feather data frame\rDescribe the dataframe\rDrop features\rRename target\rNA enconding\rFeature Engeniring\rTarget Variable\rNA imputing\rFeature encoding\rPrepare df to export to r\r\rR\rImport df from python\rImport libraries\rEncode types\rExplore in r with data explorer\rMore exploration\rTrain test split\rRecipe for models\rPrep Data\rLogistic Regression\rMetrics Logistic\rMetrics Lasso\r\rRidge\rMetrics Ridge\r\rRandom Forest\rMetrics Random forest\r\rh2o\rStart CLuster\rUpload df’s\rFit auto ml\rModel results\rUsing a stacked model\rPerformance\r\r\rDALEX - Are machinge learning models Black Boxes?\rDalex X e Y\rModel Explainer\rFeature Importance\rVariable explanation\rAccumulated Local Effects Profiles aka ALEPlots\rFactor explanation\r\rSingle prediction explanation\r\r\r\rGoal\rRead data from Brazil’s cell phone companies and predict customer satisfaction\nlibrary(reticulate)\ruse_miniconda(\u0026quot;r-reticulate\u0026quot;,required = TRUE)\rfile_path \u0026lt;- here::here()\rfile_path_linux \u0026lt;- paste(file_path,\u0026quot;content\u0026quot;,\u0026quot;post\u0026quot;,\u0026quot;data\u0026quot;,sep = \u0026quot;/\u0026quot;)\r\rPython\rImport libraries\rimport pandas as pd\rimport numpy as np\r\rRead feather data frame\rdf1 = pd.read_feather(r.file_path_linux + \u0026quot;/BD_PRE.feather\u0026quot;)\r\rDescribe the dataframe\rdf1.describe()\r## IDTNS ANO_BASE ... PESO I2\r## count 1.284110e+05 128411.000000 ... 128411.000000 84441.000000\r## mean 2.062114e+07 2016.269774 ... 0.999992 1.179806\r## std 2.192746e+07 1.120365 ... 1.315625 0.384028\r## min 3.780000e+02 2015.000000 ... 0.015936 1.000000\r## 25% 6.160118e+06 2015.000000 ... 0.180556 1.000000\r## 50% 6.804225e+06 2016.000000 ... 0.601990 1.000000\r## 75% 4.105798e+07 2017.000000 ... 1.348837 1.000000\r## max 6.203986e+07 2018.000000 ... 10.965368 2.000000\r## ## [8 rows x 47 columns]\r\rDrop features\rMy group read the data dictionary and glanced at the data to decido to drop of multipe features with for low variance or too high cardinality\ndf1=df1.drop([\u0026quot;IDTNS\u0026quot;,\u0026quot;TIPO\u0026quot;,\u0026quot;DATA\u0026quot;,\u0026quot;H0\u0026quot;,\u0026quot;Q1\u0026quot;,\u0026quot;Q2\u0026quot;,\u0026quot;Q3\u0026quot;,\u0026quot;Q4\u0026quot;,\u0026quot;Q6\u0026quot;,\u0026quot;Q7\u0026quot;],axis=1)\rdf1.head()\r## OPERADORA ESTADO ANO_BASE Q5 Q8 ... H2 H2a I1 PESO I2\r## 0 OI RJ 2018 1 44 ... 999998 1 2 1.165414 2.0\r## 1 OI BA 2018 1 50 ... 999998 6 1 1.911877 1.0\r## 2 VIVO ES 2018 1 37 ... 1000 1 1 0.695489 1.0\r## 3 CLARO RR 2018 1 19 ... 999998 2 1 0.054054 1.0\r## 4 OI ES 2018 1 39 ... 999998 7 1 0.111111 1.0\r## ## [5 rows x 42 columns]\r\rRename target\rdf1 = df1.rename(columns = {\u0026#39;J1\u0026#39;:\u0026#39;Target\u0026#39;})\r\rNA enconding\rThe dictionary defined 99 as missing in multiple features\ndf2 = df1.copy()\rdf2[\u0026#39;B1_1\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;B1_2\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;C1_1\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;C1_2\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;D2_1\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;D2_2\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;D2_3\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;F5\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;F4\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;F2\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;A5\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;A4\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;A3\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;A2_1\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;A2_2\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;A2_3\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;E1_1\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;E1_2\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;E1_3\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;F4\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;F5\u0026#39;].replace([99], np.NaN,inplace = True)\rdf2[\u0026#39;F6\u0026#39;].replace([99], np.NaN,inplace = True)\rSometimes variations of missing like didn’t want to answer were also enconded as numbers so we encoded those ase missing as well\ndf2[\u0026#39;Q8\u0026#39;].replace([999999], np.NaN,inplace = True)\rdf2[\u0026#39;H1\u0026#39;].replace([99,99999], np.NaN,inplace = True)\rdf2[\u0026#39;H2\u0026#39;].replace([99997,99998,99999,100000,999998,999999], np.NaN,inplace = True)\r\rFeature Engeniring\rDroped H2a for now in order to code it as categories\ndf2.drop([\u0026quot;H2a\u0026quot;],inplace = True,axis = 1)\rdf3 = df2.copy()\rdf3.loc[(df3[\u0026quot;H2\u0026quot;] \u0026gt;=0) \u0026amp; (df3[\u0026quot;H2\u0026quot;] \u0026lt;1000), \u0026quot;RIQUEZA\u0026quot;]=1\rdf3.loc[(df3[\u0026quot;H2\u0026quot;] \u0026gt;=1000) \u0026amp; (df3[\u0026quot;H2\u0026quot;] \u0026lt;3000), \u0026quot;RIQUEZA\u0026quot;]=2\rdf3.loc[(df3[\u0026quot;H2\u0026quot;] \u0026gt;=3000) \u0026amp; (df3[\u0026quot;H2\u0026quot;] \u0026lt;6000), \u0026quot;RIQUEZA\u0026quot;]=3\rdf3.loc[(df3[\u0026quot;H2\u0026quot;] \u0026gt;=6000) \u0026amp; (df3[\u0026quot;H2\u0026quot;] \u0026lt;10000), \u0026quot;RIQUEZA\u0026quot;]=4\rdf3.loc[(df3[\u0026quot;H2\u0026quot;] \u0026gt;=10000) \u0026amp; (df3[\u0026quot;H2\u0026quot;] \u0026lt;15000), \u0026quot;RIQUEZA\u0026quot;]=5\rdf3.loc[(df3[\u0026quot;H2\u0026quot;] \u0026gt;=15000) \u0026amp; (df3[\u0026quot;H2\u0026quot;] \u0026lt;20000), \u0026quot;RIQUEZA\u0026quot;]=6\rdf3.loc[(df3[\u0026quot;H2\u0026quot;] \u0026gt;=20000), \u0026quot;RIQUEZA\u0026quot;]=7\rdf3.RIQUEZA.value_counts(dropna =False)\r## 2.0 48387\r## 1.0 33554\r## NaN 29784\r## 3.0 12543\r## 4.0 2704\r## 5.0 850\r## 7.0 315\r## 6.0 274\r## Name: RIQUEZA, dtype: int64\r\rTarget Variable\rWe decided with an nps system that scores above 8 were good scores, and encoded these cases as 1 and the rest as 0.\ndf3[\u0026#39;Target\u0026#39;].replace([99], np.NaN,inplace = True)\rdf3.loc[(df3[\u0026quot;Target\u0026quot;] \u0026lt;8) ,\u0026quot;Target2\u0026quot;]= 0\rdf3.loc[(df3[\u0026quot;Target\u0026quot;] \u0026gt;=8 ) ,\u0026quot;Target2\u0026quot;]= 1\rdf3.dropna(subset=[\u0026#39;Target\u0026#39;],inplace = True)\rVariaveis Categoricas Moda\rEstado\nOperadora\nRIQUEZA\nQ9\nI1\rD1\nQ5\nF1\rF3\nF5\nG1\nVariaveis Categoricas Missing Explicito\rA1_x\n\rNA imputing\rWe decided that these numeric features would be imputted with 0s a more robust approach could be taken but the main idea was for to create a simple model\ndf3[\u0026quot;A1_1\u0026quot;].fillna(0,inplace = True)\rdf3[\u0026quot;A1_2\u0026quot;].fillna(0,inplace = True)\rdf3[\u0026quot;A1_3\u0026quot;].fillna(0,inplace = True)\rdf3[\u0026quot;A1_4\u0026quot;].fillna(0,inplace = True)\rdf3[\u0026quot;F1\u0026quot;].fillna(0,inplace = True)\rdf3[\u0026quot;F3\u0026quot;].fillna(0,inplace = True)\rdf3[\u0026quot;F5\u0026quot;].fillna(0,inplace = True)\r\rFeature encoding\rWe originally hand encoded all the features in python, this would help to automate the predictions latter down the pipe\runfortunally when replicating the code it seems I have a bug on reticulate so I will do that in r instead\n# df3 = df3.astype({\u0026#39;Q9\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;I1\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;D1\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;Q5\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;F1\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;F3\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;F5\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;G1\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;A1_1\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;A1_2\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;A1_3\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;A1_4\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;RIQUEZA\u0026#39;: \u0026#39;category\u0026#39;})\r# df3 = df3.astype({\u0026#39;Target2\u0026#39;: \u0026#39;category\u0026#39;})\rdf3.dtypes\r## OPERADORA object\r## ESTADO object\r## ANO_BASE int64\r## Q5 int64\r## Q8 float64\r## Q8a int64\r## Q9 int64\r## Target float64\r## B1_1 float64\r## B1_2 float64\r## C1_1 float64\r## C1_2 float64\r## D1 int64\r## D2_1 float64\r## D2_2 float64\r## D2_3 float64\r## E1_1 float64\r## E1_2 float64\r## E1_3 float64\r## A1_1 float64\r## A1_2 float64\r## A1_3 float64\r## A1_4 float64\r## A2_1 float64\r## A2_2 float64\r## A2_3 float64\r## A3 float64\r## A4 float64\r## A5 float64\r## F1 int64\r## F2 float64\r## F3 int64\r## F4 float64\r## F5 float64\r## F6 float64\r## G1 int64\r## H1 float64\r## H2 float64\r## I1 int64\r## PESO float64\r## I2 float64\r## RIQUEZA float64\r## Target2 float64\r## dtype: object\r\rPrepare df to export to r\rdf4=df3.loc[:,[\u0026#39;Q5\u0026#39;,\u0026#39;Q8\u0026#39;,\u0026#39;Q8a\u0026#39;,\u0026#39;Q9\u0026#39;,\u0026#39;B1_1\u0026#39;,\u0026#39;B1_2\u0026#39;,\u0026#39;C1_1\u0026#39;,\u0026#39;C1_2\u0026#39;,\u0026#39;D1\u0026#39;,\u0026#39;D2_1\u0026#39;,\u0026#39;D2_2\u0026#39;,\u0026#39;D2_3\u0026#39;,\u0026#39;E1_1\u0026#39;,\u0026#39;E1_2\u0026#39;,\u0026#39;E1_3\u0026#39;,\u0026#39;A1_1\u0026#39;,\u0026#39;A1_2\u0026#39;,\u0026#39;A1_3\u0026#39;,\u0026#39;A1_4\u0026#39;,\u0026#39;F1\u0026#39;,\u0026#39;F3\u0026#39;,\u0026#39;F5\u0026#39;,\u0026#39;G1\u0026#39;,\u0026#39;H1\u0026#39;,\u0026#39;I1\u0026#39;,\u0026#39;PESO\u0026#39;,\u0026#39;RIQUEZA\u0026#39;,\u0026quot;Target2\u0026quot;]]\r\r\rR\rImport df from python\rdf_r \u0026lt;- py$df4\r\rImport libraries\rlibrary(DataExplorer)\rlibrary(tidyverse)\r## -- Attaching packages ----------------------------------------------------------------------------------------------- tidyverse 1.3.0 --\r## v ggplot2 3.3.0 v purrr 0.3.3\r## v tibble 3.0.0 v dplyr 0.8.5\r## v tidyr 1.0.2 v stringr 1.4.0\r## v readr 1.3.1 v forcats 0.5.0\r## -- Conflicts -------------------------------------------------------------------------------------------------- tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rlibrary(tidymodels)\r## -- Attaching packages ---------------------------------------------------------------------------------------------- tidymodels 0.1.0 --\r## v broom 0.5.5 v rsample 0.0.6 ## v dials 0.0.6 v tune 0.1.0 ## v infer 0.5.1 v workflows 0.1.1 ## v parsnip 0.1.1 v yardstick 0.0.6 ## v recipes 0.1.12\r## -- Conflicts ------------------------------------------------------------------------------------------------- tidymodels_conflicts() --\r## x scales::discard() masks purrr::discard()\r## x dplyr::filter() masks stats::filter()\r## x recipes::fixed() masks stringr::fixed()\r## x dplyr::lag() masks stats::lag()\r## x dials::margin() masks ggplot2::margin()\r## x yardstick::spec() masks readr::spec()\r## x recipes::step() masks stats::step()\rlibrary(furrr)\r## Loading required package: future\rlibrary(h2o)\r## ## ----------------------------------------------------------------------\r## ## Your next step is to start H2O:\r## \u0026gt; h2o.init()\r## ## For H2O package documentation, ask for help:\r## \u0026gt; ??h2o\r## ## After starting H2O, you can use the Web UI at http://localhost:54321\r## For more information visit http://docs.h2o.ai\r## ## ----------------------------------------------------------------------\r## ## Attaching package: \u0026#39;h2o\u0026#39;\r## The following objects are masked from \u0026#39;package:stats\u0026#39;:\r## ## cor, sd, var\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## %*%, %in%, \u0026amp;\u0026amp;, ||, apply, as.factor, as.numeric, colnames,\r## colnames\u0026lt;-, ifelse, is.character, is.factor, is.numeric, log,\r## log10, log1p, log2, round, signif, trunc\rlibrary(DALEX)\r## Welcome to DALEX (version: 1.2.1).\r## Find examples and detailed introduction at: https://pbiecek.github.io/ema/\r## Additional features will be available after installation of: ggpubr.\r## Use \u0026#39;install_dependencies()\u0026#39; to get all suggested dependencies\r## ## Attaching package: \u0026#39;DALEX\u0026#39;\r## The following object is masked from \u0026#39;package:dplyr\u0026#39;:\r## ## explain\rlibrary(DALEXtra)\rlibrary(iBreakDown)\rlibrary(ingredients)\r## ## Attaching package: \u0026#39;ingredients\u0026#39;\r## The following objects are masked from \u0026#39;package:iBreakDown\u0026#39;:\r## ## describe, plotD3\r## The following object is masked from \u0026#39;package:DALEX\u0026#39;:\r## ## feature_importance\rlibrary(probably)\r## ## Attaching package: \u0026#39;probably\u0026#39;\r## The following object is masked from \u0026#39;package:h2o\u0026#39;:\r## ## as.factor\r## The following objects are masked from \u0026#39;package:base\u0026#39;:\r## ## as.factor, as.ordered\r\rEncode types\rdf_r %\u0026gt;% glimpse()\r## Rows: 128,198\r## Columns: 28\r## $ Q5 \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,...\r## $ Q8 \u0026lt;dbl\u0026gt; 44, 50, 37, 19, 39, 38, NaN, 19, 22, 27, 24, 47, 29, 21, 40...\r## $ Q8a \u0026lt;dbl\u0026gt; 7, 7, 6, 3, 6, 6, 7, 3, 3, 4, 3, 7, 4, 3, 6, 3, 4, 8, 7, 6,...\r## $ Q9 \u0026lt;dbl\u0026gt; 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2,...\r## $ B1_1 \u0026lt;dbl\u0026gt; 7, 4, 9, 5, 10, 10, 8, 8, 10, 4, 0, 8, 5, 5, 10, 5, 7, 10, ...\r## $ B1_2 \u0026lt;dbl\u0026gt; 9, 3, 10, 6, NaN, 10, 8, 6, 9, 6, 5, 8, 5, 3, 10, 10, 5, 10...\r## $ C1_1 \u0026lt;dbl\u0026gt; 10, 3, 10, 8, 10, 8, 7, 10, 10, 8, 7, 10, 10, 8, 10, 2, 7, ...\r## $ C1_2 \u0026lt;dbl\u0026gt; 10, 4, 10, 9, 10, 9, 6, 10, 5, 9, 0, 8, 10, 2, 10, 9, 8, 10...\r## $ D1 \u0026lt;dbl\u0026gt; 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1,...\r## $ D2_1 \u0026lt;dbl\u0026gt; 8, NaN, 9, 4, 7, 10, NaN, 1, 10, 5, NaN, 6, NaN, 2, 8, 9, N...\r## $ D2_2 \u0026lt;dbl\u0026gt; 7, NaN, 7, 3, 5, 10, NaN, 1, 9, 7, NaN, 8, NaN, 0, 8, 7, Na...\r## $ D2_3 \u0026lt;dbl\u0026gt; 7, NaN, 7, 5, 5, 10, NaN, 1, 10, 6, NaN, 6, NaN, 0, 8, 8, N...\r## $ E1_1 \u0026lt;dbl\u0026gt; 8, 2, 9, 7, 8, 10, 7, 3, 9, 8, 0, 5, 7, 0, 10, 7, 7, 10, 10...\r## $ E1_2 \u0026lt;dbl\u0026gt; 8, 2, 9, 9, 10, 10, 7, 8, 9, 5, 0, 6, 5, 0, 10, 6, 7, 10, 1...\r## $ E1_3 \u0026lt;dbl\u0026gt; 10, 5, 9, 10, 8, 10, 8, 10, 10, 8, 0, 8, 5, 5, 10, 6, 8, 10...\r## $ A1_1 \u0026lt;dbl\u0026gt; 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,...\r## $ A1_2 \u0026lt;dbl\u0026gt; 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0,...\r## $ A1_3 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0,...\r## $ A1_4 \u0026lt;dbl\u0026gt; 97, 0, 0, 0, 97, 0, 0, 97, 0, 0, 97, 97, 97, 0, 0, 0, 0, 0,...\r## $ F1 \u0026lt;dbl\u0026gt; 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1,...\r## $ F3 \u0026lt;dbl\u0026gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2,...\r## $ F5 \u0026lt;dbl\u0026gt; 2, 0, 2, 1, 2, 2, 0, 1, 2, 1, 0, 2, 0, 2, 1, 2, 0, 0, 0, 2,...\r## $ G1 \u0026lt;dbl\u0026gt; 1, 1, 2, 2, 1, 2, 3, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\r## $ H1 \u0026lt;dbl\u0026gt; 3, NaN, 1, 3, 1, 5, 1, 2, 2, 1, 1, 1, 1, 4, 2, 2, 1, 2, 2, ...\r## $ I1 \u0026lt;dbl\u0026gt; 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2,...\r## $ PESO \u0026lt;dbl\u0026gt; 1.1654135, 1.9118774, 0.6954887, 0.0540541, 0.1111111, 0.11...\r## $ RIQUEZA \u0026lt;dbl\u0026gt; NaN, NaN, 2, NaN, NaN, 1, NaN, NaN, 2, 2, 1, NaN, 2, NaN, 2...\r## $ Target2 \u0026lt;dbl\u0026gt; 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,...\rcategory_pipe \u0026lt;- . %\u0026gt;% as.character() %\u0026gt;% if_else(. == \u0026quot;NaN\u0026quot;,NA_character_,.) %\u0026gt;% as_factor()\rdf_r \u0026lt;- df_r %\u0026gt;% mutate_at(vars(Q9,I1,D1,Q5,F1,F3,F5,G1,starts_with(\u0026quot;A1\u0026quot;),RIQUEZA,Target2),.funs = category_pipe)\r\rExplore in r with data explorer\rDataExplorer::introduce(df_r)\r## rows columns discrete_columns continuous_columns all_missing_columns\r## 1 128198 28 14 14 0\r## total_missing_values complete_rows total_observations memory_usage\r## 1 218989 51924 3589544 21551024\rDataExplorer::plot_intro(df_r)\rplot_missing(df_r)\r## Drop features\ndf_r \u0026lt;- df_r %\u0026gt;% select(-starts_with(\u0026quot;D2\u0026quot;))\rplot_missing(df_r)\r## Encode response in r\n df_r \u0026lt;- df_r %\u0026gt;% rename(response = Target2) %\u0026gt;% select(-PESO)\r\rMore exploration\rdf_r %\u0026gt;%\rmutate(response = response %\u0026gt;% fct_recode(bad = \u0026quot;0\u0026quot;,good =\u0026quot;1\u0026quot;)) %\u0026gt;% count(response) %\u0026gt;%\rggplot(aes(response, n, fill = response)) + geom_col(width = .5, show.legend = FALSE) + scale_y_continuous(labels = scales::comma) +\rscale_fill_manual(values = c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;)) +\rlabs(\rx = NULL,\ry = NULL,\rtitle = \u0026quot;Distribution of cases\u0026quot;\r)\r# Modeling\n\rTrain test split\rtelefone_initial_split \u0026lt;- df_r %\u0026gt;% rsample::initial_split(prop = 0.9)\rtelefone_initial_split\r## \u0026lt;Training/Validation/Total\u0026gt;\r## \u0026lt;115379/12819/128198\u0026gt;\rtrain_data \u0026lt;- training(telefone_initial_split)\rtest_data \u0026lt;- testing(telefone_initial_split)\r\rRecipe for models\rrecipe_telefone \u0026lt;- recipe(response ~.,data = train_data) %\u0026gt;%\r#step_upsample(response,skip = TRUE) %\u0026gt;% step_modeimpute(all_predictors(),-all_numeric()) %\u0026gt;% step_medianimpute(all_predictors(),-all_nominal()) %\u0026gt;% step_normalize(all_numeric()) %\u0026gt;% step_rm(RIQUEZA)\r#step_dummy(all_predictors(),-all_numeric())\r\rPrep Data\rsimple_model_recipe \u0026lt;- recipe_telefone %\u0026gt;%\rprep(retain = TRUE)\rsimple_train \u0026lt;- simple_model_recipe %\u0026gt;% juice()\rsimple_test \u0026lt;- simple_model_recipe %\u0026gt;% bake(test_data)\r\rLogistic Regression\rlogistic_regression \u0026lt;- logistic_reg(mode = \u0026quot;classification\u0026quot;,penalty = 0) %\u0026gt;%\rset_engine(\u0026quot;glmnet\u0026quot;) %\u0026gt;% fit(response ~.,data = simple_train)\rmetrics_log_reg \u0026lt;- logistic_regression %\u0026gt;% predict(simple_test) %\u0026gt;% bind_cols(simple_test %\u0026gt;% select(response)) %\u0026gt;% metrics(truth = response,estimate = .pred_class)\rmetrics_roc_auc \u0026lt;- logistic_regression %\u0026gt;% predict(simple_test,type = \u0026quot;prob\u0026quot;) %\u0026gt;% bind_cols(simple_test %\u0026gt;% select(response)) %\u0026gt;% roc_auc(truth = response,predictor =.pred_0)\rMetrics Logistic\rmetrics_log_reg\r## # A tibble: 2 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.804\r## 2 kap binary 0.608\rmetrics_roc_auc\r## # A tibble: 1 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 roc_auc binary 0.885\rI am going to keep using roc from now on\r## Lasso\nlasso_regression \u0026lt;- logistic_reg(mode = \u0026quot;classification\u0026quot;,mixture = 0) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;) %\u0026gt;% fit(response~ .,data = simple_train)\rlasso_roc_auc_cv \u0026lt;- lasso_regression %\u0026gt;% multi_predict(new_data = simple_test,type = \u0026quot;prob\u0026quot;) %\u0026gt;% bind_cols(simple_test) %\u0026gt;%\runnest() %\u0026gt;% group_by(penalty) %\u0026gt;% do(ok = roc_auc(.,truth = response,predictor = .pred_0)) %\u0026gt;% unnest() %\u0026gt;%\rspread(key = .metric,value = .estimate) %\u0026gt;%\rarrange(roc_auc %\u0026gt;% desc)\r## Warning: `cols` is now required.\r## Please use `cols = c(.pred)`\r## Warning: `cols` is now required.\r## Please use `cols = c(ok)`\r\rMetrics Lasso\rlasso_roc_auc_cv\r## # A tibble: 100 x 3\r## penalty .estimator roc_auc\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0.0273 binary 0.884\r## 2 0.0300 binary 0.884\r## 3 0.0329 binary 0.884\r## 4 0.0361 binary 0.884\r## 5 0.0396 binary 0.884\r## 6 0.0435 binary 0.884\r## 7 0.0477 binary 0.884\r## 8 0.0523 binary 0.884\r## 9 0.0575 binary 0.884\r## 10 0.0631 binary 0.884\r## # ... with 90 more rows\r\r\rRidge\rridge_regression \u0026lt;- logistic_reg(mode = \u0026quot;classification\u0026quot;,mixture = 1) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;) %\u0026gt;% fit(response~ .,data = simple_train)\rridge_results_cv \u0026lt;- ridge_regression %\u0026gt;% multi_predict(new_data = simple_test,type = \u0026quot;prob\u0026quot;) %\u0026gt;% bind_cols(simple_test) %\u0026gt;%\runnest() %\u0026gt;% group_by(penalty) %\u0026gt;% do(ok = roc_auc(.,truth = response,predictor = .pred_0)) %\u0026gt;% unnest() %\u0026gt;%\rspread(key = .metric,value = .estimate) %\u0026gt;%\rarrange(roc_auc %\u0026gt;% desc)\r## Warning: `cols` is now required.\r## Please use `cols = c(.pred)`\r## Warning: `cols` is now required.\r## Please use `cols = c(ok)`\rMetrics Ridge\rridge_results_cv\r## # A tibble: 65 x 3\r## penalty .estimator roc_auc\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0.00136 binary 0.885\r## 2 0.00149 binary 0.885\r## 3 0.00164 binary 0.885\r## 4 0.00124 binary 0.885\r## 5 0.00180 binary 0.885\r## 6 0.00103 binary 0.885\r## 7 0.00113 binary 0.885\r## 8 0.000936 binary 0.885\r## 9 0.000777 binary 0.885\r## 10 0.000708 binary 0.885\r## # ... with 55 more rows\r\r\rRandom Forest\r random_forest \u0026lt;- rand_forest(mode = \u0026quot;classification\u0026quot;,trees = 100) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% fit(response~ .,data = simple_train)\rMetrics Random forest\rThe best model currently\nrandom_forest %\u0026gt;% predict(simple_test,type = \u0026quot;prob\u0026quot;) %\u0026gt;% bind_cols(simple_test %\u0026gt;% select(response)) %\u0026gt;% roc_auc(truth = response,predictor =.pred_0)\r## # A tibble: 1 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 roc_auc binary 0.889\r\r\rh2o\rh2o is usually very fast but not fast enough for this blogpost but here is the code for it\nStart CLuster\r\rUpload df’s\r# simple_train_hex \u0026lt;- as.h2o(simple_train)\r# simple_test_hex = as.h2o(simple_test)\r# simple_y_hex \u0026lt;- simple_train %\u0026gt;% select(response) %\u0026gt;% pull %\u0026gt;% as.numeric()\r# simple_x_hex \u0026lt;- simple_train %\u0026gt;% select(-response)\r\rFit auto ml\rWith a 2 minutes timer\n# h2o.no_progress()\r# # aml \u0026lt;- h2o.automl(y = \u0026quot;response\u0026quot;,\r# training_frame = simple_train_hex,\r# max_runtime_secs = 120,\r# seed = 1)\r# \r\rModel results\r# pred \u0026lt;- h2o.predict(aml, simple_test_hex)\r# aml@leaderboard\r# model_ids \u0026lt;- as.data.frame(aml@leaderboard$model_id)[,1]\r# model_ids\r\rUsing a stacked model\r# best_h2o \u0026lt;- h2o.getModel(model_ids[model_ids %\u0026gt;% str_detect(\u0026quot;StackedEnsemble_BestOfFamily_AutoML\u0026quot;)])\r\rPerformance\r# result_predictions \u0026lt;- predict(best_h2o,simple_test_hex)\r# result_predictions %\u0026gt;% # as_tibble() %\u0026gt;% # bind_cols(simple_test) %\u0026gt;% # roc_auc(truth = response,predictor = p0)\r\r\r\rDALEX - Are machinge learning models Black Boxes?\rCode based from Dalex page\nDalex X e Y\rx_dalex \u0026lt;- simple_test %\u0026gt;% select(-response)\ry_dalex \u0026lt;- simple_test %\u0026gt;%\rtransmute(response = response %\u0026gt;%\ras.numeric()) %\u0026gt;% mutate(response = if_else(response == 1,\r0,\r1)) %\u0026gt;% as.data.frame()\ry_dalex \u0026lt;- y_dalex[,1]\r\rModel Explainer\rexplainer_log_reg \u0026lt;- DALEX::explain(logistic_regression, data=x_dalex, y=y_dalex, label=\u0026quot;logistic_reg\u0026quot;)\rexplainer_rf \u0026lt;- explain(random_forest,x_dalex,y_dalex,label =\u0026quot;random_forest\u0026quot;)\r\rFeature Importance\rmp_log_reg \u0026lt;- model_parts(explainer_log_reg)\rmp_rf \u0026lt;- model_parts(explainer_rf)\rplot(mp_log_reg,mp_rf)\r\rVariable explanation\rAccumulated Local Effects Profiles aka ALEPlots\rB1_2: Note in regards to how well the company has delivered on its publicity.\nadp_log_reg \u0026lt;- accumulated_dependence(explainer_log_reg,variables = \u0026quot;B1_2\u0026quot;)\radp_rf \u0026lt;- accumulated_dependence(explainer_rf,variables = \u0026quot;B1_2\u0026quot;)\rplot(adp_log_reg,adp_rf)\r\rFactor explanation\rG1: Does another company exist that is serving the same area:\nYes\rNo\rDon’t know\r\rexpl_log_reg \u0026lt;- accumulated_dependence(explainer_log_reg,variables = \u0026quot;G1\u0026quot;, variable_type = \u0026quot;categorical\u0026quot;)\rexpl_rf\u0026lt;- accumulated_dependence(explainer_rf,variables = \u0026quot;G1\u0026quot;, variable_type = \u0026quot;categorical\u0026quot;)\rplot(expl_log_reg,expl_rf)\r\r\rSingle prediction explanation\rOnly the first case\nbd_log_reg \u0026lt;- predict_parts(explainer_log_reg, x_dalex[1,])\rbd_rf \u0026lt;- predict_parts(explainer_rf, x_dalex[1,])\rLogistic Regression\nplot(bd_log_reg)\rRandom Forest\nplot(bd_rf)\rNot the coolest graph since unfortunately we use a normalization process, maybe in the future with the workflows package we can see better graphs\n\r\r","date":1590019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590019200,"objectID":"27457e6c8bfd52df1bc5db251f77010c","permalink":"https://twosidesdata.netlify.com/2020/05/21/exploratory-data-analysis-basics-part2/","publishdate":"2020-05-21T00:00:00Z","relpermalink":"/2020/05/21/exploratory-data-analysis-basics-part2/","section":"post","summary":"Part of our gradution grading exercises","tags":["R Markdown","reticulate","pandas","dplyr","modeling"],"title":"Presentation Data Science and Decision Making 1","type":"post"},{"authors":null,"categories":null,"content":"#external_link.\n","date":1589079600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589079600,"objectID":"21048ca8f4e00acdb4e56e5a59e1bedb","permalink":"https://twosidesdata.netlify.com/project/integrate-activity/","publishdate":"2020-05-10T00:00:00-03:00","relpermalink":"/project/integrate-activity/","section":"project","summary":"A r shiny app to analyse data from OLIST, done in a group for Insper Advanced Degree","tags":["shiny","portuguese"],"title":"Shiny OLIST","type":"project"},{"authors":["Bruno Carlin"],"categories":["R and Python"],"content":"\r\r\rLibraries\rSecond Post\r\rObjectives\rDefine the variables used in the conclusion\rUsing masks or other methods to filter the data\rVisualizing the hypothesis\rConclusion\rBefore we start\r\rReservations\rData Dictionary\r\r\rPython\r\rImporting the dataset from part 1\rDifference in means\rLinear Regression\r\rLinearity\rRandom\rNon-Collinearity\rExogeneity\rHomoscedasticity / Homogeneity of Variance/ Assumption of Equal Variance\r\rFitting the linear regression\r\rLinear Regression plots\rQQ plot\rScale-Location Plot\rLeverage plot\r\r\rFinal Remarks\r\rNext post\r\r\r\rLibraries\rLet’s see what version of python this env is running.\nreticulate::py_config()\r## python: C:/Users/bruno/AppData/Local/r-miniconda/envs/r-reticulate/python.exe\r## libpython: C:/Users/bruno/AppData/Local/r-miniconda/envs/r-reticulate/python36.dll\r## pythonhome: C:/Users/bruno/AppData/Local/r-miniconda/envs/r-reticulate\r## version: 3.6.12 (default, Dec 9 2020, 00:11:44) [MSC v.1916 64 bit (AMD64)]\r## Architecture: 64bit\r## numpy: C:/Users/bruno/AppData/Local/r-miniconda/envs/r-reticulate/Lib/site-packages/numpy\r## numpy_version: 1.19.5\rimport numpy as np\rimport matplotlib.pyplot as plt\rimport seaborn as sns\rimport pandas as pd\rimport scipy.stats as ss\rimport statsmodels.api as sm\rimport statsmodels.formula.api as smf\rimport os\rfrom statsmodels.graphics.gofplots import ProbPlot\r\rSecond Post\rObjectives\r\rDefine the variables used in the conclusion\rIn our case, we initially choose to use salary ~ sex,region region was added to test whether Simpson’s paradox was at play.\nBut then I augmented our analysis with a simple linear regression.\n\rUsing masks or other methods to filter the data\rWe used it once.\n\rVisualizing the hypothesis\rWe were advised to use two histograms combined to get a preview of our answer.\n\rConclusion\rComment on our findings.\n\rBefore we start\rReservations\rThis is an exercise where we were supposed to ask a relevant question using the data from the IBGE(Brazil’s main data collector) database of 1970.\nOur group decided to ask whether women received less than man, we expanded the analysis hoping to avoid the Simpson’s paradox.\nThis is just an basic inference, and it’s results are therefore only used for studying purposes I don’t believe any finding would be relevant using just this approach but some basic operations can be used in a more impact full work.\n\rData Dictionary\rWe got a Data Dictionary that will be very useful for our Analysis, it contains all the required information about the encoding of the columns and the intended format that the folks at STATA desired.\n\r\rPortuguese\r\r\rDescrição do Registro de Indivíduos nos EUA.\nDataset do software STATA (pago), vamos abri-lo com o pandas e transforma-lo em DataFrame.\nVariável 1 – CHAVE DO INDIVÍDUO ? Formato N - Numérico ? Tamanho 11 dígitos (11 bytes) ? Descrição Sumária Identifica unicamente o indivíduo na amostra.\nVariável 2 - IDADE CALCULADA EM ANOS ? Formato N - Numérico ? Tamanho 3 dígitos (3 bytes) ? Descrição Sumária Identifica a idade do morador em anos completos.\nVariável 3 – SEXO ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 3 ? Descrição Sumária Identifica o sexo do morador. Categorias (1) homem, (2) mulher e (3) gestante.\nVariável 4 – ANOS DE ESTUDO ? Formato N - Numérico ? Tamanho 2 dígitos (2 bytes) ? Quantidade de Categorias 11 ? Descrição Sumária Identifica o número de anos de estudo do morador. Categorias (05) Cinco ou menos, (06) Seis, (07) Sete, (08) Oito, (09) Nove, (10) Dez, (11) Onze, (12) Doze, (13) Treze, (14) Quatorze, (15) Quinze ou mais.\nVariável 5 – COR OU RAÇA ? Formato N - Numérico ? Tamanho 2 dígitos (2 bytes) ? Quantidade de Categorias 6 ? Descrição Sumária Identifica a Cor ou Raça declarada pelo morador. Categorias (01) Branca, (02) Preta, (03) Amarela, (04) Parda, (05) Indígena e (09) Não Sabe.\nVariável 6 – VALOR DO SALÁRIO (ANUALIZADO) ? Formato N - Numérico ? Tamanho 8 dígitos (8 bytes) ? Quantidade de Decimais 2 ? Descrição Sumária Identifica o valor resultante do salário anual do indivíduo. Categorias especiais (-1) indivíduo ausente na data da pesquisa e (999999) indivíduo não quis responder.\nVariável 7 – ESTADO CIVIL ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 2 ? Descrição Sumária Dummy que identifica o estado civil declarado pelo morador. Categorias (1) Casado, (0) não casado.\nVariável 8 – REGIÃO GEOGRÁFICA ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 5 ? Descrição Sumária Identifica a região geográfica do morador. Categorias (1) Norte, (2) Nordeste, (3) Sudeste, (4) Sul e (5) Centro-oeste.\n\r\r\rEnglish\r\r\rDescription of the US Individual Registry.\nDataset of the STATA software (paid), we will open it with pandas and turn it into DataFrame.\nVariable 1 - KEY OF THE INDIVIDUAL? Format N - Numeric? Size 11 digits (11 bytes)? Summary Description Uniquely identifies the individual in the sample.\nVariable 2 - AGE CALCULATED IN YEARS? Format N - Numeric? Size 3 digits (3 bytes)? Summary Description Identifies the age of the resident in full years.\nVariable 3 - SEX? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 3? Summary Description Identifies the gender of the resident. Categories (1) men, (2) women and (3) pregnant women.\nVariable 4 - YEARS OF STUDY? Format N - Numeric? Size 2 digits (2 bytes)? Number of Categories 11? Summary Description Identifies the number of years of study of the resident. Categories (05) Five or less, (06) Six, (07) Seven, (08) Eight, (09) Nine, (10) Dec, (11) Eleven, (12) Twelve, (13) Thirteen, (14 ) Fourteen, (15) Fifteen or more.\nVariable 5 - COLOR OR RACE? Format N - Numeric? Size 2 digits (2 bytes)? Number of Categories 6? Summary Description Identifies the Color or Race declared by the resident. Categories (01) White, (02) Black, (03) Yellow, (04) Brown, (05) Indigenous and (09) Don’t know.\nVariable 6 - WAGE VALUE (ANNUALIZED)? Format N - Numeric? Size 8 digits (8 bytes)? Number of decimals 2? Summary Description Identifies the amount resulting from the individual’s annual salary. Special categories (-1) individual absent on the survey date and (999999) individual did not want to answer.\nVariable 7 - CIVIL STATE? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 2? Summary Description Dummy that identifies the marital status declared by the resident. Categories (1) Married, (0) Not married.\nVariable 8 - GEOGRAPHICAL REGION? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 5? Summary Description Identifies the resident’s geographic region. Categories (1) North, (2) Northeast, (3) Southeast, (4) South and (5) Midwest.\n\r\r\r\rPython\rImporting the dataset from part 1\rYou can also dowload it from the github page from this blog\ndf_sex_thesis =pd.read_feather(r.file_path_linux + \u0026#39;/sex_thesis_assignment.feather\u0026#39;)\rdf_sex_thesis.info()\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r## RangeIndex: 65795 entries, 0 to 65794\r## Data columns (total 9 columns):\r## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 index 65795 non-null int64 ## 1 age 65795 non-null int64 ## 2 sex 65795 non-null object ## 3 years_study 65795 non-null category\r## 4 color_race 65795 non-null object ## 5 salary 65795 non-null float64 ## 6 civil_status 65795 non-null object ## 7 region 65795 non-null object ## 8 log_salary 65795 non-null float64 ## dtypes: category(1), float64(2), int64(2), object(4)\r## memory usage: 4.1+ MB\rLet’s get going first define which variables to add to the hypothesis, to isolate the factor of salary ~ sex, if we consider that our sample of individuals is random in nature comparing the means of the individuals given their sex and seeing if there is a significant difference in their means.\nA good graphic to get an idea if these effects would be significant was the bar plots used in part 1.\nWhen working with Categorical variable it is possible to use a groupby approach to glimpse at the difference in means.\n\rDifference in means\rUsing the log salary feature from post 1.\ndf_agg1 = df_sex_thesis.groupby(\u0026#39;sex\u0026#39;).mean().log_salary\rdf_agg1\r## sex\r## man 9.026607\r## woman 8.607023\r## Name: log_salary, dtype: float64\rRemember that in order to transform back our log variables you can do e^variable like this e ^ 9.03 is 8321.57 but the log of the mean is not the same as the mean of the log.\ndf_agg2 = df_sex_thesis.groupby(\u0026#39;sex\u0026#39;).mean().salary\rdf_agg2\r## sex\r## man 14302.491879\r## woman 10642.502734\r## Name: salary, dtype: float64\r9.03 is not the same as 9.57\nTherefore which one should be done first log or mean?\nThe most common order is log then mean, because it is the order that reduces variance the most, you can read more about this here\n\r\rGroup by explanation\r\r\rGroup by in pandas is a method that accepts a list of elements in this case just ‘sex’ and applies consequent operation in each group, in this case the mean method from a pandas DataFrame, .salary returns just the mean for the salary variable.\n\rdf_sex_thesis.groupby([\u0026#39;sex\u0026#39;,\u0026#39;region\u0026#39;]).mean().log_salary\r## sex region ## man midwest 9.155421\r## north 8.678263\r## south 9.123554\r## southeast 9.113084\r## woman midwest 8.847172\r## north 8.291580\r## northeast 9.462870\r## south 8.345867\r## southeast 8.766985\r## Name: log_salary, dtype: float64\rAdding standard deviations\ndf_sex_thesis.groupby([\u0026#39;sex\u0026#39;]).std().log_salary\r## sex\r## man 1.397496\r## woman 2.009225\r## Name: log_salary, dtype: float64\rThese are really big Standard deviations! Remembering from stats that +2 SD’s gives about a 95% confidence interval we are not even close.\nCombining mean and std using pandas agg method.\ndf_agg =df_sex_thesis.groupby([\u0026#39;sex\u0026#39;]).agg([\u0026#39;mean\u0026#39;,\u0026#39;std\u0026#39;]).log_salary\rCalculating boundaries\ndf_agg[\u0026#39;lower_bound\u0026#39;] = df_agg[\u0026#39;mean\u0026#39;] - df_agg[\u0026#39;std\u0026#39;] * 2\rdf_agg[\u0026#39;upper_bound\u0026#39;] = df_agg[\u0026#39;mean\u0026#39;] + df_agg[\u0026#39;std\u0026#39;] * 2\rdf_agg\r## mean std lower_bound upper_bound\r## sex ## man 9.026607 1.397496 6.231615 11.821599\r## woman 8.607023 2.009225 4.588573 12.625473\rCool, but to verbose to be repeated multiple times, it is better to convert this series of operations into a function.\ndef groupby_bound(df,groupby_variables,value_variables):\rdf_agg = df.groupby(groupby_variables).agg([\u0026#39;mean\u0026#39;,\u0026#39;std\u0026#39;])[value_variables]\rdf_agg[\u0026#39;lower_bound\u0026#39;] = df_agg[\u0026#39;mean\u0026#39;] - df_agg[\u0026#39;std\u0026#39;] * 2\rdf_agg[\u0026#39;upper_bound\u0026#39;] = df_agg[\u0026#39;mean\u0026#39;] + df_agg[\u0026#39;std\u0026#39;] * 2\rreturn df_agg\rgroupby_bound(df=df_sex_thesis,groupby_variables=\u0026#39;sex\u0026#39;,value_variables=\u0026#39;log_salary\u0026#39;)\r## mean std lower_bound upper_bound\r## sex ## man 9.026607 1.397496 6.231615 11.821599\r## woman 8.607023 2.009225 4.588573 12.625473\rLet’s try to find the difference in salary on some strata of the population.\ngroupby_bound(df=df_sex_thesis,groupby_variables=[\u0026#39;sex\u0026#39;,\u0026#39;region\u0026#39;],value_variables=\u0026#39;log_salary\u0026#39;)\r## mean std lower_bound upper_bound\r## sex region ## man midwest 9.155421 1.192631 6.770160 11.540683\r## north 8.678263 1.733378 5.211507 12.145019\r## south 9.123554 1.426591 6.270371 11.976736\r## southeast 9.113084 1.226845 6.659395 11.566773\r## woman midwest 8.847172 1.575228 5.696717 11.997627\r## north 8.291580 2.470543 3.350495 13.232665\r## northeast 9.462870 1.172049 7.118773 11.806968\r## south 8.345867 2.461307 3.423253 13.268482\r## southeast 8.766985 1.639338 5.488309 12.045660\rNo.\ngroupby_bound(df=df_sex_thesis,groupby_variables=[\u0026#39;sex\u0026#39;,\u0026#39;civil_status\u0026#39;],value_variables=\u0026#39;log_salary\u0026#39;)\r## mean std lower_bound upper_bound\r## sex civil_status ## man married 9.173335 1.247871 6.677593 11.669077\r## not_married 8.810261 1.567870 5.674521 11.946001\r## woman married 8.487709 2.263061 3.961586 13.013831\r## not_married 8.771966 1.578347 5.615272 11.928659\rNo.\ngroupby_bound(df=df_sex_thesis,groupby_variables=[\u0026#39;sex\u0026#39;,\u0026#39;civil_status\u0026#39;],value_variables=\u0026#39;log_salary\u0026#39;)\r## mean std lower_bound upper_bound\r## sex civil_status ## man married 9.173335 1.247871 6.677593 11.669077\r## not_married 8.810261 1.567870 5.674521 11.946001\r## woman married 8.487709 2.263061 3.961586 13.013831\r## not_married 8.771966 1.578347 5.615272 11.928659\rNo.\ngroupby_bound(df=df_sex_thesis,groupby_variables=[\u0026#39;sex\u0026#39;,\u0026#39;color_race\u0026#39;],value_variables=\u0026#39;log_salary\u0026#39;)\r## mean std lower_bound upper_bound\r## sex color_race ## man black 8.885541 1.215289 6.454962 11.316119\r## brown 8.878127 1.348974 6.180179 11.576076\r## indigenous 7.480596 3.328801 0.822994 14.138197\r## white 9.215328 1.369700 6.475927 11.954728\r## yellow 9.503222 1.398690 6.705843 12.300601\r## woman black 8.617966 1.683712 5.250543 11.985389\r## brown 8.518060 2.025707 4.466647 12.569473\r## indigenous 7.623917 3.342682 0.938553 14.309282\r## white 8.696991 2.001864 4.693263 12.700718\r## yellow 8.904886 1.877233 5.150420 12.659351\rNo.\ngroupby_bound(df=df_sex_thesis,groupby_variables=[\u0026#39;sex\u0026#39;,\u0026#39;years_study\u0026#39;],value_variables=\u0026#39;log_salary\u0026#39;)\r## mean std lower_bound upper_bound\r## sex years_study ## man 5.0 8.702990 1.495661 5.711668 11.694312\r## 6.0 8.836372 1.285517 6.265338 11.407407\r## 7.0 8.883292 1.302857 6.277578 11.489006\r## 8.0 8.939945 1.342110 6.255724 11.624166\r## 9.0 8.873640 1.292672 6.288296 11.458984\r## 10.0 9.064804 1.122187 6.820430 11.309177\r## 11.0 9.182335 1.163925 6.854486 11.510184\r## 12.0 9.274507 1.187977 6.898553 11.650462\r## 13.0 9.308213 1.651300 6.005613 12.610812\r## 14.0 9.563542 1.295638 6.972267 12.154817\r## 15.0 10.083954 1.334792 7.414369 12.753538\r## woman 5.0 8.150536 2.574328 3.001881 13.299191\r## 6.0 8.577851 1.854171 4.869508 12.286194\r## 7.0 8.572043 1.743439 5.085165 12.058921\r## 8.0 8.480459 1.952266 4.575927 12.384991\r## 9.0 8.609875 1.583736 5.442403 11.777346\r## 10.0 8.735525 1.403048 5.929428 11.541622\r## 11.0 8.755282 1.518318 5.718647 11.791918\r## 12.0 8.906622 1.505541 5.895540 11.917705\r## 13.0 8.963868 1.555327 5.853213 12.074523\r## 14.0 9.146458 1.263748 6.618962 11.673954\r## 15.0 9.572653 1.228241 7.116171 12.029134\rAlso no.\nDoes that mean that there were no Gender pay differences in Brazil in 1970?\nNo, it just means that there were no signs of this difference when looking at the whole population combined with one extra factor, but what if we combine all factors and isolate each influence in the salary? This would be a way to analyse the Ceteris Paribus(all else equal) effect of each feature in the salary, here is where Linear Regression comes in.\n\rLinear Regression\rBut what is Linear Regression? you might ask, wasn’t it just one method for prediction? Not really, Linear Regression coefficients are really useful for hypothesis testing, meaning that tossing everything at it and then interpreting the results that come out without having to individually compare each feature pair, while also capturing the effect that all features have simultaneously.\nIs Linear Regression always perfect? No. In fact most of the time the results are a little biased or a underestimate the variance or are just flat out wrong.\nTo understand the kinds of errors we might face when doing a linear regression we can use the Gauss Markov Theorem.\nTerminology:\nPredictor/Independent Variable: Theses are the features e.g sex,years_study, region we can have p predictors where p = n -1 and n is the numbers of rows our dataset possesses in this case 65795 rows are present.\nPredicted/Dependent variable: This is the single “column” also called ‘target’ that we are modeling in this case we can use either log_salary or salary.\nfor a more in depth read this great blog post and for a more in depth usage in R and Python\nLinearity\rTo get good results using Linear Regression the relationship of the Predictors and the Predicted variable has to be a linear relationship, to check for Linearity it is possible to use a simple line plot, and look for patterns like a parabola that would indicate that the Predictor has a quadratic relationship with the Dependent variable, there are ways of fixing non-linear relationships like we did with log_salary or by taking the power of the Predictor.\nLinearity can easily be tested for numerical Predictors, categorical predictors are harder to test, so in our case we only checked the age feature.\nNow it is time to flex these matplotlib graphs…\nx = df_sex_thesis[\u0026#39;age\u0026#39;]\ry = df_sex_thesis[\u0026#39;log_salary\u0026#39;]\rplt.scatter(x, y)\rz = np.polyfit(x, y, 1)\rp = np.poly1d(z)\rplt.plot(x,p(x),\u0026quot;r--\u0026quot;)\rplt.show()\rIt seems age is not a great fit let’s try to also log age as well.\nx = np.log(df_sex_thesis[\u0026#39;age\u0026#39;])\ry = df_sex_thesis[\u0026#39;log_salary\u0026#39;]\rplt.scatter(x, y)\rz = np.polyfit(x, y, 1)\rp = np.poly1d(z)\rplt.plot(x,p(x),\u0026quot;r--\u0026quot;)\rplt.show()\rBetter but still close to no impact, maybe if we filter our sample to just the earning population, we can improve on it, we can call theses filters ‘masks’ in pandas.\ndf_filter = df_sex_thesis[df_sex_thesis[\u0026#39;log_salary\u0026#39;]\u0026gt; 2]\rdf_filter\r## index age sex ... civil_status region log_salary\r## 0 0 53 man ... married north 11.060384\r## 1 1 49 woman ... married north 9.427336\r## 2 2 22 woman ... not_married northeast 8.378713\r## 3 3 55 man ... married north 11.478344\r## 4 4 56 woman ... married north 11.969090\r## ... ... ... ... ... ... ... ...\r## 65790 66465 34 woman ... married midwest 9.427336\r## 65791 66466 40 man ... married midwest 7.793999\r## 65792 66467 36 woman ... married midwest 7.793999\r## 65793 66468 27 woman ... married midwest 8.617075\r## 65794 66469 37 man ... married midwest 6.134157\r## ## [63973 rows x 9 columns]\rx = df_filter[\u0026#39;age\u0026#39;]\ry = df_filter[\u0026#39;log_salary\u0026#39;]\rplt.scatter(x, y)\rz = np.polyfit(x, y, 1)\rp = np.poly1d(z)\rplt.plot(x,p(x),\u0026quot;r--\u0026quot;)\rplt.show()\rThere is some slight improvement, it is a good question whether to filter otherwise sane values, the point is that the entire analysis would change, changing to salary ~ sex in the earning population in 1970 in Brazil instead of the salary ~ sex for the whole population in 1970 in Brazil.\nI think in both cases analyzing the Gender Pay Gap would be interesting it is even possible to split the hypothesis in two, analysing if Men and Women earn the same, and if Men and Women are have the same employment rate.\nSo for here on out We are analyzing just the Gender Pay Difference of employed people.\n\rRandom\rThis is a vital hypotheses it means that the observations(rows) were chosen at random for the entire Brazilian population, in this case We choose to trust that IBGE did a good job, if IBGE failed to correctly sample the population or if we mess to much with our filters we risk invalidating the whole process, yes that is right, if you don’t respect this hypothesis everything you have analysed is worthless.\n\rNon-Collinearity\rThe effect of each Predictors is reduced when you introduce Colinear predictors, you are spliting the effect between the Predictors whenever a new predictor is added, meaning that you are in the worst case only calculating half of the coefficient, Collinearity always happens, Women live more so age is related to Sex, therefore age ‘steals’ part of the calculated effect from Sex, the more variables you introduce to your Linear Regression model the more that Collinearity plagues your estimations, everything is correlated.\nSo be careful when doing Linear Regression for estimating Ceteris Paribus effects so that you don’t introduce too many features or features that are too correlated with you hypothesis, remember that our hypothesis is Salary ~ Sex.\nA good way too know if you are introducing too much Collinearity is looking at the heatmap.\ncorr = pd.get_dummies(df_sex_thesis[[\u0026#39;sex\u0026#39;,\u0026#39;age\u0026#39;]]).corr()\rsns.heatmap(corr)\rThis is quite cloudy let’s get rid of the Sex interaction with itself.\nWe are using a really cool pandas operation inspired this stack overflow answer, and combining it with the negate operator ‘~’ effectively selecting just the columns that don’t start with sex.\nnew_corr = corr.loc[:,~corr.columns.str.startswith(\u0026#39;sex\u0026#39;)]\rsns.heatmap(new_corr)\rVery little correlation, we are fine.\nOnce again we don’t usually calculate the correlation between Categorical Variables.\n\rExogeneity\rIf violated this hypothesis blasts your study into oblivion, Exogeneity is a one way road, your Independent Variables influence your Dependent Variable, and that is it.\nDiscussion on whether we are violating this assumption creates really cool intellectual pursuits, Nobel’s were won discovering if there was some violation to this assumption see Trygve_Haavelmo.\nIn our case let’s hypothesize for all variables\nSex ~ Salary - Maybe people that get richer/poorer change Sex, probably not.\nAge ~ Salary - You can’t buy year with money .\rYears Study ~ Salary - Possible but this probably only happen to the latter years of education, still worth considering.\rColor/Race ~ Salary - No.\rCivil Status ~ Salary - Yes I can see that, taking this feature out.\nRegion ~ Salary - Do richer people migrate to richer regions? I think so, taking this feature out.\nIt is also nice to notice that this may be reason why we call the Predicted Variable the Independent Variable.\n\rHomoscedasticity / Homogeneity of Variance/ Assumption of Equal Variance\rAssumption of Equal Variance of predicted values means that for any value for the whole distribution of the Dependent Variable the estimated values remain equally distributted, meaning that we are as sure on our predictions for 1000 moneys as for 100000 moneys this assumption is really hard to adhere.\nIf broken the variance of the coefficients may be under or over estimated, meaning that we may fail to consider relevant features or consider wrongly irrelevant features, there are many formal statistical tests for this assumption let’s use scipy’s Bartlett’s test for homogeneity of variances where Ho is Homoscedasticity confirmation meaning we hope for p-values \u0026lt; 0.05.\nWe used a significance level of 5% for this assignment.\nss.bartlett(df_filter[\u0026#39;log_salary\u0026#39;],df_filter[\u0026#39;age\u0026#39;])\r## BartlettResult(statistic=232468.57113474328, pvalue=0.0)\rDon’t reject H0 -\u0026gt; ok\nAnother way to check this assumption is using tests called Breusch-Pagan and Goldfeld-Quandt post fitting the linear model.\n\r\rFitting the linear regression\rFitting the linear regression using yet another library called statsmodels.\nmod = smf.ols(formula=\u0026#39;log_salary ~ sex + age + years_study + color_race\u0026#39;, data=df_filter)\rmodel_fit = mod.fit()\rprint(model_fit.summary())\r## OLS Regression Results ## ==============================================================================\r## Dep. Variable: log_salary R-squared: 0.133\r## Model: OLS Adj. R-squared: 0.133\r## Method: Least Squares F-statistic: 613.6\r## Date: Thu, 21 Jan 2021 Prob (F-statistic): 0.00\r## Time: 01:05:49 Log-Likelihood: -81546.\r## No. Observations: 63973 AIC: 1.631e+05\r## Df Residuals: 63956 BIC: 1.633e+05\r## Df Model: 16 ## Covariance Type: nonrobust ## ============================================================================================\r## coef std err t P\u0026gt;|t| [0.025 0.975]\r## --------------------------------------------------------------------------------------------\r## Intercept 8.3200 0.019 440.777 0.000 8.283 8.357\r## sex[T.woman] -0.2125 0.007 -30.970 0.000 -0.226 -0.199\r## years_study[T.6.0] 0.1520 0.020 7.756 0.000 0.114 0.190\r## years_study[T.7.0] 0.1728 0.018 9.441 0.000 0.137 0.209\r## years_study[T.8.0] 0.1721 0.014 12.391 0.000 0.145 0.199\r## years_study[T.9.0] 0.1395 0.019 7.449 0.000 0.103 0.176\r## years_study[T.10.0] 0.2414 0.018 13.444 0.000 0.206 0.277\r## years_study[T.11.0] 0.3314 0.009 35.414 0.000 0.313 0.350\r## years_study[T.12.0] 0.3826 0.018 21.100 0.000 0.347 0.418\r## years_study[T.13.0] 0.5989 0.025 24.032 0.000 0.550 0.648\r## years_study[T.14.0] 0.6619 0.025 25.988 0.000 0.612 0.712\r## years_study[T.15.0] 1.0396 0.013 78.438 0.000 1.014 1.066\r## color_race[T.brown] 0.0487 0.013 3.696 0.000 0.023 0.075\r## color_race[T.indigenous] 0.0587 0.040 1.451 0.147 -0.021 0.138\r## color_race[T.white] 0.1805 0.013 13.736 0.000 0.155 0.206\r## color_race[T.yellow] 0.2401 0.050 4.776 0.000 0.142 0.339\r## age 0.0129 0.000 40.197 0.000 0.012 0.014\r## ==============================================================================\r## Omnibus: 14771.140 Durbin-Watson: 1.806\r## Prob(Omnibus): 0.000 Jarque-Bera (JB): 45603.025\r## Skew: -1.188 Prob(JB): 0.00\r## Kurtosis: 6.386 Cond. No. 584.\r## ==============================================================================\r## ## Notes:\r## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\rLooking at the results, it is possible that there isGender Pay Gap, calculating the difference in estimated salaries done by\n\rplus intercept + e ^ beta_variable = 5077.12\rminus intercept 4105.16\n\requals 971.96.\r\rThere is a 971.96 difference between men and women salaries in the earning population of Brazil in 1970 quite significant at 7.59% of the mean salary at the time.\nLinear Regression plots\rUsing the code from this excellent post and combining it with the understanding from this post.\n# fitted values (need a constant term for intercept)\rmodel_fitted_y = model_fit.fittedvalues\r# model residuals\rmodel_residuals = model_fit.resid\r# normalized residuals\rmodel_norm_residuals = model_fit.get_influence().resid_studentized_internal\r# absolute squared normalized residuals\rmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\r# absolute residuals\rmodel_abs_resid = np.abs(model_residuals)\r# leverage, from statsmodels internals\rmodel_leverage = model_fit.get_influence().hat_matrix_diag\r# cook\u0026#39;s distance, from statsmodels internals\rmodel_cooks = model_fit.get_influence().cooks_distance[0]\rInitializing some variables.\r### Residual plot\nplot_lm_1 = plt.figure(1)\rplot_lm_1.axes[0] = sns.residplot(model_fitted_y, \u0026#39;log_salary\u0026#39;, data=df_filter,\rlowess=True,\rscatter_kws={\u0026#39;alpha\u0026#39;: 0.5},\rline_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;, \u0026#39;lw\u0026#39;: 1, \u0026#39;alpha\u0026#39;: 0.8})\r## C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\r## FutureWarning\rplot_lm_1.axes[0].set_title(\u0026#39;Residuals vs Fitted\u0026#39;)\rplot_lm_1.axes[0].set_xlabel(\u0026#39;Fitted values\u0026#39;)\rplot_lm_1.axes[0].set_ylabel(\u0026#39;Residuals\u0026#39;)\r# annotations\rabs_resid = model_abs_resid.sort_values(ascending=False)\rabs_resid_top_3 = abs_resid[:3]\rfor i in abs_resid_top_3.index:\rplot_lm_1.axes[0].annotate(i, xy=(model_fitted_y[i], model_residuals[i]));\rHere we are looking for the red line to get as close to the doted black line meaning that our Predictors would have a perfectly linear relationship with our Dependent variable following the assumption of linearity.\nI think we are close enough.\n\rQQ plot\rQQ = ProbPlot(model_norm_residuals)\rplot_lm_2 = QQ.qqplot(line=\u0026#39;45\u0026#39;, alpha=0.5, color=\u0026#39;#4C72B0\u0026#39;, lw=1)\rplot_lm_2.axes[0].set_title(\u0026#39;Normal Q-Q\u0026#39;)\rplot_lm_2.axes[0].set_xlabel(\u0026#39;Theoretical Quantiles\u0026#39;)\rplot_lm_2.axes[0].set_ylabel(\u0026#39;Standardized Residuals\u0026#39;);\r# annotations\rabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\rabs_norm_resid_top_3 = abs_norm_resid[:3]\rfor r, i in enumerate(abs_norm_resid_top_3):\rplot_lm_2.axes[0].annotate(i, xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\rmodel_norm_residuals[i]));\rHere we are looking for the circles to get as close to the red line as possible meaning that our variables follow a normal distribution and therefore our p-values are not biased.\nI think we have two problems the extremes may be a bit too distant and there are three concerning outliers.\n\rScale-Location Plot\rplot_lm_3 = plt.figure(3)\rplt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)\rsns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, scatter=False, ci=False, lowess=True,\rline_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;, \u0026#39;lw\u0026#39;: 1, \u0026#39;alpha\u0026#39;: 0.8})\rplot_lm_3.axes[0].set_title(\u0026#39;Scale-Location\u0026#39;)\rplot_lm_3.axes[0].set_xlabel(\u0026#39;Fitted values\u0026#39;)\rplot_lm_3.axes[0].set_ylabel(\u0026#39;$\\sqrt{|Standardized Residuals|}$\u0026#39;);\r# annotations\rabs_sq_norm_resid = np.flip(np.argsort(model_norm_residuals_abs_sqrt), 0)\rabs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\rfor i in abs_norm_resid_top_3:\rplot_lm_3.axes[0].annotate(i, xy=(model_fitted_y[i], model_norm_residuals_abs_sqrt[i]));\rThis is the graph where we check the homoscedasticity assumption, we want the red line to be as straight as possible meaning that our Predictor variance is constant among the Dependent Variable values.\nI think it is fine.\n\rLeverage plot\rplot_lm_4 = plt.figure(4)\rplt.scatter(model_leverage, model_norm_residuals, alpha=0.5)\rsns.regplot(model_leverage, model_norm_residuals, scatter=False, ci=False, lowess=True,\rline_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;, \u0026#39;lw\u0026#39;: 1, \u0026#39;alpha\u0026#39;: 0.8})\rplot_lm_4.axes[0].set_xlim(0, 0.005)\r## (0.0, 0.005)\rplot_lm_4.axes[0].set_ylim(-3, 5)\r## (-3.0, 5.0)\rplot_lm_4.axes[0].set_title(\u0026#39;Residuals vs Leverage\u0026#39;)\rplot_lm_4.axes[0].set_xlabel(\u0026#39;Leverage\u0026#39;)\rplot_lm_4.axes[0].set_ylabel(\u0026#39;Standardized Residuals\u0026#39;)\r# annotations\rleverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\rfor i in leverage_top_3:\rplot_lm_4.axes[0].annotate(i, xy=(model_leverage[i], model_norm_residuals[i]))\r# shenanigans for cook\u0026#39;s distance contours\rdef graph(formula, x_range, label=None):\rx = x_range\ry = formula(x)\rplt.plot(x, y, label=label, lw=1, ls=\u0026#39;--\u0026#39;, color=\u0026#39;red\u0026#39;)\rp = len(model_fit.params) # number of model parameters\rgraph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), np.linspace(0.000, 0.005, 50), \u0026#39;Cook\\\u0026#39;s distance\u0026#39;) # 0.5 line\r## C:/Users/bruno/AppData/Local/r-miniconda/envs/r-reticulate/python.exe:1: RuntimeWarning: divide by zero encountered in true_divide\rgraph(lambda x: np.sqrt((1 * p * (1 - x)) / x), np.linspace(0.000, 0.005, 50)) # 1 line\r## C:/Users/bruno/AppData/Local/r-miniconda/envs/r-reticulate/python.exe:1: RuntimeWarning: divide by zero encountered in true_divide\rplt.legend(loc=\u0026#39;upper right\u0026#39;);\rFinally in this plot we are looking for outliers, it failed on the Python version, but it should show if the outliers plague the betas enough to the point where it may be worth studying removing them.\nWe want the Red line to be as close as possible to the dotted line.\nLooking at the R plot We can say it is fine.\nAt the end I am comfortable not denying our Hypothesis that Salary ~ Sex in 1970 Brazil working population.\nAnd that is it, Statistical analysis with almost no R! .\n\r\r\rFinal Remarks\rI guess my opinion is important in this post, this was really hard, Python may be an excellent Prediction based language but it lacks so much on my normal Economist features that I have easily available even when using Stata/E-Views/SAS, like look at how much code for a simple linear regression plot!\nI don’t have much hope that this will improve with time, normal statistics just doesn’t get as much hype as Deep Learning and stuff I feel sorry for whoever has to learn stats alongside Python, you guys deserve a Medal! Also I applaud the guys that Developed statsmodels.formula.api it really helps!\nWhoever develops with matplotlib deserves two medals, you guys make me feel dumber than when I read my first Time Series paper and that was a really low point in my self esteem, the graphs turned out great in my honest opinion.\nIf you liked it please share it.\nNext post\rIn the next part we repeat everything from part 1 with a few twists in R using the tidyverse!\n\r\r","date":1579910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579910400,"objectID":"885c7846165d139fbc00209f4c6a12bd","permalink":"https://twosidesdata.netlify.com/2020/01/25/exploratory-data-analysis-basics-part2/","publishdate":"2020-01-25T00:00:00Z","relpermalink":"/2020/01/25/exploratory-data-analysis-basics-part2/","section":"post","summary":"Basics exploratory Data Analysis: Part 2 of 4","tags":["R Markdown","reticulate","pandas","scipy","statsmodels"],"title":"exploratory data analysis: basics Python part 2","type":"post"},{"authors":["Bruno Carlin"],"categories":["R and Python"],"content":"\r\r\rLibraries\rThe Exercise\r\rBefore we get into it\r\rObjectives\rReservations\rData Dictionary\r\r\rPython\r\rPre-processing\r\rReading Data\rAnalyzing some basic stuff about our data frame\rReplacing columns names\rCleaning categorical data\rSeeing the effects of categorical Variables\rCleaning numerical data\r\r\rSaving our work for later\rNext post\r\r\rI am currently doing exercises from digital house brasil\nLibraries\rLet’s see what version of python this env is running.\nimport platform\rprint(platform.python_version())\r## 3.6.12\rimport numpy as np\rimport matplotlib.pyplot as plt\rimport seaborn as sns\rimport pandas as pd\rimport os\rfile_path_linux = r.file_path_linux\r\rThe Exercise\rBefore we get into it\rObjectives\rOpen and read a DataFrame using pandas\rSimple stuff right?\n\rBasic analysis of each column using value counts.\rI improved a bit on the base python capabilities\n\rCreating a hypothesis that we care about\rIn our case the hypothesis is simple do women earn on average less than men?\n\rData preprocessing\rWe need to clean the data removing outliers, biases or any other factors that could in theory compromise our hypothesis testing.\n\rVisualize all the variables\rWe were free to apply any technique.\nCategorical Data\n\rTo do in the second post\r\rDefine the variables used in the conclusion\rIn our case, we choose to use salary ~ sex,region region was added to test whether Simpson’s paradox was at play.\n\rUsing masks or other methods to filter the data\rThis objective was mostly done using the groupby function.\n\rVisualizing the hypothesis\rWe were advised to use two histograms combined to get a preview of our answer.\n\rConclusion\rComment on our findings.\n\r\rReservations\rThis is an exercise where we were supposed to ask a relevant question using the data from the IBGE(Brazil’s main data collector) database of 1970.\nOur group decided to ask whether women received less than man, we expanded the analysis hoping to avoid the Simpson’s paradox.\nThis is just an basic inference, and it’s results are therefore only used for studying purposes I don’t believe any finding would be relevant using just this approach but some basic operations can be used in a more impact full work.\n\rData Dictionary\rWe got a Data Dictionary that will be very useful for our Analysis, it contains all the required information about the encoding of the columns and the intended format that the folks at STATA desired.\n\r\rPortuguese\r\r\rDescrição do Registro de Indivíduos nos EUA.\nDataset do software STATA (pago), vamos abri-lo com o pandas e transforma-lo em DataFrame.\nVariável 1 – CHAVE DO INDIVÍDUO ? Formato N - Numérico ? Tamanho 11 dígitos (11 bytes) ? Descrição Sumária Identifica unicamente o indivíduo na amostra.\nVariável 2 - IDADE CALCULADA EM ANOS ? Formato N - Numérico ? Tamanho 3 dígitos (3 bytes) ? Descrição Sumária Identifica a idade do morador em anos completos.\nVariável 3 – SEXO ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 3 ? Descrição Sumária Identifica o sexo do morador. Categorias (1) homem, (2) mulher e (3) gestante.\nVariável 4 – ANOS DE ESTUDO ? Formato N - Numérico ? Tamanho 2 dígitos (2 bytes) ? Quantidade de Categorias 11 ? Descrição Sumária Identifica o número de anos de estudo do morador. Categorias (05) Cinco ou menos, (06) Seis, (07) Sete, (08) Oito, (09) Nove, (10) Dez, (11) Onze, (12) Doze, (13) Treze, (14) Quatorze, (15) Quinze ou mais.\nVariável 5 – COR OU RAÇA ? Formato N - Numérico ? Tamanho 2 dígitos (2 bytes) ? Quantidade de Categorias 6 ? Descrição Sumária Identifica a Cor ou Raça declarada pelo morador. Categorias (01) Branca, (02) Preta, (03) Amarela, (04) Parda, (05) Indígena e (09) Não Sabe.\nVariável 6 – VALOR DO SALÁRIO (ANUALIZADO) ? Formato N - Numérico ? Tamanho 8 dígitos (8 bytes) ? Quantidade de Decimais 2 ? Descrição Sumária Identifica o valor resultante do salário anual do indivíduo. Categorias especiais (-1) indivíduo ausente na data da pesquisa e (999999) indivíduo não quis responder.\nVariável 7 – ESTADO CIVIL ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 2 ? Descrição Sumária Dummy que identifica o estado civil declarado pelo morador. Categorias (1) Casado, (0) não casado.\nVariável 8 – REGIÃO GEOGRÁFICA ? Formato N - Numérico ? Tamanho 1 dígito (1 byte) ? Quantidade de Categorias 5 ? Descrição Sumária Identifica a região geográfica do morador. Categorias (1) Norte, (2) Nordeste, (3) Sudeste, (4) Sul e (5) Centro-oeste.\n\r\r\rEnglish\r\r\rDescription of the US Individual Registry.\nDataset of the STATA software (paid), we will open it with pandas and turn it into DataFrame.\nVariable 1 - KEY OF THE INDIVIDUAL? Format N - Numeric? Size 11 digits (11 bytes)? Summary Description Uniquely identifies the individual in the sample.\nVariable 2 - AGE CALCULATED IN YEARS? Format N - Numeric? Size 3 digits (3 bytes)? Summary Description Identifies the age of the resident in full years.\nVariable 3 - SEX? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 3? Summary Description Identifies the gender of the resident. Categories (1) men, (2) women and (3) pregnant women.\nVariable 4 - YEARS OF STUDY? Format N - Numeric? Size 2 digits (2 bytes)? Number of Categories 11? Summary Description Identifies the number of years of study of the resident. Categories (05) Five or less, (06) Six, (07) Seven, (08) Eight, (09) Nine, (10) Dec, (11) Eleven, (12) Twelve, (13) Thirteen, (14 ) Fourteen, (15) Fifteen or more.\nVariable 5 - COLOR OR RACE? Format N - Numeric? Size 2 digits (2 bytes)? Number of Categories 6? Summary Description Identifies the Color or Race declared by the resident. Categories (01) White, (02) Black, (03) Yellow, (04) Brown, (05) Indigenous and (09) Don’t know.\nVariable 6 - WAGE VALUE (ANNUALIZED)? Format N - Numeric? Size 8 digits (8 bytes)? Number of decimals 2? Summary Description Identifies the amount resulting from the individual’s annual salary. Special categories (-1) individual absent on the survey date and (999999) individual did not want to answer.\nVariable 7 - CIVIL STATE? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 2? Summary Description Dummy that identifies the marital status declared by the resident. Categories (1) Married, (0) Not married.\nVariable 8 - GEOGRAPHICAL REGION? Format N - Numeric? Size 1 digit (1 byte)? Number of Categories 5? Summary Description Identifies the resident’s geographic region. Categories (1) North, (2) Northeast, (3) Southeast, (4) South and (5) Midwest.\n\r\r\r\rPython\rPre-processing\rReading Data\rThe path is specific for my computer but it is easy to adapt\nYou can also dowload it from the github page from this blog\n# Abertura e leitura dos dados em um DeteFrame em Pandas\rpath = r.file_path_linux\rdf = pd.read_csv(path + \u0026#39;/stata_data_1970.csv\u0026#39;)\r\rAnalyzing some basic stuff about our data frame\r#Análise básica dos conteúdos de cada coluna com contagem de valores\rdf.info()\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r## RangeIndex: 66470 entries, 0 to 66469\r## Data columns (total 9 columns):\r## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 Unnamed: 0 66470 non-null int64 ## 1 id 66470 non-null float64\r## 2 idade 66470 non-null int64 ## 3 sexo 66470 non-null object ## 4 anos_estudo 66036 non-null float64\r## 5 cor/raca 66228 non-null object ## 6 salario 47878 non-null float64\r## 7 estado_civil 66470 non-null float64\r## 8 regiao 66470 non-null object ## dtypes: float64(4), int64(2), object(3)\r## memory usage: 4.6+ MB\rI do enjoy python’s base value_counts but when used in a loop it can create some ugly outputs, in order to fix I created a function that adds some flavor text to the print output and generates new information about the accumulated percentage of the data being displayed.\nCustom count_values()\rdef pretty_value_counts(data_frame,\rnumber_of_rows = 5,\rcum_perc = True):\rfor col in data_frame:\rcounts = data_frame[col].value_counts(dropna=False)\rpercentages = data_frame[col].value_counts(dropna=False, normalize=True)\rif cum_perc == True:\rcum_percentages = percentages.cumsum()\rtb = pd.concat([counts,\rpercentages,\rcum_percentages],\raxis=1,\rkeys=[\u0026#39;counts\u0026#39;,\r\u0026#39;percentages\u0026#39;,\r\u0026quot;cum_percentages\u0026quot;]\r).head(number_of_rows)\relse:\rtb = pd.concat([counts,\rpercentages],\raxis=1,\rkeys=[\u0026#39;counts\u0026#39;,\r\u0026#39;percentages\u0026#39;]).head(number_of_rows)\rprint(\u0026quot;Column %s with %s data type\u0026quot; % (col,data_frame[col].dtype),\r\u0026quot;\\n\u0026quot;,\rtb,\r\u0026quot;\\n\u0026quot;)\rNow we can apply our new function.\n\rUsing a custom function\rpretty_value_counts(df)\r## Column Unnamed: 0 with int64 data type ## counts percentages cum_percentages\r## 2047 1 0.000015 0.000015\r## 41601 1 0.000015 0.000030\r## 21151 1 0.000015 0.000045\r## 23198 1 0.000015 0.000060\r## 17053 1 0.000015 0.000075 ## ## Column id with float64 data type ## counts percentages cum_percentages\r## 1.100351e+10 2 0.000030 0.000030\r## 3.132701e+10 1 0.000015 0.000045\r## 1.501501e+10 1 0.000015 0.000060\r## 3.230631e+10 1 0.000015 0.000075\r## 5.003991e+10 1 0.000015 0.000090 ## ## Column idade with int64 data type ## counts percentages cum_percentages\r## 20 2104 0.031653 0.031653\r## 28 2056 0.030931 0.062585\r## 26 2040 0.030691 0.093275\r## 22 2034 0.030600 0.123875\r## 27 2017 0.030345 0.154220 ## ## Column sexo with object data type ## counts percentages cum_percentages\r## mulher 33607 0.505597 0.505597\r## homem 32791 0.493320 0.998917\r## gestante 72 0.001083 1.000000 ## ## Column anos_estudo with float64 data type ## counts percentages cum_percentages\r## 5.0 23349 0.351271 0.351271\r## 11.0 16790 0.252595 0.603866\r## 15.0 5636 0.084790 0.688657\r## 8.0 5017 0.075478 0.764134\r## 10.0 2704 0.040680 0.804814 ## ## Column cor/raca with object data type ## counts percentages cum_percentages\r## Branca 31689 0.476741 0.476741\r## Parda 28370 0.426809 0.903550\r## Preta 5249 0.078968 0.982518\r## Indigena 597 0.008981 0.991500\r## Amarela 323 0.004859 0.996359 ## ## Column salario with float64 data type ## counts percentages cum_percentages\r## NaN 18592 0.279705 0.279705\r## 0.0 1841 0.027697 0.307402\r## -1.0 1101 0.016564 0.323966\r## 999999.0 367 0.005521 0.329487\r## 5229.0 277 0.004167 0.333654 ## ## Column estado_civil with float64 data type ## counts percentages cum_percentages\r## 1.0 39066 0.587724 0.587724\r## 0.0 27404 0.412276 1.000000 ## ## Column regiao with object data type ## counts percentages cum_percentages\r## sudeste 25220 0.379419 0.379419\r## centro-oeste 14702 0.221182 0.600602\r## norte 14653 0.220445 0.821047\r## sul 11890 0.178878 0.999925\r## nordeste 5 0.000075 1.000000\rJust for comparison lets look how we could do the same thing without the function.\n\rfor col in df:\rdf[col].value_counts(dropna=False).head(5)\r## 2047 1\r## 41601 1\r## 21151 1\r## 23198 1\r## 17053 1\r## Name: Unnamed: 0, dtype: int64\r## 1.100351e+10 2\r## 3.132701e+10 1\r## 1.501501e+10 1\r## 3.230631e+10 1\r## 5.003991e+10 1\r## Name: id, dtype: int64\r## 20 2104\r## 28 2056\r## 26 2040\r## 22 2034\r## 27 2017\r## Name: idade, dtype: int64\r## mulher 33607\r## homem 32791\r## gestante 72\r## Name: sexo, dtype: int64\r## 5.0 23349\r## 11.0 16790\r## 15.0 5636\r## 8.0 5017\r## 10.0 2704\r## Name: anos_estudo, dtype: int64\r## Branca 31689\r## Parda 28370\r## Preta 5249\r## Indigena 597\r## Amarela 323\r## Name: cor/raca, dtype: int64\r## NaN 18592\r## 0.0 1841\r## -1.0 1101\r## 999999.0 367\r## 5229.0 277\r## Name: salario, dtype: int64\r## 1.0 39066\r## 0.0 27404\r## Name: estado_civil, dtype: int64\r## sudeste 25220\r## centro-oeste 14702\r## norte 14653\r## sul 11890\r## nordeste 5\r## Name: regiao, dtype: int64\r\r\rReplacing columns names\rThe columns are named in Portuguese we can replace their names for English equivalents in a lot of different ways\ndf.columns\r## Index([\u0026#39;Unnamed: 0\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;idade\u0026#39;, \u0026#39;sexo\u0026#39;, \u0026#39;anos_estudo\u0026#39;, \u0026#39;cor/raca\u0026#39;,\r## \u0026#39;salario\u0026#39;, \u0026#39;estado_civil\u0026#39;, \u0026#39;regiao\u0026#39;],\r## dtype=\u0026#39;object\u0026#39;)\rMy favorite way of doing this sort of trades is using a dictionary defined outside the replace method, the cool thing about replace is that if we liked some of the column names previously defined we can simply omit them, for example, both “Unnamed: 0” and “id” are useless but since their names are already in English I don’t need to mess with them right now\n\r\rTranslation discussion on race\r\r\rThere is some valid discussion on whether to translate “cor/raca” into ethnic_group or color_race, but I am personally on the opinion that the ones making this data frame in 1970 were probably under other standards of naming conventions and racism accusations so I will keep their naming scheme, I apologize if anyone feels offended by the use of these terms\n\rdict_cols = {\u0026quot;idade\u0026quot; : \u0026quot;age\u0026quot;,\r\u0026quot;sexo\u0026quot; : \u0026quot;sex\u0026quot;,\r\u0026quot;anos_estudo\u0026quot; : \u0026quot;years_study\u0026quot;,\r\u0026quot;cor/raca\u0026quot; : \u0026quot;color_race\u0026quot;,\r\u0026quot;salario\u0026quot; : \u0026quot;salary\u0026quot;,\r\u0026quot;estado_civil\u0026quot; : \u0026quot;civil_status\u0026quot;,\r\u0026quot;regiao\u0026quot; : \u0026quot;region\u0026quot;\r}\rdf.rename(columns = dict_cols, inplace = True)\rLet’s see what changed\ndf.columns\r## Index([\u0026#39;Unnamed: 0\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;sex\u0026#39;, \u0026#39;years_study\u0026#39;, \u0026#39;color_race\u0026#39;, \u0026#39;salary\u0026#39;,\r## \u0026#39;civil_status\u0026#39;, \u0026#39;region\u0026#39;],\r## dtype=\u0026#39;object\u0026#39;)\rIt look fine now we can translate some of our main features\n\rCleaning categorical data\rFirst we need to know the categories present in each of our columns a simple loop would fails us when we reached a numeric variable, the simplest way to solve that would be using an if statement, another alternative is using conditional execution, I personally don’t know a simple way of doing that in python but I will show it in the R post\nTo discover the numeric and “categorical” variables, know that sometimes you will have to change some elements of these lists but looking at my outputs I think I got all the relevant ones\nFinding which columns are categorical\rThese are the numerical variables\ndf.select_dtypes(include=[np.number]).columns\r## Index([\u0026#39;Unnamed: 0\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;years_study\u0026#39;, \u0026#39;salary\u0026#39;, \u0026#39;civil_status\u0026#39;], dtype=\u0026#39;object\u0026#39;)\rAnd these are the Categorical variables\nlist_cat = df.select_dtypes(exclude=[np.number]).columns\rNow we can run a simple loop\nfor col in list_cat:\rdf[col].unique()\r## array([\u0026#39;homem\u0026#39;, \u0026#39;mulher\u0026#39;, \u0026#39;gestante\u0026#39;], dtype=object)\r## array([\u0026#39;Parda\u0026#39;, \u0026#39;Amarela\u0026#39;, \u0026#39;Indigena\u0026#39;, \u0026#39;Branca\u0026#39;, \u0026#39;Preta\u0026#39;, nan],\r## dtype=object)\r## array([\u0026#39;norte\u0026#39;, \u0026#39;nordeste\u0026#39;, \u0026#39;sudeste\u0026#39;, \u0026#39;sul\u0026#39;, \u0026#39;centro-oeste\u0026#39;],\r## dtype=object)\rThe simpler method is comparing the dtype in each column to the desired output, but this would be harder if we needed the np.numeric\nfor col in df:\rif df[col].dtype == \u0026quot;O\u0026quot;:\rdf[col].unique()\r## array([\u0026#39;homem\u0026#39;, \u0026#39;mulher\u0026#39;, \u0026#39;gestante\u0026#39;], dtype=object)\r## array([\u0026#39;Parda\u0026#39;, \u0026#39;Amarela\u0026#39;, \u0026#39;Indigena\u0026#39;, \u0026#39;Branca\u0026#39;, \u0026#39;Preta\u0026#39;, nan],\r## dtype=object)\r## array([\u0026#39;norte\u0026#39;, \u0026#39;nordeste\u0026#39;, \u0026#39;sudeste\u0026#39;, \u0026#39;sul\u0026#39;, \u0026#39;centro-oeste\u0026#39;],\r## dtype=object)\rThe problem with the simpler approach is that sometimes you have columns that are categories and not objects so the simpler approach would fail when the more complex one would not, let’s convert sex to a category to prove my point\ndf.sex =df.sex.astype(\u0026quot;category\u0026quot;)\rdf.dtypes\r## Unnamed: 0 int64\r## id float64\r## age int64\r## sex category\r## years_study float64\r## color_race object\r## salary float64\r## civil_status float64\r## region object\r## dtype: object\rfor col in df:\rif df[col].dtype == \u0026quot;O\u0026quot;:\rdf[col].unique()\r## array([\u0026#39;Parda\u0026#39;, \u0026#39;Amarela\u0026#39;, \u0026#39;Indigena\u0026#39;, \u0026#39;Branca\u0026#39;, \u0026#39;Preta\u0026#39;, nan],\r## dtype=object)\r## array([\u0026#39;norte\u0026#39;, \u0026#39;nordeste\u0026#39;, \u0026#39;sudeste\u0026#39;, \u0026#39;sul\u0026#39;, \u0026#39;centro-oeste\u0026#39;],\r## dtype=object)\rIt does not work anymore, of course you can still solve this “problem” with the simpler approach by including a “and” clause on your if statement but at that point you might as well use the more extensible appoach\n\rReplacing values with an dictionary: 1 column\rAfter looking into the categories I can create a dictionary for each column if I want to be safe on repeating terms or I can pass a master dictionary for the whole data frame, I think the column by column approach is tidier but for each their own\ndict_sex = {\u0026quot;mulher\u0026quot; : \u0026quot;woman\u0026quot;,\r\u0026quot;homem\u0026quot; : \u0026quot;man\u0026quot;,\r\u0026quot;gestante\u0026quot; : \u0026quot;woman\u0026quot;} # pregnant\rThis is one strange data frame, it probably made sense to split women into pregnant and not pregnant but I think it will only complicate the otherwise simple analyses so I will group both into “woman”\ndf.sex.replace(dict_sex,inplace = True)\rShowing the new amounts of women/mean\ndf.sex.value_counts()\r## woman 33679\r## man 32791\r## Name: sex, dtype: int64\rdf.sex.unique()\r## array([\u0026#39;man\u0026#39;, \u0026#39;woman\u0026#39;], dtype=object)\rThis fails\npretty_value_counts(df.sex)\r## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: \u0026#39;man\u0026#39;\r## ## Detailed traceback: ## File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt;\r## File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 6, in pretty_value_counts\r## File \u0026quot;C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas\\core\\series.py\u0026quot;, line 882, in __getitem__\r## return self._get_value(key)\r## File \u0026quot;C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas\\core\\series.py\u0026quot;, line 990, in _get_value\r## loc = self.index.get_loc(label)\r## File \u0026quot;C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u0026quot;, line 358, in get_loc\r## raise KeyError(key)\rHere is actually a example on why I don’t personally enjoy Pandas conversion of data, the function that we created pretty_value_counts is not gonna work in this example because Pandas converts a single column to an Series object, so we would have to write a pretty_value_counts for Series as well or we would have to mess with the Pandas method or we could convert the series back into a DataFrame like this\npretty_value_counts(pd.DataFrame(data= df.sex))\r## Column sex with object data type ## counts percentages cum_percentages\r## woman 33679 0.50668 0.50668\r## man 32791 0.49332 1.00000\r\rReplacing values with an dictionary: multiple columns\r\r\rTranslation discussion on race part 2\r\r\rAgain there is relevant discussion on whether I should translate “Parda” as brown but basically Brazil’s population sometimes answers that their skin color is “Parda” = brown when asked about for many reasons I will propose two, “Preta” black can be used as an racist term so some people prefer to be called “brown”, the second explanation is that most of the population is actually pretty well integrated meaning that there a lot of biracial couples in this case we see something like “Preta” parent + “Branca” parent = “Parda” = in English “brown”.\nThere is also the case for the English equivalent of brown skin we simply use “Indiano” = “Indian”.\nCuriously the term “Negra” =~ \"N*gger\" is often preferred in Brazil, that may cause some confusion between Portuguese and English speakers.\nI will use brown but do notice that there were multiple sensible approaches here.\n\rThis is a good opportunity to show failures in the master dictionary approach, realize that if I were to replace “nan” as no_answer or something like that python could thrown me an error because there are “nan” in some numerical columns such as salary but instead I get silence conversion of a numerical columns into object columns a dangerous feature.\nfor col in list_cat:\rdf[col].unique()\r## array([\u0026#39;man\u0026#39;, \u0026#39;woman\u0026#39;], dtype=object)\r## array([\u0026#39;Parda\u0026#39;, \u0026#39;Amarela\u0026#39;, \u0026#39;Indigena\u0026#39;, \u0026#39;Branca\u0026#39;, \u0026#39;Preta\u0026#39;, nan],\r## dtype=object)\r## array([\u0026#39;norte\u0026#39;, \u0026#39;nordeste\u0026#39;, \u0026#39;sudeste\u0026#39;, \u0026#39;sul\u0026#39;, \u0026#39;centro-oeste\u0026#39;],\r## dtype=object)\rdict_all = {\u0026quot;Parda\u0026quot; : \u0026quot;brown\u0026quot;,\r\u0026quot;Amarela\u0026quot; : \u0026quot;yellow\u0026quot;,\r\u0026quot;Indigena\u0026quot; : \u0026quot;indigenous\u0026quot;,\r\u0026quot;Branca\u0026quot; : \u0026quot;white\u0026quot;,\r\u0026quot;Preta\u0026quot; : \u0026quot;black\u0026quot;,\rnp.nan : \u0026quot;no_answer\u0026quot;}\rdf.replace(dict_all).salary.dtype\r## dtype(\u0026#39;O\u0026#39;)\rdict_all = {\u0026quot;Parda\u0026quot; : \u0026quot;brown\u0026quot;, #col color_race\r\u0026quot;Amarela\u0026quot; : \u0026quot;yellow\u0026quot;,\r\u0026quot;Indigena\u0026quot; : \u0026quot;indigenous\u0026quot;,\r\u0026quot;Branca\u0026quot; : \u0026quot;white\u0026quot;,\r\u0026quot;Preta\u0026quot; : \u0026quot;black\u0026quot;,\r\u0026quot;norte\u0026quot; : \u0026quot;north\u0026quot;, # col region \u0026quot;nordeste\u0026quot; : \u0026quot;northeast\u0026quot;,\r\u0026quot;sudeste\u0026quot; : \u0026quot;southeast\u0026quot;,\r\u0026quot;sul\u0026quot; : \u0026quot;south\u0026quot;,\r\u0026quot;centro-oeste\u0026quot; : \u0026quot;midwest\u0026quot;}\rLet’s pray that we don’t have this problem and use this shared dictionary\ndf.replace(dict_all, inplace = True)\r\rDid we correctly clean the Categorical Variables?\rConversion of types\rWell not really I would argue that year_study is an categorical variable as well\rso let’s convert it.\ndf.years_study = df.years_study.astype(\u0026#39;category\u0026#39;)\rdf.years_study.unique()\r## [5.0, 8.0, 11.0, 15.0, 13.0, ..., 9.0, 10.0, 14.0, 12.0, NaN]\r## Length: 12\r## Categories (11, float64): [5.0, 8.0, 11.0, 15.0, ..., 9.0, 10.0, 14.0, 12.0]\rSome nan but otherwise this is could be a useful feature, I will convert it back into a numerical column so that if we can easily impute the NaN’s based on a mathematical method such as the mean of the column.\ndf.years_study = df.years_study.astype(\u0026#39;interger\u0026#39;)\r## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: data type \u0026#39;interger\u0026#39; not understood\r## ## Detailed traceback: ## File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt;\r## File \u0026quot;C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas\\core\\generic.py\u0026quot;, line 5548, in astype\r## new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors,)\r## File \u0026quot;C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u0026quot;, line 604, in astype\r## return self.apply(\u0026quot;astype\u0026quot;, dtype=dtype, copy=copy, errors=errors)\r## File \u0026quot;C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u0026quot;, line 409, in apply\r## applied = getattr(b, f)(**kwargs)\r## File \u0026quot;C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u0026quot;, line 548, in astype\r## dtype = pandas_dtype(dtype)\r## File \u0026quot;C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u0026quot;, line 1763, in pandas_dtype\r## npdtype = np.dtype(dtype)\rAnother numpy quirk you can’t use integers because there are NaN values.\ndf.years_study = df.years_study.astype(\u0026#39;float\u0026#39;)\rConverting civil_status into a category.\ndf.civil_status.unique()\r## array([1., 0.])\rTo know what 1 or 0 mean, so we need to check the dictionary\ndict_civil_status = { 0. : \u0026quot;not_married\u0026quot;,\r1. : \u0026quot;married\u0026quot;}\rdf.civil_status = df.civil_status.replace(dict_civil_status)\rdf.civil_status.head()\r## 0 married\r## 1 married\r## 2 not_married\r## 3 married\r## 4 married\r## Name: civil_status, dtype: object\rBefore we deal with numerical variables I will get rid of ‘Unnamed: 0’ and ‘id’ features because they are useless in this case.\ndf.drop(columns=[\u0026#39;Unnamed: 0\u0026#39;, \u0026#39;id\u0026#39;],inplace=True)\r\r\r\rSeeing the effects of categorical Variables\rWe can use a colored barplot to see the interaction of these Categorical Variables with our Hypothesis.\nsns_plot = sns.catplot(x=\u0026quot;sex\u0026quot;, y=\u0026quot;salary\u0026quot;, hue=\u0026quot;region\u0026quot;, kind=\u0026quot;bar\u0026quot;, data=df)\rpy$sns_plot\r## \u0026lt;seaborn.axisgrid.FacetGrid\u0026gt;\rsns_plot = sns.catplot(x=\u0026quot;sex\u0026quot;, y=\u0026quot;salary\u0026quot;, hue=\u0026quot;civil_status\u0026quot;, kind=\u0026quot;bar\u0026quot;, data=df)\rplt.show(sns_plot)\rsns_plot = sns.catplot(x=\u0026quot;sex\u0026quot;, y=\u0026quot;salary\u0026quot;, hue=\u0026quot;color_race\u0026quot;, kind=\u0026quot;bar\u0026quot;, data=df)\rplt.show(sns_plot)\r\rCleaning numerical data\rIf we pull back the code that we used here are the numerical features of this dataset\ndf.select_dtypes(include=[np.number]).columns\r## Index([\u0026#39;age\u0026#39;, \u0026#39;years_study\u0026#39;, \u0026#39;salary\u0026#39;], dtype=\u0026#39;object\u0026#39;)\rIt is very common to reuse these kind of codes in Data Science scripts, so you shouldn’t fell as bad about repeating yourself as you do in other endeavors such in normal software engendering and you call always clean your analysis latter.\nIn order to know what to “clean” in numerical data I like to use plot such as a histogram\ndf.salary.hist(bins = 10)\rplt.show()\rHere we can see that the data may have a few outliers at 1000000 and that most of the salary data has a large Positive skew meaning that most data point are left to the mean of the dataset we can see that better using an density plot instead\nplot_density = df.salary.plot.kde()\rplot_density.set_xlim(0,100000)\r## (0.0, 100000.0)\rplot_density\r#### Replacing variables {#python_custom_function_2}\nIf we go back to our custom function we can find that the values -1 and 999999 are unusually common after consulting the dictionary we decided to replace these values with the mean of the group.\nThis operation would be wrong for machine learning purposes since the mean of our train group would leak information from the test set as well but here in exploratory data analysis it is mostly fine also you need to replace the values with the numpy nan or else this operation doesn’t work as expected.\ndf_copy = df.copy()\rdf_copy.salary.replace({-1: \u0026quot;NaN\u0026quot;,999999:\u0026#39;NaN\u0026#39;},inplace = True)\rdf_copy.salary.fillna(df.salary.mean(),inplace= True)\rpretty_value_counts(pd.DataFrame(df_copy.salary))\r## Column salary with object data type ## counts percentages cum_percentages\r## 19706.790323432902 18592 0.279705 0.279705\r## 0.0 1841 0.027697 0.307402\r## NaN 1468 0.022085 0.329487\r## 5229.0 277 0.004167 0.333654\r## 7200.0 260 0.003912 0.337566\r# Create the new na values\rdf.salary.replace({-1:np.nan,999999:np.nan},inplace = True)\rdf.salary.fillna(df.salary.mean(),inplace= True)\rpretty_value_counts(pd.DataFrame(df.salary))\r## Column salary with float64 data type ## counts percentages cum_percentages\r## 12422.39119 20060 0.301790 0.301790\r## 0.00000 1841 0.027697 0.329487\r## 5229.00000 277 0.004167 0.333654\r## 7200.00000 260 0.003912 0.337566\r## 7560.00000 244 0.003671 0.341237\rAnd that is the magic of mutable Data Structures no extra assignments are required, quite useful, but be careful there is no going back if you haven’t saved a copy of your data.\nLog of numerical data\rThere is also a statisticall solution for the Positive skew in our Data we can take the log of the salary column, but we will have to add one to all values since log of 0 goes to -Inf\ndf.log_salary = np.log1p(df.salary)\r## C:/Users/bruno/AppData/Local/r-miniconda/envs/r-reticulate/python.exe:1: UserWarning: Pandas doesn\u0026#39;t allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\rpretty_value_counts(pd.DataFrame(df.log_salary))\r## Column salary with float64 data type ## counts percentages cum_percentages\r## 9.427336 20060 0.301790 0.301790\r## 0.000000 1841 0.027697 0.329487\r## 8.562167 277 0.004167 0.333654\r## 8.881975 260 0.003912 0.337566\r## 8.930759 244 0.003671 0.341237\rBut then it is gonne\ndf.info()\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r## RangeIndex: 66470 entries, 0 to 66469\r## Data columns (total 7 columns):\r## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 age 66470 non-null int64 ## 1 sex 66470 non-null object ## 2 years_study 66036 non-null float64\r## 3 color_race 66228 non-null object ## 4 salary 66470 non-null float64\r## 5 civil_status 66470 non-null object ## 6 region 66470 non-null object ## dtypes: float64(2), int64(1), object(4)\r## memory usage: 3.6+ MB\rYou are better off using the [ notation\ndf[\u0026#39;log_salary\u0026#39;] = np.log1p(df.salary)\rplot_density = df.log_salary.plot.kde(bw_method= 0.5)\rplot_density.set_xlim(0,15)\r## (0.0, 15.0)\rplot_density\rIt is now a usefull feature for most simple linear models\n\rOther numerical columns\rdf.age.hist(bins = 20)\rplt.show()\rplot_density = df.age.plot.kde()\rplot_density\rpretty_value_counts(pd.DataFrame(df.age))\r## Column age with int64 data type ## counts percentages cum_percentages\r## 20 2104 0.031653 0.031653\r## 28 2056 0.030931 0.062585\r## 26 2040 0.030691 0.093275\r## 22 2034 0.030600 0.123875\r## 27 2017 0.030345 0.154220\rAge seems fine\nRemember from the the categorical variables we passed years_study here so that we could impute its missing values\ndf.info()\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r## RangeIndex: 66470 entries, 0 to 66469\r## Data columns (total 8 columns):\r## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 age 66470 non-null int64 ## 1 sex 66470 non-null object ## 2 years_study 66036 non-null float64\r## 3 color_race 66228 non-null object ## 4 salary 66470 non-null float64\r## 5 civil_status 66470 non-null object ## 6 region 66470 non-null object ## 7 log_salary 66470 non-null float64\r## dtypes: float64(3), int64(1), object(4)\r## memory usage: 4.1+ MB\rWe are missing 66470 - 66036 = 434 observation, this is a small enough number that we decided to drop these rows\nWhile we are droping missing values lets drop the color_race missing observations as well\ndf.dropna(subset = [\u0026quot;years_study\u0026quot;,\u0026quot;color_race\u0026quot;],inplace= True)\rdf.info()\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r## Int64Index: 65795 entries, 0 to 66469\r## Data columns (total 8 columns):\r## # Column Non-Null Count Dtype ## --- ------ -------------- ----- ## 0 age 65795 non-null int64 ## 1 sex 65795 non-null object ## 2 years_study 65795 non-null float64\r## 3 color_race 65795 non-null object ## 4 salary 65795 non-null float64\r## 5 civil_status 65795 non-null object ## 6 region 65795 non-null object ## 7 log_salary 65795 non-null float64\r## dtypes: float64(3), int64(1), object(4)\r## memory usage: 4.5+ MB\rChecking on year_study\ndf.years_study.hist(bins = 20)\rplt.show()\rLet’s convert it back into a Category\ndf.years_study = df.years_study.astype(\u0026#39;category\u0026#39;)\r\r\r\r\rSaving our work for later\rHere we have many options we can for example run this script later or save this modified df as a csv, both options are okay but I will promote the usage of an Data format that keeps the mindful choices of encoding that we made into consideration, there are many alternatives in this case as well but I will use feather.\nIt is also always a good idea to separate the Data from the script if you want reproducible work, that is where Excel mostly fails for me.\nSo showing our Data Types\ndf.dtypes\r## age int64\r## sex object\r## years_study category\r## color_race object\r## salary float64\r## civil_status object\r## region object\r## log_salary float64\r## dtype: object\rUsing csv will may lose some Data Types\ndf.to_csv(file_path_linux + \u0026#39;/finished_work.csv\u0026#39;)\rpd.read_csv(file_path_linux + \u0026#39;/finished_work.csv\u0026#39;).dtypes\r## Unnamed: 0 int64\r## age int64\r## sex object\r## years_study float64\r## color_race object\r## salary float64\r## civil_status object\r## region object\r## log_salary float64\r## dtype: object\rWe lost our encoding of years_study and when writing a csv we made this useless to us Unnamed: 0 column\na better way is using the feather file format, you need to pip install pyarrow beforehand\ndf.reset_index().to_feather(file_path_linux + \u0026#39;/sex_thesis_assignment.feather\u0026#39;)\rpd.read_feather(file_path_linux + \u0026#39;/sex_thesis_assignment.feather\u0026#39;).dtypes\r## index int64\r## age int64\r## sex object\r## years_study category\r## color_race object\r## salary float64\r## civil_status object\r## region object\r## log_salary float64\r## dtype: object\rFeather does keep the years study dtype, but feather is still in a experimental phase so be carefull with it, parquet unfortunally fails to keep the dtypes I don’t know why.\nIt is also a good idea to keep good file names so that you can easily identify your datasets and scripts.\nIf you then need to delete these files you can do it inside python\nos.remove(file_path_linux +\u0026#39;/finished_work.csv\u0026#39;)\r#os.remove(file_path_linux + \u0026#39;/sex_thesis_assignment.feather\u0026#39;)\r\rNext post\rIn the next post I will show the end of the analysis and the “answer” to our hypothesis.\n\r","date":1579392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579392000,"objectID":"c2f49cfb9158030459eb0971bd055b44","permalink":"https://twosidesdata.netlify.com/2020/01/19/exploratory-data-analysis-basics-part1/","publishdate":"2020-01-19T00:00:00Z","relpermalink":"/2020/01/19/exploratory-data-analysis-basics-part1/","section":"post","summary":"Basics exploratory Data Analysis: Part 1 of 4","tags":["R Markdown","reticulate","pandas"],"title":"exploratory data analysis: basics Python part 1","type":"post"},{"authors":["Bruno Carlin"],"categories":["r-project"],"content":"\r\rTurning strings into numbers\rReframing the problem\rCalculating occupied and available minutes\rFiltering lists\rCalculating start and end minutes\rConverting minutes back into readable hours\rPairing Start and End Hours\rBig O problem and Data Science\r\r\rSo to start it all, I learned about this question from the recommended youtube channel Clément Mihailescu in his youtube video \nIn this video both Clément and Tim from Tech with Tim work together to solve this question:\nSuppose you have two People that want to Schedule a meeting, how would you schedule it, if each Person has already set up meetings in this day and that each Person has different working hours\nTo help us understand it the problem provided us with this example data\nPerson 1 has three meetings 9:00 to 10:30, 12:00 to 13:00 and 16:00 to 18:00, this Person works from 9:00 to 20:00\nPerson 2 also has three meetings 10:00 to 11:30, 12:30 to 14:30 and 18:00 to 18:30 this Person works from 10:00 to 18:30\nFor those just starting out with R and the tidyverse I will explain each part in detail in the collapsible parts of the post.\nSo coding this info in R we have\nlibrary(tidyverse)\rperson1 \u0026lt;- list(list(\u0026#39;9:00\u0026#39;, \u0026#39;10:30\u0026#39;),\rlist(\u0026#39;12:00\u0026#39;, \u0026#39;13:00\u0026#39;),\rlist(\u0026#39;16:00\u0026#39;, \u0026#39;18:00\u0026#39;))\rallowed_time1 \u0026lt;- list(list(\u0026#39;9:00\u0026#39;,\u0026#39;20:00\u0026#39;))\rperson2 \u0026lt;- list(list(\u0026#39;10:00\u0026#39;, \u0026#39;11:30\u0026#39;),\rlist(\u0026#39;12:30\u0026#39;, \u0026#39;14:30\u0026#39;),\rlist(\u0026#39;18:00\u0026#39;, \u0026#39;18:30\u0026#39;))\rallowed_time2 \u0026lt;- list(list(\u0026#39;10:00\u0026#39;,\u0026#39;18:30\u0026#39;))\rIf you see the video Tim was able to solve it in 45 minutes while having spent a good amount of the time talking back and forth with his interviewer and explaining his reasoning, that was impressive.\nWell I failed to finish this problem in 45 minutes, in fact it took close to 3 hours to stitch together the solution I am about to show, but I think my solution is something that I am proud of and that it follows most of what I love about functional programming.\nTurning strings into numbers\rI saw that Tim created a function to compare time in his program\nCreating a function that calculates the amount of minutes in a string such as ‘10:30’\ncalculate_minutes \u0026lt;- function(x) {\rx \u0026lt;- str_split(x,pattern = \u0026quot;:\u0026quot;)\ras.integer(x[[1]][[1]]) * 60 + as.integer(x[[1]][[2]])\r}\rcalculate_minutes(\u0026#39;10:30\u0026#39;)\r## [1] 630\r\rexplanation on calculate minutes\rcalculate_minutes splits the string ‘10:30’ into ‘10’ and ‘30’ and then multiples the left hand side by 60, because each hour has 60 minutes in it and then adds the right hand side the minutes to the result 630 = 10 * 60 + 30 minutes.\n\r\rReframing the problem\rWhile at first the calculate_minutes may seem useless in both r and python since ‘14:30’ \u0026lt; “10:30” will return FALSE/False\nWe can reframe the focus on minutes overlapping of each appointment instead of time comparison like Tim used is his solution.\nSo based on this new idea, I created a function to convert the the strings we received into ranges of minutes, basically the interval of each appointment.\nWe will also need the interval of the full Day\nfull_day \u0026lt;- 1:(24*60) # 24 hours * 60 minutes -\u0026gt; range 1: to result\rHere is the function that applies Step One and Two\ncalculate_interval \u0026lt;- . %\u0026gt;% map(calculate_minutes) %\u0026gt;% reduce(seq)\rexample_appointment \u0026lt;- list(\u0026#39;10:30\u0026#39;,\u0026#39;10:40\u0026#39;)\rexample_appointment %\u0026gt;% calculate_interval\r## [1] 630 631 632 633 634 635 636 637 638 639 640\r\rexplanation on calculate_interval\rI use “.” as a shortcut for function(x) {} it is really useful in function pipes like this one\nmap is a function that applies another function to all elements of an list and returns ideally the same number of elements like this,\nexample_list \u0026lt;- list(list(\u0026#39;hi\u0026#39;,\u0026#39;johnny\u0026#39;),list(\u0026#39;how\u0026#39;,\u0026#39;are\u0026#39;,\u0026#39;you\u0026#39;))\rmap(example_list,.f = str_to_upper)\r## [[1]]\r## [1] \u0026quot;HI\u0026quot; \u0026quot;JOHNNY\u0026quot;\r## ## [[2]]\r## [1] \u0026quot;HOW\u0026quot; \u0026quot;ARE\u0026quot; \u0026quot;YOU\u0026quot;\rI also use pipes (%\u0026gt;%), the pipes allow us to change the nested nature of function calls into a sequential one\rfor example\nexample_list %\u0026gt;%\rmap(str_to_title)\r## [[1]]\r## [1] \u0026quot;Hi\u0026quot; \u0026quot;Johnny\u0026quot;\r## ## [[2]]\r## [1] \u0026quot;How\u0026quot; \u0026quot;Are\u0026quot; \u0026quot;You\u0026quot;\rAnd finally I use another core function of functional programming reduce, reduce works by applying the same function in a list until the is only one element left for example\nreduce(1:4,sum)\r## [1] 10\r\rWe then need to use this function on all of our info\nperson1_interval \u0026lt;- person1 %\u0026gt;%\rmap(calculate_interval)\rperson2_interval \u0026lt;- person2 %\u0026gt;% map(calculate_interval)\rallowed_time1_interval \u0026lt;- allowed_time1 %\u0026gt;% map(calculate_interval)\rallowed_time2_interval \u0026lt;- allowed_time2 %\u0026gt;% map(calculate_interval)\r\rexample Person1\nperson1 %\u0026gt;% map(calculate_interval)\r## [[1]]\r## [1] 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\r## [20] 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577\r## [39] 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596\r## [58] 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615\r## [77] 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630\r## ## [[2]]\r## [1] 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738\r## [20] 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757\r## [39] 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776\r## [58] 777 778 779 780\r## ## [[3]]\r## [1] 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974\r## [16] 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989\r## [31] 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004\r## [46] 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019\r## [61] 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034\r## [76] 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049\r## [91] 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064\r## [106] 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079\r## [121] 1080\r\r\rCalculating occupied and available minutes\rThis is a simple step we collapse the lists into one\nschedule_1_occupied \u0026lt;- person1_interval \u0026lt;- person1_interval %\u0026gt;% reduce(c)\rschedule_1_avalaible \u0026lt;- allowed_time1_interval %\u0026gt;% reduce(c)\rschedule_2_occupied \u0026lt;- person2_interval %\u0026gt;% reduce(c)\rschedule_2_avalaible \u0026lt;- allowed_time2_interval %\u0026gt;% reduce(c)\r\rexample reduce c\rHere is an example of how to collapse a simple list\nlist(c(1,3,4),c(2,5)) %\u0026gt;% reduce(c)\r## [1] 1 3 4 2 5\r\r\rFiltering lists\rNow finally to our last core functional language function (filter), in the tidyverse filter was split into two functions discard and keep, mostly because the actual filter function is used in dplyr\nWe can start with the full_day and take away occupied minutes and keep valid minutes, all using filter.\npossible_minutes \u0026lt;- full_day %\u0026gt;% discard(~ .x %in% c(schedule_1_occupied,schedule_2_occupied)) %\u0026gt;%\rkeep(~ .x %in% schedule_1_avalaible) %\u0026gt;% keep(~ .x %in% schedule_2_avalaible)\rpossible_minutes\r## [1] 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708\r## [19] 709 710 711 712 713 714 715 716 717 718 719 871 872 873 874 875 876 877\r## [37] 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895\r## [55] 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913\r## [73] 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931\r## [91] 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949\r## [109] 950 951 952 953 954 955 956 957 958 959\r\rexplanation filter\rfilter works by receiving an vector or list and return only elements that have passed or failed a test for example\ndiscard(1:10, ~ . \u0026gt; 5)\r## [1] 1 2 3 4 5\rkeep(1:10, ~ . \u0026gt; 5)\r## [1] 6 7 8 9 10\r\rKeep in mind that while discarding we can evaluate everything together and the result stays the same, but when keeping it is important in this case to separate the call into two\n\rCalculating start and end minutes\rThis function solves the problem that we as human prefer to receive just the start and end minutes instead of the whole duration\ncalculate_break_time \u0026lt;- function(intergers){\rintergers \u0026lt;- sort(intergers)\rend \u0026lt;- time \u0026lt;- which(diff(intergers) != 1)\rbegin \u0026lt;- end + 1\rintergers[c(1,end,begin,length(intergers)) %\u0026gt;% sort()]\r}\r\rexplanation calculate_break_time\rThis is the function that I am least happy, but basically the two minutes that we know we will need are the smallest minute that will start the first appointment and the last minute which will end the last appointment\rThe in between minutes are found by looking for jumps between minutes using the diff function\ndiff(c(1,3,4,5,9))\r## [1] 2 1 1 4\rIf there is more than one minute of difference it is the end of an appointment and one minute later there will be the start of a new appointment\n\rstart_end_minutes \u0026lt;- possible_minutes %\u0026gt;% calculate_break_time()\rstart_end_minutes\r## [1] 691 719 871 959\r\rConverting minutes back into readable hours\rA simple function that undoes our transformation if you need to read %/% explanation\nturn_back_into_time \u0026lt;- function(x) {\rx_hour \u0026lt;- x %/% 60\rx_minute \u0026lt;- x %% 60\rstr_c(x_hour,x_minute,sep = \u0026#39;:\u0026#39;)\r}\r(readable_time \u0026lt;- start_end_minutes %\u0026gt;% turn_back_into_time)\r## [1] \u0026quot;11:31\u0026quot; \u0026quot;11:59\u0026quot; \u0026quot;14:31\u0026quot; \u0026quot;15:59\u0026quot;\r\rPairing Start and End Hours\rI will over complicate the flagging of even and odd numbers to show another really cool functional concept of currying, while currying is not encouraged by the tidyverse the partial functions does make it pretty easy to use (same as Python)\nThe other cool concept is that you can negate a function, in this case turning the results of is_even into is_odd\ndiviseble_by \u0026lt;- function(number_vector,divisor,quocient) {\rnumber_vector %% divisor == quocient\r}\ris_even \u0026lt;- partial(diviseble_by,divisor = 2,quocient = 0)\ris_odd \u0026lt;- is_even %\u0026gt;% negate()\r1:10 %\u0026gt;% is_even\r## [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE\r1:10 %\u0026gt;% is_odd\r## [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE\rWe then use these functions to unite our start and end minutes\npair_wise_combination \u0026lt;- function(character_vector){\rvector_i \u0026lt;- seq_along(character_vector)\rends \u0026lt;- vector_i %\u0026gt;% is_even\rbegins \u0026lt;- vector_i %\u0026gt;% is_odd\rstr_c(character_vector[begins],character_vector[ends],sep = \u0026quot; \u0026quot;)\r}\rreadable_time %\u0026gt;% pair_wise_combination\r## [1] \u0026quot;11:31 11:59\u0026quot; \u0026quot;14:31 15:59\u0026quot;\rThat is all folks.\nMy answer also works for n people\nI can answer questions anywhere, please do share it if you have enjoyed it.\n\rBig O problem and Data Science\rI also failed because, I would have blankly stared into the interviewer face for a while before admitting that I have no idea the Big O of this answer is\nWhile I do understand big O notation and its importance, I am not a Software Engineer nor a Computer Scientist, I have no idea how efficient my solution is, I know it is fast enough for me, but I understand that big O knowledge is a major difference while learning DS compared to the usual programming paths, rarely if ever people mention the Big O of our algorithms so I never deeply studied about the subject.\n\r","date":1578787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578787200,"objectID":"40e2476ab8438866e2c5a95e21cdde17","permalink":"https://twosidesdata.netlify.com/2020/01/12/google-interview-question-in-r/","publishdate":"2020-01-12T00:00:00Z","relpermalink":"/2020/01/12/google-interview-question-in-r/","section":"post","summary":"Google questions are hard","tags":["tidyverse","programming"],"title":"Google interview question in R - Calendar","type":"post"},{"authors":null,"categories":["r-project"],"content":"\rFizzBuzz is and old kids games\nNot that popular where I am from Brazil, Fizz Buzz has a simple set of rules\nYou start counting from 1 (obviously) and when a number is a multiple of 3 you say Fizz,\nif the number is a multiple of 5 you say Buzz,\nand if the number is a multiple of both you shout FizzBuzz,\rAnd for every other case you can say the number itself, simple right?\nI watched this really cool video on Tom Scott channel and realized that I have never attempted this problem as a programmer\nThis is an blog post full of tricks I will try to point them all out.\nScott’s video\nNaive FizzBuzz\rNaive FizzBuzz\nfor (i in 1:15){\rif(i%%3 == 0 \u0026amp; i%%5 == 0) {\rprint(\u0026#39;FizzBuzz\u0026#39;)\r}\relse if(i%%3 == 0) {\rprint(\u0026#39;Fizz\u0026#39;)\r}\relse if (i%%5 == 0){\rprint(\u0026#39;Buzz\u0026#39;)\r}\relse {\rprint(i)\r}\r}\r## [1] 1\r## [1] 2\r## [1] \u0026quot;Fizz\u0026quot;\r## [1] 4\r## [1] \u0026quot;Buzz\u0026quot;\r## [1] \u0026quot;Fizz\u0026quot;\r## [1] 7\r## [1] 8\r## [1] \u0026quot;Fizz\u0026quot;\r## [1] \u0026quot;Buzz\u0026quot;\r## [1] 11\r## [1] \u0026quot;Fizz\u0026quot;\r## [1] 13\r## [1] 14\r## [1] \u0026quot;FizzBuzz\u0026quot;\rSimple flow control with if, else statements\rSome basic operators ($, ==)\r\r\rExtending the loop approach\rUsing Scott’s approach we can improve a bit on the logic\nfor (i in 1:15){\rcurrent_out \u0026lt;- \u0026#39;\u0026#39;\rif(i%%3 == 0) {\rcurrent_out \u0026lt;- paste0(current_out,\u0026#39;Fizz\u0026#39;)\r}\rif (i%%5 == 0){\rcurrent_out \u0026lt;- paste0(current_out,\u0026#39;Buzz\u0026#39;)\r}\rif (current_out == \u0026#39;\u0026#39;){\rprint(i)\r}\relse print(current_out)\r}\r## [1] 1\r## [1] 2\r## [1] \u0026quot;Fizz\u0026quot;\r## [1] 4\r## [1] \u0026quot;Buzz\u0026quot;\r## [1] \u0026quot;Fizz\u0026quot;\r## [1] 7\r## [1] 8\r## [1] \u0026quot;Fizz\u0026quot;\r## [1] \u0026quot;Buzz\u0026quot;\r## [1] 11\r## [1] \u0026quot;Fizz\u0026quot;\r## [1] 13\r## [1] 14\r## [1] \u0026quot;FizzBuzz\u0026quot;\rWhile it is possible to improve open this loop, I think it already is close to the limits of what I would call a very simple example\n\rFunctional approach\rThanks Functional FizzBuzz\ndivisor \u0026lt;-\rfunction(number, string) {\rfunction(d) {\rif (d %% number == 0) string else \u0026quot;\u0026quot;\r}\r}\rmod3er \u0026lt;- divisor(3, \u0026quot;Fizz\u0026quot;)\rmod5er \u0026lt;- divisor(5, \u0026quot;Buzz\u0026quot;)\rfizzbuzz \u0026lt;- function(i) {\rres \u0026lt;- paste0(mod3er(i), mod5er(i))\rifelse(res == \u0026quot;\u0026quot;, i, res)\r}\rsapply(1:15, fizzbuzz)\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;Fizz\u0026quot; \u0026quot;4\u0026quot; \u0026quot;Buzz\u0026quot; \u0026quot;Fizz\u0026quot; ## [7] \u0026quot;7\u0026quot; \u0026quot;8\u0026quot; \u0026quot;Fizz\u0026quot; \u0026quot;Buzz\u0026quot; \u0026quot;11\u0026quot; \u0026quot;Fizz\u0026quot; ## [13] \u0026quot;13\u0026quot; \u0026quot;14\u0026quot; \u0026quot;FizzBuzz\u0026quot;\rSo enumerating the new concepts here:\nFunctions that create functions (mod3er,mod5er)\rFunctions that create functions that create functions (divisor)\rApplying functions (sapply)\rFunctional if else (I prefer it)\r\rAll of which seen pretty complicated at first but will pay off big time latter.\n\rMy approach (tidyverse)\rThe Basics\rLoading the tidyverse\nlibrary(tidyverse)\r## -- Attaching packages ---------------------------------------------------------------------------------------------------------------------------------------------- tidyverse 1.3.0 --\r## \u0026lt;U+2713\u0026gt; ggplot2 3.2.1 \u0026lt;U+2713\u0026gt; purrr 0.3.3\r## \u0026lt;U+2713\u0026gt; tibble 2.1.3 \u0026lt;U+2713\u0026gt; dplyr 0.8.3\r## \u0026lt;U+2713\u0026gt; tidyr 1.0.0 \u0026lt;U+2713\u0026gt; stringr 1.4.0\r## \u0026lt;U+2713\u0026gt; readr 1.3.1 \u0026lt;U+2713\u0026gt; forcats 0.4.0\r## -- Conflicts ------------------------------------------------------------------------------------------------------------------------------------------------- tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rdivisor \u0026lt;- function(number, string) {\rfunction(input) {\rif_else(condition = input %% number == 0,\rtrue = string,\rfalse = \u0026quot;\u0026quot;)\r}\r}\rmod3 \u0026lt;- divisor(3, \u0026quot;Fizz\u0026quot;)\rmod5 \u0026lt;- divisor(5, \u0026quot;Buzz\u0026quot;)\rlist_functions \u0026lt;- list(mod3,mod5)\rmapper_list \u0026lt;- function(i,list_functions) map(list_functions, exec,i)\rmap(1:15,mapper_list,list_functions) %\u0026gt;% map(reduce,str_c)\r## [[1]]\r## [1] \u0026quot;\u0026quot;\r## ## [[2]]\r## [1] \u0026quot;\u0026quot;\r## ## [[3]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[4]]\r## [1] \u0026quot;\u0026quot;\r## ## [[5]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[6]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[7]]\r## [1] \u0026quot;\u0026quot;\r## ## [[8]]\r## [1] \u0026quot;\u0026quot;\r## ## [[9]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[10]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[11]]\r## [1] \u0026quot;\u0026quot;\r## ## [[12]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[13]]\r## [1] \u0026quot;\u0026quot;\r## ## [[14]]\r## [1] \u0026quot;\u0026quot;\r## ## [[15]]\r## [1] \u0026quot;FizzBuzz\u0026quot;\rWe learned two new tricks:\nExecuting a list of functions using exec\rreducing an list\r\r\rMaking just one call\rfancy \u0026lt;- function(i,...) {\rlist_functions \u0026lt;- list(...)\rmapper_list \u0026lt;- function(i,list_functions) map(list_functions, exec,i)\rmap(i,mapper_list,list_functions) %\u0026gt;%\rmap(reduce,str_c)\r}\rfancy(1:15,mod3,mod5)\r## [[1]]\r## [1] \u0026quot;\u0026quot;\r## ## [[2]]\r## [1] \u0026quot;\u0026quot;\r## ## [[3]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[4]]\r## [1] \u0026quot;\u0026quot;\r## ## [[5]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[6]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[7]]\r## [1] \u0026quot;\u0026quot;\r## ## [[8]]\r## [1] \u0026quot;\u0026quot;\r## ## [[9]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[10]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[11]]\r## [1] \u0026quot;\u0026quot;\r## ## [[12]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[13]]\r## [1] \u0026quot;\u0026quot;\r## ## [[14]]\r## [1] \u0026quot;\u0026quot;\r## ## [[15]]\r## [1] \u0026quot;FizzBuzz\u0026quot;\rOne new trick using ellipsis\n\rOr preparing for an api\rapi_less_fancy \u0026lt;- function(i,list_functions) {\rmapper_list \u0026lt;- function(i,list_functions) map(list_functions, exec,i)\rmap(i,mapper_list,list_functions) %\u0026gt;%\rmap(reduce,str_c)\r}\rapi_less_fancy(1:15,list(mod3,mod5))\r## [[1]]\r## [1] \u0026quot;\u0026quot;\r## ## [[2]]\r## [1] \u0026quot;\u0026quot;\r## ## [[3]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[4]]\r## [1] \u0026quot;\u0026quot;\r## ## [[5]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[6]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[7]]\r## [1] \u0026quot;\u0026quot;\r## ## [[8]]\r## [1] \u0026quot;\u0026quot;\r## ## [[9]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[10]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[11]]\r## [1] \u0026quot;\u0026quot;\r## ## [[12]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[13]]\r## [1] \u0026quot;\u0026quot;\r## ## [[14]]\r## [1] \u0026quot;\u0026quot;\r## ## [[15]]\r## [1] \u0026quot;FizzBuzz\u0026quot;\r\r\rExtending FizzBuzz\rLet’s see how easy it is too make the game more difficult:\nChanging names\rmod3n \u0026lt;- divisor(3, \u0026quot;Buzz\u0026quot;)\rmod5n \u0026lt;- divisor(5,\u0026#39;Fizz\u0026#39;)\rfancy(1:15,mod3n,mod5n)\r## [[1]]\r## [1] \u0026quot;\u0026quot;\r## ## [[2]]\r## [1] \u0026quot;\u0026quot;\r## ## [[3]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[4]]\r## [1] \u0026quot;\u0026quot;\r## ## [[5]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[6]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[7]]\r## [1] \u0026quot;\u0026quot;\r## ## [[8]]\r## [1] \u0026quot;\u0026quot;\r## ## [[9]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[10]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[11]]\r## [1] \u0026quot;\u0026quot;\r## ## [[12]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[13]]\r## [1] \u0026quot;\u0026quot;\r## ## [[14]]\r## [1] \u0026quot;\u0026quot;\r## ## [[15]]\r## [1] \u0026quot;BuzzFizz\u0026quot;\r\rAdding divisors\rmod2 \u0026lt;- divisor(2, \u0026quot;Deuce\u0026quot;)\rfancy(1:30,mod2,mod3,mod5)\r## [[1]]\r## [1] \u0026quot;\u0026quot;\r## ## [[2]]\r## [1] \u0026quot;Deuce\u0026quot;\r## ## [[3]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[4]]\r## [1] \u0026quot;Deuce\u0026quot;\r## ## [[5]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[6]]\r## [1] \u0026quot;DeuceFizz\u0026quot;\r## ## [[7]]\r## [1] \u0026quot;\u0026quot;\r## ## [[8]]\r## [1] \u0026quot;Deuce\u0026quot;\r## ## [[9]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[10]]\r## [1] \u0026quot;DeuceBuzz\u0026quot;\r## ## [[11]]\r## [1] \u0026quot;\u0026quot;\r## ## [[12]]\r## [1] \u0026quot;DeuceFizz\u0026quot;\r## ## [[13]]\r## [1] \u0026quot;\u0026quot;\r## ## [[14]]\r## [1] \u0026quot;Deuce\u0026quot;\r## ## [[15]]\r## [1] \u0026quot;FizzBuzz\u0026quot;\r## ## [[16]]\r## [1] \u0026quot;Deuce\u0026quot;\r## ## [[17]]\r## [1] \u0026quot;\u0026quot;\r## ## [[18]]\r## [1] \u0026quot;DeuceFizz\u0026quot;\r## ## [[19]]\r## [1] \u0026quot;\u0026quot;\r## ## [[20]]\r## [1] \u0026quot;DeuceBuzz\u0026quot;\r## ## [[21]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[22]]\r## [1] \u0026quot;Deuce\u0026quot;\r## ## [[23]]\r## [1] \u0026quot;\u0026quot;\r## ## [[24]]\r## [1] \u0026quot;DeuceFizz\u0026quot;\r## ## [[25]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[26]]\r## [1] \u0026quot;Deuce\u0026quot;\r## ## [[27]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[28]]\r## [1] \u0026quot;Deuce\u0026quot;\r## ## [[29]]\r## [1] \u0026quot;\u0026quot;\r## ## [[30]]\r## [1] \u0026quot;DeuceFizzBuzz\u0026quot;\r\rAdding new rules\rless \u0026lt;- function(number, string) {\rfunction(input) {\rif_else(condition = input \u0026lt; number,\rtrue = string,\rfalse = \u0026quot;\u0026quot;)\r}\r}\rless10 \u0026lt;- less(10,\u0026quot;Small\u0026quot;)\rfancy(1:15,less10,mod3,mod5)\r## [[1]]\r## [1] \u0026quot;Small\u0026quot;\r## ## [[2]]\r## [1] \u0026quot;Small\u0026quot;\r## ## [[3]]\r## [1] \u0026quot;SmallFizz\u0026quot;\r## ## [[4]]\r## [1] \u0026quot;Small\u0026quot;\r## ## [[5]]\r## [1] \u0026quot;SmallBuzz\u0026quot;\r## ## [[6]]\r## [1] \u0026quot;SmallFizz\u0026quot;\r## ## [[7]]\r## [1] \u0026quot;Small\u0026quot;\r## ## [[8]]\r## [1] \u0026quot;Small\u0026quot;\r## ## [[9]]\r## [1] \u0026quot;SmallFizz\u0026quot;\r## ## [[10]]\r## [1] \u0026quot;Buzz\u0026quot;\r## ## [[11]]\r## [1] \u0026quot;\u0026quot;\r## ## [[12]]\r## [1] \u0026quot;Fizz\u0026quot;\r## ## [[13]]\r## [1] \u0026quot;\u0026quot;\r## ## [[14]]\r## [1] \u0026quot;\u0026quot;\r## ## [[15]]\r## [1] \u0026quot;FizzBuzz\u0026quot;\rThat is it have a great day.\n\r\r","date":1576972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576972800,"objectID":"44816a2105949ea87250f6f2cd17420e","permalink":"https://twosidesdata.netlify.com/2019/12/22/fizzbuzz-in-the-tidyverse/","publishdate":"2019-12-22T00:00:00Z","relpermalink":"/2019/12/22/fizzbuzz-in-the-tidyverse/","section":"post","summary":"FizzBuzz is and old kids games\nNot that popular where I am from Brazil, Fizz Buzz has a simple set of rules\nYou start counting from 1 (obviously) and when a number is a multiple of 3 you say Fizz,\nif the number is a multiple of 5 you say Buzz,\nand if the number is a multiple of both you shout FizzBuzz,\rAnd for every other case you can say the number itself, simple right?","tags":["tidyverse","programming"],"title":"FizzBuzz in the tidyverse","type":"post"},{"authors":["Bruno Carlin"],"categories":["r-project"],"content":"\r\r\r\r\r\rBreast Cancer problem.\nThis is a problem that I have trid to solve using just the old tidymodels package and got stuck so here is the new implementation using the amazing tune and workflows\rpackages\nSetting up Rmarkdown\rroot.dir =\n\rLoading Libraries\rlibrary(tidyverse)\r## -- Attaching packages ---------------------------------------------------------------------------------------------------------------------------------------------- tidyverse 1.3.0 --\r## \u0026lt;U+2713\u0026gt; ggplot2 3.2.1 \u0026lt;U+2713\u0026gt; purrr 0.3.3\r## \u0026lt;U+2713\u0026gt; tibble 2.1.3 \u0026lt;U+2713\u0026gt; dplyr 0.8.3\r## \u0026lt;U+2713\u0026gt; tidyr 1.0.0 \u0026lt;U+2713\u0026gt; stringr 1.4.0\r## \u0026lt;U+2713\u0026gt; readr 1.3.1 \u0026lt;U+2713\u0026gt; forcats 0.4.0\r## -- Conflicts ------------------------------------------------------------------------------------------------------------------------------------------------- tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rlibrary(tidymodels)\r## Registered S3 method overwritten by \u0026#39;xts\u0026#39;:\r## method from\r## as.zoo.xts zoo\r## -- Attaching packages --------------------------------------------------------------------------------------------------------------------------------------------- tidymodels 0.0.3 --\r## \u0026lt;U+2713\u0026gt; broom 0.5.3 \u0026lt;U+2713\u0026gt; recipes 0.1.8\r## \u0026lt;U+2713\u0026gt; dials 0.0.4 \u0026lt;U+2713\u0026gt; rsample 0.0.5\r## \u0026lt;U+2713\u0026gt; infer 0.5.1 \u0026lt;U+2713\u0026gt; yardstick 0.0.4\r## \u0026lt;U+2713\u0026gt; parsnip 0.0.4\r## -- Conflicts ------------------------------------------------------------------------------------------------------------------------------------------------ tidymodels_conflicts() --\r## x scales::discard() masks purrr::discard()\r## x dplyr::filter() masks stats::filter()\r## x recipes::fixed() masks stringr::fixed()\r## x dplyr::lag() masks stats::lag()\r## x dials::margin() masks ggplot2::margin()\r## x yardstick::spec() masks readr::spec()\r## x recipes::step() masks stats::step()\r## x recipes::yj_trans() masks scales::yj_trans()\rlibrary(janitor)\r## ## Attaching package: \u0026#39;janitor\u0026#39;\r## The following objects are masked from \u0026#39;package:stats\u0026#39;:\r## ## chisq.test, fisher.test\rlibrary(skimr)\rlibrary(DataExplorer)\rLoading Libraries: 3.65 sec elapsed\n\rGetting Data\rGot the dataset with headers on kaggle link, there is also a cool explanation about the problem there.\ndf \u0026lt;- read_csv(\u0026quot;breast_cancer.csv\u0026quot;)\r## Warning: Missing column names filled in: \u0026#39;X33\u0026#39; [33]\r## Parsed with column specification:\r## cols(\r## .default = col_double(),\r## diagnosis = col_character(),\r## X33 = col_character()\r## )\r## See spec(...) for full column specifications.\r## Warning: 569 parsing failures.\r## row col expected actual file\r## 1 -- 33 columns 32 columns \u0026#39;breast_cancer.csv\u0026#39;\r## 2 -- 33 columns 32 columns \u0026#39;breast_cancer.csv\u0026#39;\r## 3 -- 33 columns 32 columns \u0026#39;breast_cancer.csv\u0026#39;\r## 4 -- 33 columns 32 columns \u0026#39;breast_cancer.csv\u0026#39;\r## 5 -- 33 columns 32 columns \u0026#39;breast_cancer.csv\u0026#39;\r## ... ... .......... .......... ...................\r## See problems(...) for more details.\rSet Chunk: 0.16 sec elapsed\nThere is a strange extra column named X33 dealing with that using janitor package\ndf \u0026lt;- df %\u0026gt;% janitor::remove_empty_cols()\r## Warning: \u0026#39;janitor::remove_empty_cols\u0026#39; is deprecated.\r## Use \u0026#39;remove_empty(\u0026quot;cols\u0026quot;)\u0026#39; instead.\r## See help(\u0026quot;Deprecated\u0026quot;)\rCleaning Data: 0.01 sec elapsed\n\rVisualizing the data using DataExplorer and Skimr\rSkimr is a fast way to get info on your data even though the hist plot fails on my blog :(\rdf %\u0026gt;% skimr::skim()\r\rTable 1: Data summary\r\rName\rPiped data\r\rNumber of rows\r569\r\rNumber of columns\r32\r\r_______________________\r\r\rColumn type frequency:\r\r\rcharacter\r1\r\rnumeric\r31\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: character\n\r\rskim_variable\rn_missing\rcomplete_rate\rmin\rmax\rempty\rn_unique\rwhitespace\r\r\r\rdiagnosis\r0\r1\r1\r1\r0\r2\r0\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rid\r0\r1\r30371831.43\r125020585.61\r8670.00\r869218.00\r906024.00\r8813129.00\r911320502.00\r▇▁▁▁▁\r\rradius_mean\r0\r1\r14.13\r3.52\r6.98\r11.70\r13.37\r15.78\r28.11\r▂▇▃▁▁\r\rtexture_mean\r0\r1\r19.29\r4.30\r9.71\r16.17\r18.84\r21.80\r39.28\r▃▇▃▁▁\r\rperimeter_mean\r0\r1\r91.97\r24.30\r43.79\r75.17\r86.24\r104.10\r188.50\r▃▇▃▁▁\r\rarea_mean\r0\r1\r654.89\r351.91\r143.50\r420.30\r551.10\r782.70\r2501.00\r▇▃▂▁▁\r\rsmoothness_mean\r0\r1\r0.10\r0.01\r0.05\r0.09\r0.10\r0.11\r0.16\r▁▇▇▁▁\r\rcompactness_mean\r0\r1\r0.10\r0.05\r0.02\r0.06\r0.09\r0.13\r0.35\r▇▇▂▁▁\r\rconcavity_mean\r0\r1\r0.09\r0.08\r0.00\r0.03\r0.06\r0.13\r0.43\r▇▃▂▁▁\r\rconcave points_mean\r0\r1\r0.05\r0.04\r0.00\r0.02\r0.03\r0.07\r0.20\r▇▃▂▁▁\r\rsymmetry_mean\r0\r1\r0.18\r0.03\r0.11\r0.16\r0.18\r0.20\r0.30\r▁▇▅▁▁\r\rfractal_dimension_mean\r0\r1\r0.06\r0.01\r0.05\r0.06\r0.06\r0.07\r0.10\r▆▇▂▁▁\r\rradius_se\r0\r1\r0.41\r0.28\r0.11\r0.23\r0.32\r0.48\r2.87\r▇▁▁▁▁\r\rtexture_se\r0\r1\r1.22\r0.55\r0.36\r0.83\r1.11\r1.47\r4.88\r▇▅▁▁▁\r\rperimeter_se\r0\r1\r2.87\r2.02\r0.76\r1.61\r2.29\r3.36\r21.98\r▇▁▁▁▁\r\rarea_se\r0\r1\r40.34\r45.49\r6.80\r17.85\r24.53\r45.19\r542.20\r▇▁▁▁▁\r\rsmoothness_se\r0\r1\r0.01\r0.00\r0.00\r0.01\r0.01\r0.01\r0.03\r▇▃▁▁▁\r\rcompactness_se\r0\r1\r0.03\r0.02\r0.00\r0.01\r0.02\r0.03\r0.14\r▇▃▁▁▁\r\rconcavity_se\r0\r1\r0.03\r0.03\r0.00\r0.02\r0.03\r0.04\r0.40\r▇▁▁▁▁\r\rconcave points_se\r0\r1\r0.01\r0.01\r0.00\r0.01\r0.01\r0.01\r0.05\r▇▇▁▁▁\r\rsymmetry_se\r0\r1\r0.02\r0.01\r0.01\r0.02\r0.02\r0.02\r0.08\r▇▃▁▁▁\r\rfractal_dimension_se\r0\r1\r0.00\r0.00\r0.00\r0.00\r0.00\r0.00\r0.03\r▇▁▁▁▁\r\rradius_worst\r0\r1\r16.27\r4.83\r7.93\r13.01\r14.97\r18.79\r36.04\r▆▇▃▁▁\r\rtexture_worst\r0\r1\r25.68\r6.15\r12.02\r21.08\r25.41\r29.72\r49.54\r▃▇▆▁▁\r\rperimeter_worst\r0\r1\r107.26\r33.60\r50.41\r84.11\r97.66\r125.40\r251.20\r▇▇▃▁▁\r\rarea_worst\r0\r1\r880.58\r569.36\r185.20\r515.30\r686.50\r1084.00\r4254.00\r▇▂▁▁▁\r\rsmoothness_worst\r0\r1\r0.13\r0.02\r0.07\r0.12\r0.13\r0.15\r0.22\r▂▇▇▂▁\r\rcompactness_worst\r0\r1\r0.25\r0.16\r0.03\r0.15\r0.21\r0.34\r1.06\r▇▅▁▁▁\r\rconcavity_worst\r0\r1\r0.27\r0.21\r0.00\r0.11\r0.23\r0.38\r1.25\r▇▅▂▁▁\r\rconcave points_worst\r0\r1\r0.11\r0.07\r0.00\r0.06\r0.10\r0.16\r0.29\r▅▇▅▃▁\r\rsymmetry_worst\r0\r1\r0.29\r0.06\r0.16\r0.25\r0.28\r0.32\r0.66\r▅▇▁▁▁\r\rfractal_dimension_worst\r0\r1\r0.08\r0.02\r0.06\r0.07\r0.08\r0.09\r0.21\r▇▃▁▁▁\r\r\r\rSkimr: 0.15 sec elapsed\n\rData Explorer\rIs a imho a prettier option with individual cool plots and a super powerfull(but slow) report creation tool when working outside of an Rmarkdownm document\ndf %\u0026gt;% DataExplorer::plot_intro()\rdf %\u0026gt;% DataExplorer::plot_bar()\rdf %\u0026gt;% DataExplorer::plot_correlation()\rData Explorer individual plots: 1.05 sec elapsed\nThere are much more amaziong tools such as the ggforce package , but I hope you get the gist of the exploration stage.\n\r\rModeling\rFor now I am going to focus on the tools provided by the tidymodels packages and the KNN, in the future I may come back to add more models and probably to play around the DALEX package a little bit.\nJust to remember M is Malignant and B is Benign, we are trying to correcly classify our patients, I am going to ignore the id Varible since it should not be reliaded upon to generate predictions(Even though it may capture some interesting effects such as better screening for patients on the latter id’s).\nTrain Test Split\rUsually we split our data into training and test data to ensure a fair evaluation of the models or parameters being tested(hoping to avoid overfitting).\nThe workflow for the tidymodels is that we first split our data.\ndf_split \u0026lt;- df %\u0026gt;% rsample::initial_split(prop = 0.8)\rInitial Split: 0 sec elapsed\ndf_training \u0026lt;- df_split %\u0026gt;% training()\rdf_testing \u0026lt;- df_split %\u0026gt;% testing()\rTrain test split: 0.02 sec elapsed\nThen we model on our Training Data\n\rRecipes\rRecipes are used to preprocess our data, the main mistake here is using the whole data set.\nThe recipe package helps us with this process.\nFor those not familiarized with the formula notation I am fitting the model on all variables except the id variable.\nI am than Normalizing my data since the KNN alghoritm is sensible to the scale of the variables being used, I am also excluding variables with high absolute correlation amongst themselves.\nRecipes are easy to read and can be quite complex\ndf_recipe \u0026lt;- training(df_split) %\u0026gt;% recipe(diagnosis ~ .) %\u0026gt;%\rstep_rm(id) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors(),all_numeric()) %\u0026gt;% step_corr(all_predictors())\rrecipes: 0 sec elapsed\nWe could then create our train and test data frames by baking our recipe and juicing our recipe\n# df_testing \u0026lt;- df_recipe %\u0026gt;% # bake(testing(df_split))\r# df_testing\r#df_training \u0026lt;- juice(df_recipe)\runnamed-chunk-1: 0 sec elapsed\nBut I am going for a Bayes search approch\n\rCross Validation and Bayes Search\rCross Validation\rWe further divide our data frame into folds in order to improve our certainty that the ideal number of neighbours is right.\ncv_splits \u0026lt;- df_testing %\u0026gt;% vfold_cv(v = 5)\rcvfold split: 0.02 sec elapsed\n\rUsing the new tune package currently on github\rlibrary(tune)\rknn_mod \u0026lt;- nearest_neighbor(neighbors = tune(), weight_func = tune()) %\u0026gt;% set_engine(\u0026quot;kknn\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;)\runnamed-chunk-2: 0.05 sec elapsed\n\rCombining everything so far in the new package workflow\rlibrary(workflows)\rknn_wflow \u0026lt;- workflow() %\u0026gt;% add_model(knn_mod) %\u0026gt;% add_recipe(df_recipe)\runnamed-chunk-3: 0.04 sec elapsed\n\rLimiting our search\rknn_param \u0026lt;- knn_wflow %\u0026gt;% parameters() %\u0026gt;% update(\rneighbors = neighbors(c(3, 50)),\rweight_func = weight_func(values = c(\u0026quot;rectangular\u0026quot;, \u0026quot;inv\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;triangular\u0026quot;))\r)\runnamed-chunk-4: 0.03 sec elapsed\n\rSearching for the best model\rI used 5 iterations as the limit for the process because of printing reasons.\nKeep in mind that mtune will maximize just the first metric from the package yardstick\nctrl \u0026lt;- control_bayes(verbose = TRUE,no_improve = 5)\rset.seed(42)\rknn_search \u0026lt;- tune_bayes(knn_wflow,\rresamples = cv_splits,\rinitial = 5,\riter = 20,\rparam_info = knn_param,\rcontrol = ctrl,\rmetrics = metric_set(roc_auc,accuracy))\r## \r## \u0026gt; Generating a set of 5 initial parameter results\r## v Initialization complete\r## \r## Optimizing roc_auc using the expected improvement\r## \r## -- Iteration 1 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r## \r## i Current best: roc_auc=0.9706 (@iter 0)\r## i Gaussian process model\r## v Gaussian process model\r## i Generating 139 candidates\r## i Predicted candidates\r## i neighbors=31, weight_func=inv\r## i Estimating performance\r## v Estimating performance\r## \u0026lt;U+24E7\u0026gt; Newest results: roc_auc=0.969 (+/-0.013)\r## \r## -- Iteration 2 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r## \r## i Current best: roc_auc=0.9706 (@iter 0)\r## i Gaussian process model\r## v Gaussian process model\r## i Generating 138 candidates\r## i Predicted candidates\r## i neighbors=3, weight_func=rectangular\r## i Estimating performance\r## v Estimating performance\r## \u0026lt;U+24E7\u0026gt; Newest results: roc_auc=0.9538 (+/-0.0208)\r## \r## -- Iteration 3 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r## \r## i Current best: roc_auc=0.9706 (@iter 0)\r## i Gaussian process model\r## v Gaussian process model\r## i Generating 137 candidates\r## i Predicted candidates\r## i neighbors=50, weight_func=rectangular\r## i Estimating performance\r## v Estimating performance\r## \u0026lt;U+24E7\u0026gt; Newest results: roc_auc=0.9432 (+/-0.0214)\r## \r## -- Iteration 4 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r## \r## i Current best: roc_auc=0.9706 (@iter 0)\r## i Gaussian process model\r## v Gaussian process model\r## i Generating 136 candidates\r## i Predicted candidates\r## i neighbors=15, weight_func=rectangular\r## i Estimating performance\r## v Estimating performance\r## \u0026lt;U+24E7\u0026gt; Newest results: roc_auc=0.9676 (+/-0.0184)\r## \r## -- Iteration 5 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r## \r## i Current best: roc_auc=0.9706 (@iter 0)\r## i Gaussian process model\r## v Gaussian process model\r## i Generating 135 candidates\r## i Predicted candidates\r## i neighbors=3, weight_func=gaussian\r## i Estimating performance\r## v Estimating performance\r## \u0026lt;U+24E7\u0026gt; Newest results: roc_auc=0.9562 (+/-0.0205)\r## ! No improvement for 5 iterations; returning current results.\rBayes Search: 26.38 sec elapsed\n\rVisualizing our search\rautoplot(knn_search, type = \u0026quot;performance\u0026quot;, metric = \u0026quot;accuracy\u0026quot;)\runnamed-chunk-5: 0.16 sec elapsed\nautoplot(knn_search, type = \u0026quot;performance\u0026quot;, metric = \u0026quot;roc_auc\u0026quot;)\runnamed-chunk-6: 0.2 sec elapsed\n\rSeing the best result\rcollect_metrics(knn_search) %\u0026gt;% dplyr::filter(.metric == \u0026quot;accuracy\u0026quot;) %\u0026gt;% arrange(mean %\u0026gt;% desc)\r## # A tibble: 10 x 8\r## neighbors weight_func .iter .metric .estimator mean n std_err\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 3 gaussian 5 accuracy binary 0.939 5 0.0261\r## 2 3 rectangular 2 accuracy binary 0.939 5 0.0261\r## 3 5 inv 0 accuracy binary 0.939 5 0.0295\r## 4 13 inv 0 accuracy binary 0.939 5 0.0221\r## 5 42 gaussian 0 accuracy binary 0.939 5 0.0294\r## 6 15 rectangular 4 accuracy binary 0.938 5 0.0263\r## 7 33 inv 0 accuracy binary 0.930 5 0.0259\r## 8 31 inv 1 accuracy binary 0.929 5 0.0222\r## 9 27 rectangular 0 accuracy binary 0.921 5 0.0253\r## 10 50 rectangular 3 accuracy binary 0.859 5 0.0207\runnamed-chunk-7: 0.03 sec elapsed\ncollect_metrics(knn_search) %\u0026gt;% dplyr::filter(.metric == \u0026quot;roc_auc\u0026quot;) %\u0026gt;% arrange(mean %\u0026gt;% desc)\r## # A tibble: 10 x 8\r## neighbors weight_func .iter .metric .estimator mean n std_err\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 33 inv 0 roc_auc binary 0.971 5 0.0165\r## 2 31 inv 1 roc_auc binary 0.969 5 0.0130\r## 3 42 gaussian 0 roc_auc binary 0.968 5 0.0182\r## 4 15 rectangular 4 roc_auc binary 0.968 5 0.0184\r## 5 13 inv 0 roc_auc binary 0.967 5 0.0203\r## 6 27 rectangular 0 roc_auc binary 0.961 5 0.0168\r## 7 5 inv 0 roc_auc binary 0.959 5 0.0182\r## 8 3 gaussian 5 roc_auc binary 0.956 5 0.0205\r## 9 3 rectangular 2 roc_auc binary 0.954 5 0.0208\r## 10 50 rectangular 3 roc_auc binary 0.943 5 0.0214\runnamed-chunk-8: 0.03 sec elapsed\n\rExtracting the best model\rbest_metrics \u0026lt;- collect_metrics(knn_search) %\u0026gt;% dplyr::filter(.metric == \u0026quot;roc_auc\u0026quot;) %\u0026gt;% arrange(mean %\u0026gt;% desc) %\u0026gt;% head(1) %\u0026gt;% select(neighbors,weight_func) %\u0026gt;% as.list()\runnamed-chunk-9: 0.01 sec elapsed\n\rCreating production model\rproduction_knn \u0026lt;- nearest_neighbor(neighbors = best_metrics$neighbors,weight_func = best_metrics$weight_func) %\u0026gt;% set_engine(\u0026quot;kknn\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;)\runnamed-chunk-10: 0 sec elapsed\n\rCreating production wflow\rproduction_wflow \u0026lt;- workflow() %\u0026gt;% add_model(production_knn) %\u0026gt;% add_recipe(df_recipe)\runnamed-chunk-11: 0 sec elapsed\n\rFinally applying testing production model\rfit_prod \u0026lt;- fit(production_wflow,df_training)\rFitting production: 0.28 sec elapsed\n\rMetrics\rfit_prod %\u0026gt;% predict(df_testing) %\u0026gt;% bind_cols(df_testing %\u0026gt;% transmute(diagnosis = diagnosis %\u0026gt;% as.factor())) %\u0026gt;% yardstick::metrics(truth = diagnosis,estimate = .pred_class)\r## # A tibble: 2 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.965\r## 2 kap binary 0.926\rCalculating metrics: 0.03 sec elapsed\npredict(fit_prod,df_testing,type = \u0026#39;prob\u0026#39;) %\u0026gt;%\rbind_cols(df_testing %\u0026gt;% transmute(diagnosis = diagnosis %\u0026gt;% as.factor())) %\u0026gt;% yardstick::roc_auc(truth = diagnosis,.pred_B)\r## # A tibble: 1 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 roc_auc binary 0.989\rCalculating metrics auc: 0.04 sec elapsed\n\r\rAnother visualization\rknn_naive \u0026lt;- nearest_neighbor(neighbors = tune()) %\u0026gt;% set_engine(\u0026quot;kknn\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;)\runnamed-chunk-12: 0 sec elapsed\nCombining everything so far in the new package workflow\rknn_wflow_naive \u0026lt;- workflow() %\u0026gt;% add_model(knn_naive) %\u0026gt;% add_recipe(df_recipe)\runnamed-chunk-13: 0 sec elapsed\nknn_naive_param \u0026lt;- knn_wflow %\u0026gt;% parameters() %\u0026gt;% update(\rneighbors = neighbors(c(10, 50))\r)\runnamed-chunk-14: 0.02 sec elapsed\nOne advantage of the naive search is that it is easy to parallelize\nall_cores \u0026lt;- parallel::detectCores(logical = FALSE)\rlibrary(doParallel)\r## Loading required package: foreach\r## ## Attaching package: \u0026#39;foreach\u0026#39;\r## The following objects are masked from \u0026#39;package:purrr\u0026#39;:\r## ## accumulate, when\r## Loading required package: iterators\r## Loading required package: parallel\rcl \u0026lt;- makePSOCKcluster(all_cores)\rregisterDoParallel(cl)\rset up parallel: 1.29 sec elapsed\nctrl \u0026lt;- control_grid(verbose = FALSE)\rset.seed(42)\rnaive_search \u0026lt;- tune_grid(knn_wflow_naive,\rresamples = cv_splits,\rparam_info = knn_naive_param,\rcontrol = ctrl,\rgrid = 50,\rmetrics = metric_set(roc_auc,accuracy))\rnaive grid search: 6.31 sec elapsed\nbest_naive_metrics \u0026lt;- collect_metrics(naive_search) %\u0026gt;% dplyr::filter(.metric == \u0026quot;roc_auc\u0026quot;) %\u0026gt;% arrange(mean %\u0026gt;% desc)\rDT::datatable(best_naive_metrics,options = list(pageLength = 5, scrollX=T))\r\r{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\"],[18,20,24,25,26,28,29,30,31,32,17,21,34,35,22,23,15,16,36,37,38,39,40,41,42,46,10,11,12,13,14,43,44,45,47,48,49],[\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\",\"roc_auc\"],[\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\",\"binary\"],[0.974487179487179,0.974487179487179,0.973717948717949,0.973717948717949,0.973717948717949,0.973717948717949,0.973717948717949,0.973717948717949,0.973717948717949,0.973717948717949,0.972948717948718,0.972820512820513,0.972179487179487,0.972179487179487,0.972051282051282,0.972051282051282,0.971410256410256,0.971410256410256,0.970641025641026,0.970641025641026,0.970641025641026,0.970641025641026,0.970641025641026,0.970641025641026,0.970641025641026,0.970641025641026,0.969871794871795,0.969871794871795,0.969871794871795,0.969871794871795,0.969871794871795,0.969102564102564,0.969102564102564,0.969102564102564,0.969102564102564,0.969102564102564,0.969102564102564],[10,10,5,5,10,10,5,10,5,10,5,5,10,5,10,5,5,5,5,5,10,5,5,5,10,10,5,5,5,5,10,5,5,5,5,5,10],[0.00941568502832165,0.00941568502832165,0.0146765476846401,0.0146765476846401,0.00978436512309337,0.00978436512309337,0.0146765476846401,0.00978436512309337,0.0146765476846401,0.00978436512309337,0.0152483279604168,0.0134532941787636,0.0105578910181491,0.0158368365272237,0.00933992910550258,0.0140098936582539,0.0159840485902653,0.0159840485902653,0.0165285201685211,0.0165285201685211,0.0110190134456807,0.0165285201685211,0.0165285201685211,0.0165285201685211,0.0110190134456807,0.01089902787098,0.0168286018698568,0.0168286018698568,0.0168286018698568,0.0168286018698568,0.0112190679132379,0.0173295208976781,0.0173295208976781,0.0173295208976781,0.0173295208976781,0.0173295208976781,0.0115530139317854]],\"container\":\"\\n \\n \\n  \\n neighbors\\n .metric\\n .estimator\\n mean\\n n\\n std_err\\n \\n \\n\",\"options\":{\"pageLength\":5,\"scrollX\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,4,5,6]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}unnamed-chunk-15: 0.03 sec elapsed\np \u0026lt;- best_naive_metrics %\u0026gt;% ggplot() +\raes(neighbors,mean) +\rgeom_point() +\rylim(.95,1)\rp\runnamed-chunk-16: 0.16 sec elapsed\n\r\r\rChecking the timing table\r(x \u0026lt;- tic.log() %\u0026gt;%\ras.character() %\u0026gt;% tibble(log = .) %\u0026gt;% separate(log,sep = \u0026#39;: \u0026#39;,into = c(\u0026#39;name\u0026#39;,\u0026#39;time\u0026#39;))) %\u0026gt;% separate(time, sep = \u0026#39; \u0026#39;,c(\u0026#39;measure\u0026#39;,\u0026#39;units\u0026#39;)) %\u0026gt;%\rmutate(measure = measure %\u0026gt;% as.numeric()) %\u0026gt;% arrange(measure %\u0026gt;% desc())\r## Warning: Expected 2 pieces. Additional pieces discarded in 31 rows [1, 2, 3, 4,\r## 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\r## # A tibble: 31 x 3\r## name measure units\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;\r## 1 Bayes Search 26.4 sec ## 2 naive grid search 6.31 sec ## 3 Loading Libraries 3.65 sec ## 4 set up parallel 1.29 sec ## 5 Data Explorer individual plots 1.05 sec ## 6 Fitting production 0.28 sec ## 7 unnamed-chunk-6 0.2 sec ## 8 Set Chunk 0.16 sec ## 9 unnamed-chunk-5 0.16 sec ## 10 unnamed-chunk-16 0.16 sec ## # … with 21 more rows\runnamed-chunk-17: 0.02 sec elapsed\n\r","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"fd164806f74a21dae5f074a433a61295","permalink":"https://twosidesdata.netlify.com/2019/08/15/classification-knn/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/2019/08/15/classification-knn/","section":"post","summary":"Breast Cancer problem.\nThis is a problem that I have trid to solve using just the old tidymodels package and got stuck so here is the new implementation using the amazing tune and workflows\rpackages\nSetting up Rmarkdown\rroot.dir =\n\rLoading Libraries\rlibrary(tidyverse)\r## -- Attaching packages ---------------------------------------------------------------------------------------------------------------------------------------------- tidyverse 1.3.0 --\r## \u0026lt;U+2713\u0026gt; ggplot2 3.2.1 \u0026lt;U+2713\u0026gt; purrr 0.3.3\r## \u0026lt;U+2713\u0026gt; tibble 2.1.3 \u0026lt;U+2713\u0026gt; dplyr 0.","tags":["tidyverse","tidymodels"],"title":"Classification - KNN","type":"post"},{"authors":null,"categories":["r-project","R and Python"],"content":"\r\n\r\n\r\n\r\n","date":1554422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554422400,"objectID":"0b1239b1586ff5b25e6b3707ae1fc1ae","permalink":"https://twosidesdata.netlify.com/2019/04/05/exploratory-data-analysis-basic-statistical-inference/","publishdate":"2019-04-05T00:00:00Z","relpermalink":"/2019/04/05/exploratory-data-analysis-basic-statistical-inference/","section":"post","summary":"","tags":["dplyr","pandas","scipy","tidyverse","reticulate","R Markdown"],"title":"exploratory data analysis: basic statistical inference","type":"post"},{"authors":["Bruno Carlin"],"categories":["R and Python","r-project"],"content":"\rThis is an basic example of how you can use either R or Python to accomplish the same goals, I really enjoy using the tidyverse but as you will see sometimes Python is just the more intuitive option. If you find yourself confused on whether a code chunk is an R or Python code please ask me or check my github page for this project. \r1 Getting Started, we will use multiple functions from both languages\r1.1 How to set up reticulate?\r1.1.1 Setting root folder\r1.1.2 Libraries\r\r\r2 Python\r2.1 Knowing data frames\r2.1.1 Defining pandas series\r2.1.2 Indexing\r\r2.2 Combining two pd series\r2.2.1 Create pd series from dictionary 1\r2.2.2 Combining the pd series into a data frame\r2.2.3 Data frame properties\r2.2.4 Creating some new columns\r2.2.5 Ordering a data frame\r2.2.6 Subsetting\r\r2.3 Real data\r2.3.1 Reading data\r2.3.2 Variable types\r2.3.3 Basic Description\r2.3.4 Subsetting data\r2.3.5 Creating new columns with real data\r2.3.6 Creating a new smaller data frame\r2.3.7 Plotting an line plot\r2.3.8 Filtering and replace data\r2.3.9 Groupby example\r2.3.10 Ploting an histogram\r2.3.11 Handling Missing values\r2.3.12 Replacing names with an dictionary\r\r2.4 Passing Objects\r2.4.1 Python to R\r\r\r3 R\r3.1 Knowing data frames\r3.1.1 Defining an data frame\r3.1.2 Index search\r\r3.2 Creating an data frame from two R series\r3.2.1 Create a date frame using an list\r3.2.2 Create a date frame using an list 2\r3.2.3 Subsetting an data frame using join or cbind\r3.2.4 Some info on our data frame\r3.2.5 Creating new columns using mutate and basic R\r3.2.6 Ordering an data frame using the tidy way arrange or order.\r3.2.7 Filtering rows using standard R code or filter.\r\r3.3 Real Case\r3.3.1 Two way of importing an csv\r3.3.2 Let’s look at our data\r3.3.3 Types of columns r\r3.3.4 Basic Description real data using Glimpse and str\r3.3.5 Subsetting Data with select or base R\r3.3.6 Creating a new smaller data frame using transmute and base\r3.3.7 Ploting with ggplot\r3.3.8 Filtering and replace data\r3.3.9 Groupby example in tidyverse\r3.3.10 Ploting an histogram using ggplot2\r3.3.11 Handling Missing values in R\r3.3.12 Replacing names with an case when aproach\r\r3.4 Passing Objects to Python\r\r\r\rI am currently doing exercises from digital house brasil\n1 Getting Started, we will use multiple functions from both languages\r1.1 How to set up reticulate?\r1.1.1 Setting root folder\rI recommend using the Files tab to find the your system path to the folder containig all the data.\nUse opts_knit to guarantee that your markdown functions will search for files\rin the folder specified, it is better that setwd() because it works on\rall languages.\nknitr::opts_knit$set(root.dir = normalizePath(\r\u0026quot;~/R/Blog/content/post/data\u0026quot;))\r\r1.1.2 Libraries\r\rR part\r\rlibrary(reticulate)\rlibrary(caTools)\rlibrary(roperators)\rlibrary(tidyverse)\rset.seed(123)\r\r\rPython part\r\rI am using my second virtual conda if you have just the root\rswitch to conda_list()[[1]][1].\nconda_list()[[1]][2] %\u0026gt;% use_condaenv(required = TRUE)\rLet’s see what version of python this env is running.\nimport platform\rprint(platform.python_version())\r## 3.7.2\rSome basic Data Science Libraries.\nimport numpy as np\rimport matplotlib.pyplot as plt\rimport pandas as pd\rimport os\r\r\r\r\r2 Python\r2.1 Knowing data frames\r2.1.1 Defining pandas series\rdata = pd.Series([0.25, 0.5, 0.75, 1.0])\rdata\r## 0 0.25\r## 1 0.50\r## 2 0.75\r## 3 1.00\r## dtype: float64\rdata.values\r## array([0.25, 0.5 , 0.75, 1. ])\rdata.index\r## RangeIndex(start=0, stop=4, step=1)\rdata[1]\r## 0.5\rdata[1:3]\r## 1 0.50\r## 2 0.75\r## dtype: float64\r\r2.1.2 Indexing\rdata = pd.Series([0.25, 0.5, 0.75, 1.0],\rindex=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;])\rdata\r## a 0.25\r## b 0.50\r## c 0.75\r## d 1.00\r## dtype: float64\rdata[\u0026#39;b\u0026#39;]\r## 0.5\r\r\r2.2 Combining two pd series\r2.2.1 Create pd series from dictionary 1\rpopulation_dict = {\u0026#39;California\u0026#39;: 38332521,\r\u0026#39;Florida\u0026#39;: 19552860,\r\u0026#39;Illinois\u0026#39;: 12882135,\r\u0026#39;New York\u0026#39;: 19651127, \u0026#39;Texas\u0026#39;: 26448193,}\rpopulation = pd.Series(population_dict)\rpopulation\r## California 38332521\r## Florida 19552860\r## Illinois 12882135\r## New York 19651127\r## Texas 26448193\r## dtype: int64\rpopulation[\u0026#39;California\u0026#39;]\r## 38332521\rpopulation[\u0026#39;California\u0026#39;:\u0026#39;Illinois\u0026#39;]\r## California 38332521\r## Florida 19552860\r## Illinois 12882135\r## dtype: int64\rone more example.\narea_dict = {\u0026#39;California\u0026#39;: 423967, \u0026#39;Florida\u0026#39;: 170312,\r\u0026#39;Illinois\u0026#39;: 149995,\r\u0026#39;New York\u0026#39;: 141297,\r\u0026#39;Texas\u0026#39;: 695662}\rarea = pd.Series(area_dict)\rarea\r## California 423967\r## Florida 170312\r## Illinois 149995\r## New York 141297\r## Texas 695662\r## dtype: int64\r\r2.2.2 Combining the pd series into a data frame\rstates = pd.DataFrame({\u0026#39;population\u0026#39;: population,\r\u0026#39;area\u0026#39;: area})\rstates\r## population area\r## California 38332521 423967\r## Florida 19552860 170312\r## Illinois 12882135 149995\r## New York 19651127 141297\r## Texas 26448193 695662\rtype(states)\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\rtype(states[\u0026quot;population\u0026quot;])\r## \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt;\rtype([states[\u0026quot;population\u0026quot;]])\r## \u0026lt;class \u0026#39;list\u0026#39;\u0026gt;\r\r2.2.3 Data frame properties\rstates.shape\r## (5, 2)\rstates.info()\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r## Index: 5 entries, California to Texas\r## Data columns (total 2 columns):\r## population 5 non-null int64\r## area 5 non-null int64\r## dtypes: int64(2)\r## memory usage: 280.0+ bytes\rstates.index\r## Index([\u0026#39;California\u0026#39;, \u0026#39;Florida\u0026#39;, \u0026#39;Illinois\u0026#39;, \u0026#39;New York\u0026#39;, \u0026#39;Texas\u0026#39;], dtype=\u0026#39;object\u0026#39;)\rstates.columns\r## Index([\u0026#39;population\u0026#39;, \u0026#39;area\u0026#39;], dtype=\u0026#39;object\u0026#39;)\rstates[\u0026#39;area\u0026#39;]\r## California 423967\r## Florida 170312\r## Illinois 149995\r## New York 141297\r## Texas 695662\r## Name: area, dtype: int64\r\r2.2.4 Creating some new columns\rstates[\u0026#39;density\u0026#39;] = states[\u0026#39;population\u0026#39;] / states[\u0026#39;area\u0026#39;]\rstates\r## population area density\r## California 38332521 423967 90.413926\r## Florida 19552860 170312 114.806121\r## Illinois 12882135 149995 85.883763\r## New York 19651127 141297 139.076746\r## Texas 26448193 695662 38.018740\r\r2.2.5 Ordering a data frame\rstates.sort_values([\u0026#39;population\u0026#39;], ascending = True)\r## population area density\r## Illinois 12882135 149995 85.883763\r## Florida 19552860 170312 114.806121\r## New York 19651127 141297 139.076746\r## Texas 26448193 695662 38.018740\r## California 38332521 423967 90.413926\rstates.sort_values([\u0026#39;area\u0026#39;], ascending = True)\r## population area density\r## New York 19651127 141297 139.076746\r## Illinois 12882135 149995 85.883763\r## Florida 19552860 170312 114.806121\r## California 38332521 423967 90.413926\r## Texas 26448193 695662 38.018740\rstates.sort_values([\u0026#39;density\u0026#39;], ascending = True)\r## population area density\r## Texas 26448193 695662 38.018740\r## Illinois 12882135 149995 85.883763\r## California 38332521 423967 90.413926\r## Florida 19552860 170312 114.806121\r## New York 19651127 141297 139.076746\r\r2.2.6 Subsetting\rstates[\u0026#39;Florida\u0026#39;:\u0026#39;Illinois\u0026#39;]\r## population area density\r## Florida 19552860 170312 114.806121\r## Illinois 12882135 149995 85.883763\rstates[1:3]\r## population area density\r## Florida 19552860 170312 114.806121\r## Illinois 12882135 149995 85.883763\rdata_pop = (states[\u0026#39;population\u0026#39;] \u0026gt; 19552860) \u0026amp; (states[\u0026#39;area\u0026#39;]\u0026gt;423967)\rdata_pop\r## California False\r## Florida False\r## Illinois False\r## New York False\r## Texas True\r## dtype: bool\rstates[(states[\u0026#39;population\u0026#39;] \u0026gt; 19552860) \u0026amp; (states[\u0026#39;area\u0026#39;]\u0026gt;423967)]\r## population area density\r## Texas 26448193 695662 38.01874\rstates[[\u0026#39;area\u0026#39;,\u0026#39;density\u0026#39;]]\r## area density\r## California 423967 90.413926\r## Florida 170312 114.806121\r## Illinois 149995 85.883763\r## New York 141297 139.076746\r## Texas 695662 38.018740\rstates[states.density \u0026gt; 100]\r## population area density\r## Florida 19552860 170312 114.806121\r## New York 19651127 141297 139.076746\rstates.loc[states.density \u0026gt; 100, [\u0026#39;population\u0026#39;, \u0026#39;density\u0026#39;]]\r## population density\r## Florida 19552860 114.806121\r## New York 19651127 139.076746\rstates.loc[states.density \u0026gt; 100][[\u0026#39;population\u0026#39;, \u0026#39;density\u0026#39;]]\r## population density\r## Florida 19552860 114.806121\r## New York 19651127 139.076746\rstates.loc[\u0026#39;California\u0026#39;, \u0026#39;density\u0026#39;]\r## 90.41392608386974\rstates.loc[\u0026#39;California\u0026#39;][[\u0026#39;density\u0026#39;]]\r## density 90.413926\r## Name: California, dtype: float64\rstates.iloc[0, 2]\r## 90.41392608386974\r\r\r2.3 Real data\r2.3.1 Reading data\rsales = pd.DataFrame(pd.read_csv(\u0026#39;2019-03-23-exploratory-data-analysis-basic-pandas-and-dplyr/UKretail.csv\u0026#39;,encoding=\u0026#39;latin\u0026#39;))\rsales.head()\r## InvoiceNo StockCode ... CustomerID Country\r## 0 536365 22752 ... 17850.0 United Kingdom\r## 1 536365 71053 ... 17850.0 United Kingdom\r## 2 536365 84029G ... 17850.0 United Kingdom\r## 3 536365 85123A ... 17850.0 United Kingdom\r## 4 536366 22633 ... 17850.0 United Kingdom\r## ## [5 rows x 8 columns]\rsales.tail(3)\r## InvoiceNo StockCode ... CustomerID Country\r## 325142 581587 22899 ... 12680.0 France\r## 325143 581587 23254 ... 12680.0 France\r## 325144 581587 23256 ... 12680.0 France\r## ## [3 rows x 8 columns]\rsales.index\r## RangeIndex(start=0, stop=325145, step=1)\r\r2.3.2 Variable types\rIf you need to return.\ntype(sales)\r\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\rtype(sales[\u0026quot;CustomerID\u0026quot;])\r## \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt;\rtype([sales[\u0026quot;CustomerID\u0026quot;]])\r\r## \u0026lt;class \u0026#39;list\u0026#39;\u0026gt;\r\r2.3.3 Basic Description\rsales.shape\r\r## (325145, 8)\rsales.columns.values\r\r## array([\u0026#39;InvoiceNo\u0026#39;, \u0026#39;StockCode\u0026#39;, \u0026#39;Description\u0026#39;, \u0026#39;Quantity\u0026#39;, \u0026#39;InvoiceDate\u0026#39;,\r## \u0026#39;UnitPrice\u0026#39;, \u0026#39;CustomerID\u0026#39;, \u0026#39;Country\u0026#39;], dtype=object)\rsales.info()\r\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r## RangeIndex: 325145 entries, 0 to 325144\r## Data columns (total 8 columns):\r## InvoiceNo 325145 non-null object\r## StockCode 325145 non-null object\r## Description 324275 non-null object\r## Quantity 325145 non-null int64\r## InvoiceDate 325145 non-null object\r## UnitPrice 325145 non-null float64\r## CustomerID 244154 non-null float64\r## Country 325145 non-null object\r## dtypes: float64(2), int64(1), object(5)\r## memory usage: 19.8+ MB\rsales.describe()\r## Quantity UnitPrice CustomerID\r## count 325145.000000 325145.000000 244154.000000\r## mean 9.273340 4.845239 15288.823120\r## std 154.394112 116.830451 1713.496816\r## min -80995.000000 -11062.060000 12347.000000\r## 25% 1.000000 1.250000 13959.000000\r## 50% 3.000000 2.080000 15150.000000\r## 75% 10.000000 4.130000 16792.750000\r## max 12540.000000 38970.000000 18287.000000\r\r2.3.4 Subsetting data\rsales[:4]\r## InvoiceNo StockCode ... CustomerID Country\r## 0 536365 22752 ... 17850.0 United Kingdom\r## 1 536365 71053 ... 17850.0 United Kingdom\r## 2 536365 84029G ... 17850.0 United Kingdom\r## 3 536365 85123A ... 17850.0 United Kingdom\r## ## [4 rows x 8 columns]\rsales[\u0026quot;CustomerID\u0026quot;].head()\r## 0 17850.0\r## 1 17850.0\r## 2 17850.0\r## 3 17850.0\r## 4 17850.0\r## Name: CustomerID, dtype: float64\rsales.loc[:,[\u0026#39;Quantity\u0026#39;]].head()\r## Quantity\r## 0 2\r## 1 6\r## 2 6\r## 3 6\r## 4 6\rsales.iloc[:,[3]].head()\r## Quantity\r## 0 2\r## 1 6\r## 2 6\r## 3 6\r## 4 6\rsales.iloc[0:6,2:3]\r## Description\r## 0 SET 7 BABUSHKA NESTING BOXES\r## 1 WHITE METAL LANTERN\r## 2 KNITTED UNION FLAG HOT WATER BOTTLE\r## 3 WHITE HANGING HEART T-LIGHT HOLDER\r## 4 HAND WARMER UNION JACK\r## 5 HOME BUILDING BLOCK WORD\r\r2.3.5 Creating new columns with real data\rsales[\u0026quot;Revenue\u0026quot;] = sales.Quantity * sales.UnitPrice\rsales.head()\r## InvoiceNo StockCode ... Country Revenue\r## 0 536365 22752 ... United Kingdom 15.30\r## 1 536365 71053 ... United Kingdom 20.34\r## 2 536365 84029G ... United Kingdom 20.34\r## 3 536365 85123A ... United Kingdom 15.30\r## 4 536366 22633 ... United Kingdom 11.10\r## ## [5 rows x 9 columns]\r\r2.3.6 Creating a new smaller data frame\rraw_sales = sales[[\u0026quot;Quantity\u0026quot;,\u0026quot;UnitPrice\u0026quot;, \u0026quot;Revenue\u0026quot;]]\rraw_sales.head()\r## Quantity UnitPrice Revenue\r## 0 2 7.65 15.30\r## 1 6 3.39 20.34\r## 2 6 3.39 20.34\r## 3 6 2.55 15.30\r## 4 6 1.85 11.10\rraw_sales.info()\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r## RangeIndex: 325145 entries, 0 to 325144\r## Data columns (total 3 columns):\r## Quantity 325145 non-null int64\r## UnitPrice 325145 non-null float64\r## Revenue 325145 non-null float64\r## dtypes: float64(2), int64(1)\r## memory usage: 7.4 MB\r\r2.3.7 Plotting an line plot\rimport matplotlib as plt\rfrom pylab import *\rsales.plot(x=\u0026quot;InvoiceDate\u0026quot;, y=\u0026quot;Revenue\u0026quot;, kind=\u0026quot;line\u0026quot;)\rplt.show()\r\r2.3.8 Filtering and replace data\rTo return\ncancels = sales[sales[\u0026quot;Revenue\u0026quot;]\u0026lt;0]\rcancels.shape\r## (5588, 9)\rsales.drop(cancels.index, inplace=True)\rsales.shape\r## (319557, 9)\r\r2.3.9 Groupby example\rCountryGroups = sales.groupby([\u0026quot;Country\u0026quot;])[\u0026quot;Revenue\u0026quot;].sum().reset_index()\rCountryGroups.sort_values(by= \u0026quot;Revenue\u0026quot;, ascending=False)\r## Country Revenue\r## 36 United Kingdom 5311080.101\r## 10 EIRE 176304.590\r## 24 Netherlands 165582.790\r## 14 Germany 138778.440\r## 13 France 127193.680\r## 0 Australia 79197.590\r## 31 Spain 36116.710\r## 33 Switzerland 34315.240\r## 3 Belgium 24014.970\r## 25 Norway 23182.220\r## 32 Sweden 21762.450\r## 20 Japan 21072.590\r## 27 Portugal 20109.410\r## 30 Singapore 13383.590\r## 6 Channel Islands 12556.740\r## 12 Finland 12362.880\r## 9 Denmark 11739.370\r## 19 Italy 10837.890\r## 16 Hong Kong 8227.020\r## 7 Cyprus 7781.900\r## 1 Austria 6100.960\r## 18 Israel 4225.780\r## 26 Poland 3974.080\r## 37 Unspecified 2898.650\r## 15 Greece 2677.570\r## 17 Iceland 2461.230\r## 34 USA 2388.740\r## 5 Canada 2093.390\r## 23 Malta 1318.990\r## 35 United Arab Emirates 1277.500\r## 21 Lebanon 1120.530\r## 22 Lithuania 1038.560\r## 11 European Community 876.550\r## 4 Brazil 602.310\r## 28 RSA 573.180\r## 8 Czech Republic 488.580\r## 2 Bahrain 343.400\r## 29 Saudi Arabia 90.720\r\r2.3.10 Ploting an histogram\rsales[sales[\u0026quot;CustomerID\u0026quot;] == 17850.0][\u0026quot;Revenue\u0026quot;].plot(kind=\u0026quot;hist\u0026quot;)\rplt.show()\ranother example.\nsales[sales[\u0026quot;StockCode\u0026quot;] == \u0026#39;71053\u0026#39;][\u0026quot;Quantity\u0026quot;].hist()\rplt.show()\r\r2.3.11 Handling Missing values\rto return\nsales.info()\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r## Int64Index: 319557 entries, 0 to 325144\r## Data columns (total 9 columns):\r## InvoiceNo 319557 non-null object\r## StockCode 319557 non-null object\r## Description 318687 non-null object\r## Quantity 319557 non-null int64\r## InvoiceDate 319557 non-null object\r## UnitPrice 319557 non-null float64\r## CustomerID 238801 non-null float64\r## Country 319557 non-null object\r## Revenue 319557 non-null float64\r## dtypes: float64(3), int64(1), object(5)\r## memory usage: 24.4+ MB\rsales.CustomerID.value_counts(dropna=False).nlargest(3)\r## NaN 80756\r## 17841.0 4702\r## 14911.0 3449\r## Name: CustomerID, dtype: int64\rsales.CustomerID.fillna(0, inplace=True)\rsales[sales.CustomerID.isnull()]\r## Empty DataFrame\r## Columns: [InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country, Revenue]\r## Index: []\rsales.info()\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r## Int64Index: 319557 entries, 0 to 325144\r## Data columns (total 9 columns):\r## InvoiceNo 319557 non-null object\r## StockCode 319557 non-null object\r## Description 318687 non-null object\r## Quantity 319557 non-null int64\r## InvoiceDate 319557 non-null object\r## UnitPrice 319557 non-null float64\r## CustomerID 319557 non-null float64\r## Country 319557 non-null object\r## Revenue 319557 non-null float64\r## dtypes: float64(3), int64(1), object(5)\r## memory usage: 24.4+ MB\r\r2.3.12 Replacing names with an dictionary\rmymap = {\u0026#39;United Kingdom\u0026#39;:1, \u0026#39;Netherlands\u0026#39;:2, \u0026#39;Germany\u0026#39;:3, \u0026#39;France\u0026#39;:4, \u0026#39;USA\u0026#39;:5} sales = sales.applymap(lambda s: mymap.get(s) if s in mymap else s)\rsales.head()\r## InvoiceNo StockCode ... Country Revenue\r## 0 536365 22752 ... 1 15.30\r## 1 536365 71053 ... 1 20.34\r## 2 536365 84029G ... 1 20.34\r## 3 536365 85123A ... 1 15.30\r## 4 536366 22633 ... 1 11.10\r## ## [5 rows x 9 columns]\rsales.Country.value_counts().nlargest(7)\r## 1 292640\r## 3 5466\r## 4 5026\r## EIRE 4789\r## Spain 1420\r## 2 1393\r## Belgium 1191\r## Name: Country, dtype: int64\r\r\r2.4 Passing Objects\r2.4.1 Python to R\rdata2 = pd.Series([0.25, 0.5, 0.75, 1.0])\rdata_t = py$data2\rdata_t\r## 0 1 2 3 ## 0.25 0.50 0.75 1.00\r\r\r\r3 R\r3.1 Knowing data frames\r3.1.1 Defining an data frame\rtidy way data \u0026lt;- tibble(0.25, 0.5, 0.75, 1.0)\rdata\r## # A tibble: 1 x 4\r## `0.25` `0.5` `0.75` `1`\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0.25 0.5 0.75 1\rdata[2]\r## # A tibble: 1 x 1\r## `0.5`\r## \u0026lt;dbl\u0026gt;\r## 1 0.5\rdata[2:3]\r## # A tibble: 1 x 2\r## `0.5` `0.75`\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0.5 0.75\rNot using tidyverse.\ndata \u0026lt;- data.frame(c(0.25, 0.5, 0.75, 1.0))\rrownames(data) \u0026lt;- 1:nrow(data)\rcolnames(data) \u0026lt;- \u0026quot;nope\u0026quot;\rdata\r## nope\r## 1 0.25\r## 2 0.50\r## 3 0.75\r## 4 1.00\r\r3.1.2 Index search\rdata \u0026lt;- data.frame(c(0.25, 0.5, 0.75, 1.0),row.names = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;d\u0026quot;))\rdata\r## c.0.25..0.5..0.75..1.\r## a 0.25\r## b 0.50\r## c 0.75\r## d 1.00\rdata[\u0026quot;b\u0026quot;,]\r## [1] 0.5\r\r\r3.2 Creating an data frame from two R series\r3.2.1 Create a date frame using an list\rpopulation_dict \u0026lt;- list(\r\u0026#39;California\u0026#39; = 38332521,\r\u0026#39;Florida\u0026#39; = 19552860,\r\u0026#39;Illinois\u0026#39; = 12882135,\r\u0026#39;New York\u0026#39; = 19651127,\r\u0026#39;Texas\u0026#39; = 26448193\r)\rpopulation \u0026lt;- population_dict %\u0026gt;% as_tibble()\rpopulation[\u0026#39;California\u0026#39;]\r## # A tibble: 1 x 1\r## California\r## \u0026lt;dbl\u0026gt;\r## 1 38332521\rpopulation %\u0026gt;% select(California:Illinois)\r## # A tibble: 1 x 3\r## California Florida Illinois\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 38332521 19552860 12882135\r\r3.2.2 Create a date frame using an list 2\rarea_dict = list(\r\u0026#39;California\u0026#39; = 423967, \u0026#39;Florida\u0026#39; = 170312,\r\u0026#39;Illinois\u0026#39; = 149995,\r\u0026#39;New York\u0026#39; = 141297,\r\u0026#39;Texas\u0026#39; = 695662\r)\rarea_dict %\u0026gt;% as_tibble() -\u0026gt; area\rarea\r## # A tibble: 1 x 5\r## California Florida Illinois `New York` Texas\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 423967 170312 149995 141297 695662\r\r3.2.3 Subsetting an data frame using join or cbind\rThe tidy way doesn`t support indexes so we can tidy our data.\ntidy_area \u0026lt;- area %\u0026gt;% gather(key = \u0026quot;state\u0026quot;, value = \u0026quot;area\u0026quot;)\rtidy_state \u0026lt;- population %\u0026gt;% gather(key = \u0026quot;state\u0026quot;, value = \u0026quot;population\u0026quot;)\rtidy_area\r## # A tibble: 5 x 2\r## state area\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 California 423967\r## 2 Florida 170312\r## 3 Illinois 149995\r## 4 New York 141297\r## 5 Texas 695662\rtidy_state\r## # A tibble: 5 x 2\r## state population\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 California 38332521\r## 2 Florida 19552860\r## 3 Illinois 12882135\r## 4 New York 19651127\r## 5 Texas 26448193\rtidy_area %\u0026gt;% left_join(tidy_state)\r## Joining, by = \u0026quot;state\u0026quot;\r## # A tibble: 5 x 3\r## state area population\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 California 423967 38332521\r## 2 Florida 170312 19552860\r## 3 Illinois 149995 12882135\r## 4 New York 141297 19651127\r## 5 Texas 695662 26448193\rtidy_merge \u0026lt;- cbind(tidy_area,tidy_state[,-1])\rstates \u0026lt;- tidy_merge\r\r3.2.4 Some info on our data frame\rclass(tidy_merge)\r## [1] \u0026quot;data.frame\u0026quot;\rclass(tidy_merge$population)\r## [1] \u0026quot;numeric\u0026quot;\rclass(list(tidy_merge[\u0026quot;population\u0026quot;]))\r## [1] \u0026quot;list\u0026quot;\rstates %\u0026gt;% dim()\r## [1] 5 3\rstates %\u0026gt;% str()\r## \u0026#39;data.frame\u0026#39;: 5 obs. of 3 variables:\r## $ state : chr \u0026quot;California\u0026quot; \u0026quot;Florida\u0026quot; \u0026quot;Illinois\u0026quot; \u0026quot;New York\u0026quot; ...\r## $ area : num 423967 170312 149995 141297 695662\r## $ population: num 38332521 19552860 12882135 19651127 26448193\rstates %\u0026gt;% glimpse()\r## Observations: 5\r## Variables: 3\r## $ state \u0026lt;chr\u0026gt; \u0026quot;California\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Illinois\u0026quot;, \u0026quot;New York\u0026quot;, \u0026quot;T...\r## $ area \u0026lt;dbl\u0026gt; 423967, 170312, 149995, 141297, 695662\r## $ population \u0026lt;dbl\u0026gt; 38332521, 19552860, 12882135, 19651127, 26448193\rstates[[\u0026quot;Estado\u0026quot;]]\r## NULL\rstates %\u0026gt;% colnames() %\u0026gt;% tail(-1)\r## [1] \u0026quot;area\u0026quot; \u0026quot;population\u0026quot;\rstates$area\r## [1] 423967 170312 149995 141297 695662\r\r3.2.5 Creating new columns using mutate and basic R\rstates$density \u0026lt;- states$population / states$area\rstates\r## state area population density\r## 1 California 423967 38332521 90.41393\r## 2 Florida 170312 19552860 114.80612\r## 3 Illinois 149995 12882135 85.88376\r## 4 New York 141297 19651127 139.07675\r## 5 Texas 695662 26448193 38.01874\r# or\rstates$density \u0026lt;- states[[\u0026quot;population\u0026quot;]] / states[[\u0026quot;area\u0026quot;]]\rstates\r## state area population density\r## 1 California 423967 38332521 90.41393\r## 2 Florida 170312 19552860 114.80612\r## 3 Illinois 149995 12882135 85.88376\r## 4 New York 141297 19651127 139.07675\r## 5 Texas 695662 26448193 38.01874\r#or\rstates %\u0026gt;% mutate(density = population / area)\r## state area population density\r## 1 California 423967 38332521 90.41393\r## 2 Florida 170312 19552860 114.80612\r## 3 Illinois 149995 12882135 85.88376\r## 4 New York 141297 19651127 139.07675\r## 5 Texas 695662 26448193 38.01874\r\r3.2.6 Ordering an data frame using the tidy way arrange or order.\rYou can also use -c() or desc() sometimes -c() can give strange results.\nstates %\u0026gt;% arrange(desc(population))\r## state area population density\r## 1 California 423967 38332521 90.41393\r## 2 Texas 695662 26448193 38.01874\r## 3 New York 141297 19651127 139.07675\r## 4 Florida 170312 19552860 114.80612\r## 5 Illinois 149995 12882135 85.88376\rstates[order(states$area),]\r## state area population density\r## 4 New York 141297 19651127 139.07675\r## 3 Illinois 149995 12882135 85.88376\r## 2 Florida 170312 19552860 114.80612\r## 1 California 423967 38332521 90.41393\r## 5 Texas 695662 26448193 38.01874\r# Mix and match all three formas\rstates %\u0026gt;% arrange(-c(density),desc(population,area),state)\r## state area population density\r## 1 New York 141297 19651127 139.07675\r## 2 Florida 170312 19552860 114.80612\r## 3 California 423967 38332521 90.41393\r## 4 Illinois 149995 12882135 85.88376\r## 5 Texas 695662 26448193 38.01874\r\r3.2.7 Filtering rows using standard R code or filter.\rstates[1:3,]\r## state area population density\r## 1 California 423967 38332521 90.41393\r## 2 Florida 170312 19552860 114.80612\r## 3 Illinois 149995 12882135 85.88376\rdata_pop \u0026lt;- states[states$population \u0026gt; 19552860 \u0026amp; states$area \u0026gt; 423967,]\rdata_pop\r## state area population density\r## 5 Texas 695662 26448193 38.01874\rstates %\u0026gt;% filter(population \u0026gt; 19552860 \u0026amp; area \u0026gt; 423967)\r## state area population density\r## 1 Texas 695662 26448193 38.01874\ryou can mix and match filter for rows and select for columns.\nstates %\u0026gt;% filter(density \u0026gt; 100)\r## state area population density\r## 1 Florida 170312 19552860 114.8061\r## 2 New York 141297 19651127 139.0767\rstates %\u0026gt;%\rfilter(density \u0026gt; 100) %\u0026gt;% select(population,density)\r## population density\r## 1 19552860 114.8061\r## 2 19651127 139.0767\rstates[1,4]\r## [1] 90.41393\r\r\r3.3 Real Case\r3.3.1 Two way of importing an csv\rsales \u0026lt;- read_csv(\u0026#39;2019-03-23-exploratory-data-analysis-basic-pandas-and-dplyr/UKretail.csv\u0026#39;)\r## Parsed with column specification:\r## cols(\r## InvoiceNo = col_character(),\r## StockCode = col_character(),\r## Description = col_character(),\r## Quantity = col_double(),\r## InvoiceDate = col_datetime(format = \u0026quot;\u0026quot;),\r## UnitPrice = col_double(),\r## CustomerID = col_double(),\r## Country = col_character()\r## )\rsales \u0026lt;- read.csv(\u0026#39;2019-03-23-exploratory-data-analysis-basic-pandas-and-dplyr/UKretail.csv\u0026#39;)\rIf you think this looks like an ugly path and a was of space I would agree we\rcan fix this by using one of my favorite thinks from python the \"\"key I avoided.\nI am now using it on the python part to show the power of neat line.\npath_file = \u0026#39;\\\r2019-03-23-exploratory-data-analysis-basic-pandas-and-dplyr/\\\rUKretail.csv\u0026#39; \rsales \u0026lt;- read_csv(py$path_file)\r## Parsed with column specification:\r## cols(\r## InvoiceNo = col_character(),\r## StockCode = col_character(),\r## Description = col_character(),\r## Quantity = col_double(),\r## InvoiceDate = col_datetime(format = \u0026quot;\u0026quot;),\r## UnitPrice = col_double(),\r## CustomerID = col_double(),\r## Country = col_character()\r## )\rFinally our first usefull python to r functionality!\n\r3.3.2 Let’s look at our data\rsales %\u0026gt;% head()\r## # A tibble: 6 x 8\r## InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 536365 22752 SET 7 BABU~ 2 2010-12-01 08:26:02 7.65\r## 2 536365 71053 WHITE META~ 6 2010-12-01 08:26:02 3.39\r## 3 536365 84029G KNITTED UN~ 6 2010-12-01 08:26:02 3.39\r## 4 536365 85123A WHITE HANG~ 6 2010-12-01 08:26:02 2.55\r## 5 536366 22633 HAND WARME~ 6 2010-12-01 08:28:02 1.85\r## 6 536367 21754 HOME BUILD~ 3 2010-12-01 08:33:59 5.95\r## # ... with 2 more variables: CustomerID \u0026lt;dbl\u0026gt;, Country \u0026lt;chr\u0026gt;\rsales %\u0026gt;% tail(3)\r## # A tibble: 3 x 8\r## InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 581587 22899 CHILDREN\u0026#39;S~ 6 2011-12-09 12:49:59 2.1 ## 2 581587 23254 CHILDRENS ~ 4 2011-12-09 12:49:59 4.15\r## 3 581587 23256 CHILDRENS ~ 4 2011-12-09 12:49:59 4.15\r## # ... with 2 more variables: CustomerID \u0026lt;dbl\u0026gt;, Country \u0026lt;chr\u0026gt;\r\r3.3.3 Types of columns r\rIf you payed attention read_ tries to inform what conversion was used in each column that is specially cool because base R tends to create unesceassary factor whne in fact you are working with strings, but know you can choose between three different implementation of the read command.\nA cool thing about tibbles is that they are in fact still data.frame.\nsales %\u0026gt;% class()\r## [1] \u0026quot;spec_tbl_df\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\rPay attention to the R difference between “[[” and “[” if you recall this is the “opposite” of the python behavior.\nJump to python implementation.\nsales[[\u0026quot;CustomerID\u0026quot;]] %\u0026gt;% class()\r## [1] \u0026quot;numeric\u0026quot;\rsales[\u0026quot;CustomerID\u0026quot;] %\u0026gt;% class()\r## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\r\r3.3.4 Basic Description real data using Glimpse and str\rsales %\u0026gt;% dim()\r## [1] 325145 8\rsales %\u0026gt;% colnames()\r## [1] \u0026quot;InvoiceNo\u0026quot; \u0026quot;StockCode\u0026quot; \u0026quot;Description\u0026quot; \u0026quot;Quantity\u0026quot; \u0026quot;InvoiceDate\u0026quot;\r## [6] \u0026quot;UnitPrice\u0026quot; \u0026quot;CustomerID\u0026quot; \u0026quot;Country\u0026quot;\rsales %\u0026gt;% glimpse()\r## Observations: 325,145\r## Variables: 8\r## $ InvoiceNo \u0026lt;chr\u0026gt; \u0026quot;536365\u0026quot;, \u0026quot;536365\u0026quot;, \u0026quot;536365\u0026quot;, \u0026quot;536365\u0026quot;, \u0026quot;536366\u0026quot;, ...\r## $ StockCode \u0026lt;chr\u0026gt; \u0026quot;22752\u0026quot;, \u0026quot;71053\u0026quot;, \u0026quot;84029G\u0026quot;, \u0026quot;85123A\u0026quot;, \u0026quot;22633\u0026quot;, \u0026quot;21...\r## $ Description \u0026lt;chr\u0026gt; \u0026quot;SET 7 BABUSHKA NESTING BOXES\u0026quot;, \u0026quot;WHITE METAL LANTE...\r## $ Quantity \u0026lt;dbl\u0026gt; 2, 6, 6, 6, 6, 3, 3, 4, 6, 6, 6, 8, 4, 3, 3, 48, 2...\r## $ InvoiceDate \u0026lt;dttm\u0026gt; 2010-12-01 08:26:02, 2010-12-01 08:26:02, 2010-12...\r## $ UnitPrice \u0026lt;dbl\u0026gt; 7.65, 3.39, 3.39, 2.55, 1.85, 5.95, 5.95, 7.95, 1....\r## $ CustomerID \u0026lt;dbl\u0026gt; 17850, 17850, 17850, 17850, 17850, 13047, 13047, 1...\r## $ Country \u0026lt;chr\u0026gt; \u0026quot;United Kingdom\u0026quot;, \u0026quot;United Kingdom\u0026quot;, \u0026quot;United Kingdo...\rsales %\u0026gt;% str()\r## Classes \u0026#39;spec_tbl_df\u0026#39;, \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 325145 obs. of 8 variables:\r## $ InvoiceNo : chr \u0026quot;536365\u0026quot; \u0026quot;536365\u0026quot; \u0026quot;536365\u0026quot; \u0026quot;536365\u0026quot; ...\r## $ StockCode : chr \u0026quot;22752\u0026quot; \u0026quot;71053\u0026quot; \u0026quot;84029G\u0026quot; \u0026quot;85123A\u0026quot; ...\r## $ Description: chr \u0026quot;SET 7 BABUSHKA NESTING BOXES\u0026quot; \u0026quot;WHITE METAL LANTERN\u0026quot; \u0026quot;KNITTED UNION FLAG HOT WATER BOTTLE\u0026quot; \u0026quot;WHITE HANGING HEART T-LIGHT HOLDER\u0026quot; ...\r## $ Quantity : num 2 6 6 6 6 3 3 4 6 6 ...\r## $ InvoiceDate: POSIXct, format: \u0026quot;2010-12-01 08:26:02\u0026quot; \u0026quot;2010-12-01 08:26:02\u0026quot; ...\r## $ UnitPrice : num 7.65 3.39 3.39 2.55 1.85 5.95 5.95 7.95 1.65 2.1 ...\r## $ CustomerID : num 17850 17850 17850 17850 17850 ...\r## $ Country : chr \u0026quot;United Kingdom\u0026quot; \u0026quot;United Kingdom\u0026quot; \u0026quot;United Kingdom\u0026quot; \u0026quot;United Kingdom\u0026quot; ...\r## - attr(*, \u0026quot;spec\u0026quot;)=\r## .. cols(\r## .. InvoiceNo = col_character(),\r## .. StockCode = col_character(),\r## .. Description = col_character(),\r## .. Quantity = col_double(),\r## .. InvoiceDate = col_datetime(format = \u0026quot;\u0026quot;),\r## .. UnitPrice = col_double(),\r## .. CustomerID = col_double(),\r## .. Country = col_character()\r## .. )\rsales %\u0026gt;% summary()\r## InvoiceNo StockCode Description ## Length:325145 Length:325145 Length:325145 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## Quantity InvoiceDate UnitPrice ## Min. :-80995.00 Min. :2010-12-01 08:26:02 Min. :-11062.06 ## 1st Qu.: 1.00 1st Qu.:2011-03-28 12:13:02 1st Qu.: 1.25 ## Median : 3.00 Median :2011-07-20 10:50:59 Median : 2.08 ## Mean : 9.27 Mean :2011-07-04 14:11:43 Mean : 4.85 ## 3rd Qu.: 10.00 3rd Qu.:2011-10-19 10:47:59 3rd Qu.: 4.13 ## Max. : 12540.00 Max. :2011-12-09 12:49:59 Max. : 38970.00 ## ## CustomerID Country ## Min. :12347 Length:325145 ## 1st Qu.:13959 Class :character ## Median :15150 Mode :character ## Mean :15289 ## 3rd Qu.:16793 ## Max. :18287 ## NA\u0026#39;s :80991\rIf you agree with me that summary sucks on a data.frame object I am glad to show skimr, also if you don’t like summary behaviour on model outputs broom is there to save you, I will talk more about when I make an scikit-learn and caret + tidymodels post.\n\r3.3.5 Subsetting Data with select or base R\rsales[1:4,]\r## # A tibble: 4 x 8\r## InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 536365 22752 SET 7 BABU~ 2 2010-12-01 08:26:02 7.65\r## 2 536365 71053 WHITE META~ 6 2010-12-01 08:26:02 3.39\r## 3 536365 84029G KNITTED UN~ 6 2010-12-01 08:26:02 3.39\r## 4 536365 85123A WHITE HANG~ 6 2010-12-01 08:26:02 2.55\r## # ... with 2 more variables: CustomerID \u0026lt;dbl\u0026gt;, Country \u0026lt;chr\u0026gt;\rsales$CustomerID %\u0026gt;% head()\r## [1] 17850 17850 17850 17850 17850 13047\rsales[[\u0026quot;CustomerID\u0026quot;]] %\u0026gt;% head()\r## [1] 17850 17850 17850 17850 17850 13047\rsales[,3] %\u0026gt;% head()\r## # A tibble: 6 x 1\r## Description ## \u0026lt;chr\u0026gt; ## 1 SET 7 BABUSHKA NESTING BOXES ## 2 WHITE METAL LANTERN ## 3 KNITTED UNION FLAG HOT WATER BOTTLE\r## 4 WHITE HANGING HEART T-LIGHT HOLDER ## 5 HAND WARMER UNION JACK ## 6 HOME BUILDING BLOCK WORD\rsales[1:5,3]\r## # A tibble: 5 x 1\r## Description ## \u0026lt;chr\u0026gt; ## 1 SET 7 BABUSHKA NESTING BOXES ## 2 WHITE METAL LANTERN ## 3 KNITTED UNION FLAG HOT WATER BOTTLE\r## 4 WHITE HANGING HEART T-LIGHT HOLDER ## 5 HAND WARMER UNION JACK\rsales$Revenue2 \u0026lt;- sales$Quantity * sales$UnitPrice\rsales[[\u0026quot;Revenue3\u0026quot;]] \u0026lt;- sales[[\u0026quot;Quantity\u0026quot;]] * sales[[\u0026quot;UnitPrice\u0026quot;]]\r# () show created objects # Strange behavior right here 6 rowns on head()\r(sales \u0026lt;- sales %\u0026gt;% mutate(Revenue = Quantity * UnitPrice)) %\u0026gt;% head()\r## # A tibble: 6 x 11\r## InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 536365 22752 SET 7 BABU~ 2 2010-12-01 08:26:02 7.65\r## 2 536365 71053 WHITE META~ 6 2010-12-01 08:26:02 3.39\r## 3 536365 84029G KNITTED UN~ 6 2010-12-01 08:26:02 3.39\r## 4 536365 85123A WHITE HANG~ 6 2010-12-01 08:26:02 2.55\r## 5 536366 22633 HAND WARME~ 6 2010-12-01 08:28:02 1.85\r## 6 536367 21754 HOME BUILD~ 3 2010-12-01 08:33:59 5.95\r## # ... with 5 more variables: CustomerID \u0026lt;dbl\u0026gt;, Country \u0026lt;chr\u0026gt;,\r## # Revenue2 \u0026lt;dbl\u0026gt;, Revenue3 \u0026lt;dbl\u0026gt;, Revenue \u0026lt;dbl\u0026gt;\rsum(sales$Revenue == sales$Revenue2)/nrow(sales)\r## [1] 1\rsum(sales$Revenue == sales$Revenue3)/nrow(sales)\r## [1] 1\rsum(sales$Revenue2 == sales$Revenue3)/nrow(sales)\r## [1] 1\r# If there were any differences between our columns the sum would return \u0026lt;1 \r\r3.3.6 Creating a new smaller data frame using transmute and base\rraw_sales \u0026lt;- sales %\u0026gt;% select(Quantity, UnitPrice, Revenue)\rraw_sales %\u0026gt;% head()\r## # A tibble: 6 x 3\r## Quantity UnitPrice Revenue\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2 7.65 15.3\r## 2 6 3.39 20.3\r## 3 6 3.39 20.3\r## 4 6 2.55 15.3\r## 5 6 1.85 11.1\r## 6 3 5.95 17.8\rraw_sales %\u0026gt;% glimpse()\r## Observations: 325,145\r## Variables: 3\r## $ Quantity \u0026lt;dbl\u0026gt; 2, 6, 6, 6, 6, 3, 3, 4, 6, 6, 6, 8, 4, 3, 3, 48, 24,...\r## $ UnitPrice \u0026lt;dbl\u0026gt; 7.65, 3.39, 3.39, 2.55, 1.85, 5.95, 5.95, 7.95, 1.65...\r## $ Revenue \u0026lt;dbl\u0026gt; 15.30, 20.34, 20.34, 15.30, 11.10, 17.85, 17.85, 31....\rraw_sales %\u0026gt;% skimr::skim()\r## Skim summary statistics\r## n obs: 325145 ## n variables: 3 ## ## -- Variable type:numeric ------------------------------------------------\r## variable missing complete n mean sd p0 p25 p50 p75\r## Quantity 0 325145 325145 9.27 154.39 -80995 1 3 10 ## Revenue 0 325145 325145 17.43 331.85 -168469.6 3.4 9.48 17.4 ## UnitPrice 0 325145 325145 4.85 116.83 -11062.06 1.25 2.08 4.13\r## p100 hist\r## 12540 \u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\r## 38970 \u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\r## 38970 \u0026lt;U+2581\u0026gt;\u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\r\r3.3.7 Ploting with ggplot\rsales %\u0026gt;% ggplot() +\raes(x = InvoiceDate, y = Revenue) +\rgeom_line()\r\r3.3.8 Filtering and replace data\rHere I really couldn`t figure out an easy way to filter using this\rcancel tricky that works in python.\ncancels = sales$Revenue \u0026lt; 0\rcancels %\u0026gt;% nrow()\r## NULL\rinvert_func \u0026lt;- function(cancel){\rifelse(cancel == 1,\r0,\r1)\r}\rsales2 = sales[invert_func(cancels),]\rsales2 %\u0026gt;% dim()\r## [1] 319557 11\rI really prefer the tidy way also.\nsales \u0026lt;- sales %\u0026gt;% filter(Revenue \u0026gt; 0)\r\r3.3.9 Groupby example in tidyverse\rI prefer the tidy way here as well.\nCountryGroups \u0026lt;- sales %\u0026gt;% group_by(Country) %\u0026gt;% summarise(sum_revenue = sum(Revenue),\rnumber_cases = n()) %\u0026gt;% arrange(-sum_revenue)\rCountryGroups\r## # A tibble: 38 x 3\r## Country sum_revenue number_cases\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 United Kingdom 5311080. 291129\r## 2 EIRE 176305. 4788\r## 3 Netherlands 165583. 1391\r## 4 Germany 138778. 5465\r## 5 France 127194. 5025\r## 6 Australia 79198. 726\r## 7 Spain 36117. 1420\r## 8 Switzerland 34315. 1169\r## 9 Belgium 24015. 1191\r## 10 Norway 23182. 658\r## # ... with 28 more rows\rskimr::skim(sales)\r## Skim summary statistics\r## n obs: 318036 ## n variables: 11 ## ## -- Variable type:character ----------------------------------------------\r## variable missing complete n min max empty n_unique\r## Country 0 318036 318036 3 20 0 38\r## Description 0 318036 318036 6 35 0 3926\r## InvoiceNo 0 318036 318036 6 7 0 19107\r## StockCode 0 318036 318036 1 12 0 3835\r## ## -- Variable type:numeric ------------------------------------------------\r## variable missing complete n mean sd p0 p25\r## CustomerID 79261 238775 318036 15295.34 1713.1 12347 13969 ## Quantity 0 318036 318036 10.25 38.3 1 1 ## Revenue 0 318036 318036 19.78 104.17 0.001 3.75\r## Revenue2 0 318036 318036 19.78 104.17 0.001 3.75\r## Revenue3 0 318036 318036 19.78 104.17 0.001 3.75\r## UnitPrice 0 318036 318036 3.96 42.53 0.001 1.25\r## p50 p75 p100 hist\r## 15157 16800 18287 \u0026lt;U+2587\u0026gt;\u0026lt;U+2586\u0026gt;\u0026lt;U+2587\u0026gt;\u0026lt;U+2587\u0026gt;\u0026lt;U+2586\u0026gt;\u0026lt;U+2586\u0026gt;\u0026lt;U+2586\u0026gt;\u0026lt;U+2587\u0026gt;\r## 3 10 4800 \u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\r## 9.9 17.7 38970 \u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\r## 9.9 17.7 38970 \u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\r## 9.9 17.7 38970 \u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\r## 2.08 4.13 13541.33 \u0026lt;U+2587\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\u0026lt;U+2581\u0026gt;\r## ## -- Variable type:POSIXct ------------------------------------------------\r## variable missing complete n min max median\r## InvoiceDate 0 318036 318036 2010-12-01 2011-12-09 2011-07-20\r## n_unique\r## 17750\r\r3.3.10 Ploting an histogram using ggplot2\rsales %\u0026gt;%\rfilter(CustomerID == 17850) %\u0026gt;% ggplot() +\raes(Revenue) +\rgeom_histogram(bins = 20)\rAnother example.\nsales %\u0026gt;%\rfilter(StockCode == 71053) %\u0026gt;% ggplot() +\raes(Revenue) +\rgeom_histogram(bins = 20)\r\r3.3.11 Handling Missing values in R\rOk I got hand this one to python.\nsales2$CustomerID %\u0026gt;% table(useNA = \u0026#39;always\u0026#39;) %\u0026gt;%\rsort(decreasing = TRUE) %\u0026gt;%\rhead(3)\r## .\r## 17850 \u0026lt;NA\u0026gt; ## 319557 0\rThis is just not simple enough luckly we can create functions for our afflictions, plus this is replacement as an side effect which sucks.\n#sales[sales[[\u0026quot;CustomerID\u0026quot;]] %\u0026gt;% is.na(),\u0026quot;CustomerID\u0026quot;] \u0026lt;- 0\rThis is an way better tidy way.\n# sales %\u0026gt;% mutate_if(is.numeric, funs(replace(., is.na(.), 0)))\rsales2 \u0026lt;- sales %\u0026gt;% mutate_at(vars(CustomerID),\rlist(\r~replace(.,\ris.na(.), # function that check condition (na)\r0) # value to replace could be mean(.,na.rm = T)\r)\r)\rUsing an stronger method like mice even with an amazing multicore package takes too long for an blogpost, plus I really don’t think there should be an model for CustomerID here is some workflow if you need to split your data.\nnon_character_sales \u0026lt;- sales %\u0026gt;%\rselect_if(function(col)\ris.numeric(col) |\ris.factor(col))\r# or my favorite\rselect_cases \u0026lt;- function(col) {\ris.numeric(col) |\ris.factor(col)\r}\rnon_character_sales \u0026lt;- sales %\u0026gt;% select_if(select_cases)\rnon_character_sales %\u0026gt;% head()\r## # A tibble: 6 x 6\r## Quantity UnitPrice CustomerID Revenue2 Revenue3 Revenue\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2 7.65 17850 15.3 15.3 15.3\r## 2 6 3.39 17850 20.3 20.3 20.3\r## 3 6 3.39 17850 20.3 20.3 20.3\r## 4 6 2.55 17850 15.3 15.3 15.3\r## 5 6 1.85 17850 11.1 11.1 11.1\r## 6 3 5.95 13047 17.8 17.8 17.8\rcharacter_sales \u0026lt;- sales %\u0026gt;% select_if(negate(is.numeric))\rcharacter_sales %\u0026gt;% head()\r## # A tibble: 6 x 5\r## InvoiceNo StockCode Description InvoiceDate Country ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; ## 1 536365 22752 SET 7 BABUSHKA NESTIN~ 2010-12-01 08:26:02 United Ki~\r## 2 536365 71053 WHITE METAL LANTERN 2010-12-01 08:26:02 United Ki~\r## 3 536365 84029G KNITTED UNION FLAG HO~ 2010-12-01 08:26:02 United Ki~\r## 4 536365 85123A WHITE HANGING HEART T~ 2010-12-01 08:26:02 United Ki~\r## 5 536366 22633 HAND WARMER UNION JACK 2010-12-01 08:28:02 United Ki~\r## 6 536367 21754 HOME BUILDING BLOCK W~ 2010-12-01 08:33:59 United Ki~\rsales3 \u0026lt;- cbind(character_sales,non_character_sales)\r# if you need the same order\rsales3 \u0026lt;- sales3 %\u0026gt;% select(names(sales)) \r\r3.3.12 Replacing names with an case when aproach\rDon’t mix and match numbers and characters else this will cause an error.\nreplace_function \u0026lt;- function(country) {\rcase_when(\rcountry == \u0026#39;United Kingdom\u0026#39; ~ \u0026quot;1\u0026quot;,\rcountry == \u0026#39;Netherlands\u0026#39; ~ \u0026quot;2\u0026quot;,\rcountry == \u0026#39;Germany\u0026#39; ~ \u0026quot;3\u0026quot;,\rcountry == \u0026#39;France\u0026#39; ~ \u0026quot;4\u0026quot;,\rcountry == \u0026#39;USA\u0026#39; ~ \u0026quot;5\u0026quot;,\rTRUE ~ country\r)\r}\rsales3 \u0026lt;- sales3 %\u0026gt;% mutate(new = replace_function(Country))\rsales3 %\u0026gt;% head()\r## InvoiceNo StockCode Description Quantity\r## 1 536365 22752 SET 7 BABUSHKA NESTING BOXES 2\r## 2 536365 71053 WHITE METAL LANTERN 6\r## 3 536365 84029G KNITTED UNION FLAG HOT WATER BOTTLE 6\r## 4 536365 85123A WHITE HANGING HEART T-LIGHT HOLDER 6\r## 5 536366 22633 HAND WARMER UNION JACK 6\r## 6 536367 21754 HOME BUILDING BLOCK WORD 3\r## InvoiceDate UnitPrice CustomerID Country Revenue2\r## 1 2010-12-01 08:26:02 7.65 17850 United Kingdom 15.30\r## 2 2010-12-01 08:26:02 3.39 17850 United Kingdom 20.34\r## 3 2010-12-01 08:26:02 3.39 17850 United Kingdom 20.34\r## 4 2010-12-01 08:26:02 2.55 17850 United Kingdom 15.30\r## 5 2010-12-01 08:28:02 1.85 17850 United Kingdom 11.10\r## 6 2010-12-01 08:33:59 5.95 13047 United Kingdom 17.85\r## Revenue3 Revenue new\r## 1 15.30 15.30 1\r## 2 20.34 20.34 1\r## 3 20.34 20.34 1\r## 4 15.30 15.30 1\r## 5 11.10 11.10 1\r## 6 17.85 17.85 1\rTwo ways of solving our case_count deficiency.\nvalue_counts \u0026lt;- function(column, useNA = \u0026#39;always\u0026#39;, decreasing = TRUE) {\rcolumn %\u0026gt;% table(useNA = useNA) %\u0026gt;%\rsort(decreasing = decreasing)\r}\rsales3[[\u0026quot;new\u0026quot;]] %\u0026gt;% value_counts() %\u0026gt;% head(7)\r## .\r## 1 3 4 EIRE Spain 2 Belgium ## 291129 5465 5025 4788 1420 1391 1191\r\r\r3.4 Passing Objects to Python\rSimple example.\nsales2 = r.sales2\rtype(sales2)\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\rWe can solve our value_counts problem by simply stealing from python then returning the results to r.\nsales3_solution = \\\rr.\\\rsales3.\\\rnew.\\\rvalue_counts().\\\rnlargest(7)\rIf we want to continue working in r after the steal.\nsales3_solution = py$sales3_solution\rsales3_solution\r## 1 3 4 EIRE Spain 2 Belgium ## 291129 5465 5025 4788 1420 1391 1191\r\r\r","date":1553299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553299200,"objectID":"fdf37e9a2cb5ba6f258876dc40e651f1","permalink":"https://twosidesdata.netlify.com/2019/03/23/exploratory-data-analysis-basic-pandas-and-dplyr/","publishdate":"2019-03-23T00:00:00Z","relpermalink":"/2019/03/23/exploratory-data-analysis-basic-pandas-and-dplyr/","section":"post","summary":"This is an basic example of how you can use either R or Python to accomplish the same goals, I really enjoy using the tidyverse but as you will see sometimes Python is just the more intuitive option. If you find yourself confused on whether a code chunk is an R or Python code please ask me or check my github page for this project. \r1 Getting Started, we will use multiple functions from both languages\r1.","tags":["R Markdown","reticulate","pandas","dplyr","tidyverse"],"title":"exploratory data analysis: basic pandas and dplyr","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536462000,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"https://twosidesdata.netlify.com/tutorial/example/","publishdate":"2018-09-09T00:00:00-03:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":"#external_link.\n","date":1461726000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461726000,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://twosidesdata.netlify.com/project/external-project/","publishdate":"2016-04-27T00:00:00-03:00","relpermalink":"/project/external-project/","section":"project","summary":"An r package for combining and testing multiples forecasts.","tags":["libraries","forecast"],"title":"Forecast Bonsai","type":"project"},{"authors":null,"categories":null,"content":"Creating this website and updating it weekly is my personal task for 2019.\nContent includes blogpost, eduction, accomplishments, my TCC and more.\n","date":1461726000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461726000,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://twosidesdata.netlify.com/project/internal-project/","publishdate":"2016-04-27T00:00:00-03:00","relpermalink":"/project/internal-project/","section":"project","summary":"My first personal website.","tags":["Personal Website"],"title":"TwoSidesData","type":"project"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441076400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441076400,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"https://twosidesdata.netlify.com/publication/person-re-id/","publishdate":"2015-09-01T00:00:00-03:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372647600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372647600,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"https://twosidesdata.netlify.com/publication/clothing-search/","publishdate":"2013-07-01T00:00:00-03:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"A mobile visual clothing search system is presented whereby a smart phone user can either choose a social networking image or capture a new photo of a person wearing clothing of interest and search for similar clothing in a large cloud-based ecommerce database. The phone's GPS location is used to re-rank results by retail store location, to inform the user of local stores where similar clothing items can be tried on.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"https://twosidesdata.netlify.com/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating.","tags":null,"title":"Slides","type":"slides"}]